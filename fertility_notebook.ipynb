{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac2306c",
   "metadata": {},
   "source": [
    "# Milestone 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1586e42",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7423c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091b7ea",
   "metadata": {},
   "source": [
    "## 1. Access the data that you will be using for the final project by downloading, collecting, or scraping* from the relevant source(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ab7e9",
   "metadata": {},
   "source": [
    "We accessed **fertility rate by state over time** (the response variable) from https://www.cdc.gov/nchs/pressroom/sosmap/fertility_rate/fertility_rates.htm and saved it as `fertility_rate_census.csv` DEFINED AS: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We care about basic demographic data which we will get from the census. We will only include data for women from ages 15-44 and then aggregate by state. We will then create a new dataset of percentage of each demographic for each state. Ex: percentage of white (women 15-44) in MA, percentage of asian in MA,... percentage of white in NJ, percentage of asian in NJ etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the census data we consider important (based on our own subjective opinions):\n",
    "- race\n",
    "- socio economic status (household income) - percentage of households below the poverty line\n",
    "- education level - share that are high school graduates \n",
    "- immigration status\n",
    "\n",
    "And obviosuly we need **age**, **sex** , **year** , and **state**  in order to aggregate our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6787a3",
   "metadata": {},
   "source": [
    "We got this data from IPSUMS USA, downloading it and importing it into the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also care about religion, political makeup, and whether certain abortion laws are in place (all of which are not in the census). We will find them different ways. Please note **WE ARE TRYING TO USE AS FEW CATEGORICAL DATAPOINTS AS POSSIBLE** as we don't want our predictions to get too specific. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the data into a Jupyter notebook and understand the data by examining, among other characteristics of interest, data missingness, imbalance, and scaling issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some issues we preliminary have considered before even inspecting the data includes:\n",
    "\n",
    "\n",
    "- **under reporting immigration status**: via google, people tend to underreport whether they are immigrants. This is potentially a missingness issue\n",
    "-  **Multicollinearity**: There is most likely a relationship between racial background / household income and an individual's birth country (immigration status) so we can't use both as predictors in the same equation. We don't know how strong this correlation will be so are not worried yet, but would love to talk about it with a TF\n",
    "\n",
    "\n",
    "- **How to determine political makeup**: \n",
    "\n",
    "- **is this just too much data??**: looking at each of these attributes for each state for each year may just be too many dimensions. Is there really a big difference accross years? Should we just look at 1? If so, which? \n",
    "\n",
    "- **Abortion Laws**: Some of the literature states that abortion laws have an unintuitive effect on fertility rates. We've decided to create a variable that captures the \"severity\" of the abortion laws for each state (ex. 0 for nothing, 0.3 for 3 weeks, 0.65 for 5 weeks etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to import and inspect each dataset, looking for missingness, imbalance, and scaling issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>STATE</th>\n",
       "      <th>FERTILITY RATE</th>\n",
       "      <th>BIRTHS</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>AL</td>\n",
       "      <td>58.7</td>\n",
       "      <td>58149</td>\n",
       "      <td>/nchs/pressroom/states/alabama/al.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>AK</td>\n",
       "      <td>64.9</td>\n",
       "      <td>9359</td>\n",
       "      <td>/nchs/pressroom/states/alaska/ak.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>AZ</td>\n",
       "      <td>54.9</td>\n",
       "      <td>78547</td>\n",
       "      <td>/nchs/pressroom/states/arizona/az.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>AR</td>\n",
       "      <td>60.2</td>\n",
       "      <td>35471</td>\n",
       "      <td>/nchs/pressroom/states/arkansas/ar.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>CA</td>\n",
       "      <td>52.8</td>\n",
       "      <td>419104</td>\n",
       "      <td>/nchs/pressroom/states/california/ca.htm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR STATE  FERTILITY RATE  BIRTHS  \\\n",
       "0  2022    AL            58.7   58149   \n",
       "1  2022    AK            64.9    9359   \n",
       "2  2022    AZ            54.9   78547   \n",
       "3  2022    AR            60.2   35471   \n",
       "4  2022    CA            52.8  419104   \n",
       "\n",
       "                                        URL  \n",
       "0     /nchs/pressroom/states/alabama/al.htm  \n",
       "1      /nchs/pressroom/states/alaska/ak.htm  \n",
       "2     /nchs/pressroom/states/arizona/az.htm  \n",
       "3    /nchs/pressroom/states/arkansas/ar.htm  \n",
       "4  /nchs/pressroom/states/california/ca.htm  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fertility_rate = pd.read_csv('data/fertility_rate_census.csv')\n",
    "fertility_rate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understand and describe the preprocessing required such that data is in a form amenable to later downstream tasks such as visualizing and modeling, as is appropriate to the specific project goals.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> expected_url = './data/html/screenboston.html'\n>>> assert os.path.isfile(expected_url), f'Expected local file {expected_url}'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> expected_url = './data/html/screenboston.html'\n>>> with open(expected_url, 'r') as f:\n...     content = f.read()\n...     s = BeautifulSoup(content)\n...     assert s.select_one('p').text == 'Screen Boston', f'Content of file saved from {expected_url} should contain a <p> tag with the page name.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> n = 50\n>>> assert len(movies) > n, f'`movies` should contain more than {n} elements; you have {len(movies)}.'\n>>> assert all((isinstance(m, dict) for m in movies)), 'Elements of `movies` should all be dictionaries.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> keys = {'title', 'directors', 'year', 'genre', 'runtime', 'theater', 'screen_date', 'screen_times'}\n>>> assert all((set(keys).issubset(m.keys()) for m in movies)), f'Each dictionary in `movies` should contain all of these keys: {', '.join(keys)}. At least one dictionary is missing one or more keys.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> assert all((isinstance(m['year'], int) for m in movies)), \"The 'year' value should be an integer in all dictionaries.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> snapshots = glob.glob('./data/html/snapshot_*.html')\n>>> assert len(snapshots) >= 4, f'You should have at least 4 snapshots, but found {len(snapshots)} files with paths like ./data/html/snapshot_*.html'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> snapshots = glob.glob('./data/html/snapshot_*.html')\n>>> assert all((re.match('snapshot_\\\\d{8}\\\\.html', os.path.basename(f)) for f in snapshots)), \"All snapshot files should be named in the format 'snapshot_YYYYMMDD.html'\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> assert os.path.isfile('data/movies.json'), \"The file 'data/movies.json' should exist.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> with open('data/movies.json', 'r') as f:\n...     movies = json.load(f)\n>>> n = 300\n>>> assert len(movies) > n, f'`movies` should now contain more than {n} elements; you have {len(movies)}.'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> keys = {'title', 'directors', 'year', 'genre', 'runtime', 'theater', 'screen_date', 'screen_times'}\n>>> assert all((isinstance(m, dict) and set(keys).issubset(m.keys()) for m in movies)), f'Each dictionary in `movies` should contain all of these keys: {', '.join(keys)}. At least one dictionary is missing one or more keys.'\n",
         "hidden": false,
         "locked": false,
         "points": 5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(df, pd.DataFrame), \"You should have stored your DataFrame in a variable called 'df'\"\n>>> assert df.shape[1] == 8, 'Your DataFrame, df, should have 8 columns'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> assert df.shape[0] > 300, 'You should have found at least 300 non-duplicate rows'\n>>> assert df.shape[0] == df.drop_duplicates().shape[0], 'There are still duplicate rows in your DataFrame'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> def is_datetime_column_hacky(column):\n...     dtype_str = str(column.dtype)\n...     valid_types = ['date', 'timestamp', 'timedelta', 'duration']\n...     return any((t in dtype_str.lower() for t in valid_types))\n>>> assert is_datetime_column_hacky(df['screen_date']), \"The 'screen_date' column must be either a Pandas or PyArrow date/datetime type.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> def is_datetime_column_hacky(column):\n...     dtype_str = str(column.dtype)\n...     valid_types = ['date', 'timestamp', 'timedelta', 'duration']\n...     return any((t in dtype_str.lower() for t in valid_types))\n>>> assert is_datetime_column_hacky(df['runtime']), \"The 'runtime' column must be either a Pandas or PyArrow timedelta type.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> assert df['screen_date'].is_monotonic_increasing, \"The 'screen_date' column is not sorted in ascending order\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": 7,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'wiki_id' in df.columns, \"`df` should now have a column called 'wiki_id'.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> p = 0.5\n>>> assert (q := df['wiki_id'].notna().mean()) >= p, f'You should have been able to find wiki IDs for at least {p:.0%} of the screenings. You found {q:.0%}.'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'wiki_html' in df.columns, \"`df` should now have a column called 'wiki_html'.\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> p = 0.5\n>>> assert (q := df['wiki_html'].notna().mean()) >= p, f'You should have been able to acquire wiki page HTML content for at least {p:.0%} of the screenings. You found {q:.0%}.'\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "wrapup": {
     "name": "wrapup",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert float(hours_spent_on_hw), 'Please select a time in hours (int or float) to specify how long you spent on this assignment.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
