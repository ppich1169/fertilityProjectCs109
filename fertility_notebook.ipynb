{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1586e42",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7423c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly\n",
    "#import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import shuffle\n",
    "from warnings import simplefilter\n",
    "simplefilter('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d882ae3",
   "metadata": {},
   "source": [
    "Please note, we will be collaborating via Git. Please find our project at https://github.com/ppich1169/fertilityProjectCs109"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d461283f",
   "metadata": {},
   "source": [
    "# Milestone 1: Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07556bb7",
   "metadata": {},
   "source": [
    "Over the past fifty years, fertility rates in the US have plummeted and are currently at a historic low. Conversations about why fertility has fallen so substantially and how we can address the implications of this shift for government programs like social security have been quite salient in recent public discourse and in the 2024 election cycle. \n",
    "\n",
    "Interestingly, there is significant variation in fertility rates across US states. We’d like to understand the relative importance of various factors in determining a state’s fertility rate. \n",
    "\n",
    "We plan to run a multiple regression of fertility rate (can get state-by-state here from the CDC’s National Center for Health Statistics) on a number of regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d422fe2",
   "metadata": {},
   "source": [
    "**Goal:** Create a regression that can predict the fertility rate of a state. Then, analyze coefficients and/or use causal inference to understand why fertility rate is going down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2306c",
   "metadata": {},
   "source": [
    "# Milestone 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091b7ea",
   "metadata": {},
   "source": [
    "## 1. Access the data that you will be using for the final project by downloading, collecting, or scraping* from the relevant source(s) and 2. Load the data into a Jupyter notebook and understand the data by examining, among other characteristics of interest, data missingness, imbalance, and scaling issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225eca3b",
   "metadata": {},
   "source": [
    "**TL;DR** Hi! We didn't know how long to make our doc but wanted to get as much feedback as possible so included everything here. We are more than happy to procide an abridged version. Essentially, we aquired data with a combination of webscraping and downloading from websites. Most of the data has some element of missingess as there are some gaps in years. We would fill this missingness by creating a linear regression on the data we have, and filling in our unknown data. We also needed to scale some of our data into the range 0-1. Finally, we proprocessed that data by making some categorical variables ordinal, others one-hot encoded, and finally putting all our individal data sets together (linking on year and state). Thank you for taking the time to read this!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6af71",
   "metadata": {},
   "source": [
    "### Response Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ab7e9",
   "metadata": {},
   "source": [
    "Our response variable is **fertility rate by state over time** which can be found at https://www.cdc.gov/nchs/pressroom/sosmap/fertility_rate/fertility_rates.htm. \n",
    "\n",
    "We accessed it via download and saved it as `fertility_rate_census.csv`. \n",
    "\n",
    "Please note, fertility rate is defined as  **total number of births per 1,000 women aged 15-44** and our dataset looks at fertility rate for each of the 50 states over 9 years (2014-2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "28728668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR              0\n",
      "STATE             0\n",
      "FERTILITY RATE    0\n",
      "BIRTHS            0\n",
      "URL               0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>FERTILITY RATE</th>\n",
       "      <th>BIRTHS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>451.000000</td>\n",
       "      <td>451.000000</td>\n",
       "      <td>451.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2018.008869</td>\n",
       "      <td>60.057871</td>\n",
       "      <td>75783.962306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.588850</td>\n",
       "      <td>6.420758</td>\n",
       "      <td>86837.134690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2014.000000</td>\n",
       "      <td>44.300000</td>\n",
       "      <td>5133.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2016.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>21758.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>60.200000</td>\n",
       "      <td>55971.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>86486.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2022.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>502879.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              YEAR  FERTILITY RATE         BIRTHS\n",
       "count   451.000000      451.000000     451.000000\n",
       "mean   2018.008869       60.057871   75783.962306\n",
       "std       2.588850        6.420758   86837.134690\n",
       "min    2014.000000       44.300000    5133.000000\n",
       "25%    2016.000000       56.000000   21758.500000\n",
       "50%    2018.000000       60.200000   55971.000000\n",
       "75%    2020.000000       63.500000   86486.000000\n",
       "max    2022.000000       80.000000  502879.000000"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fertility_rate = pd.read_csv('data/fertility_rate_census.csv')\n",
    "print(fertility_rate.isna().sum(axis=0))\n",
    "fertility_rate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ca340f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>YEAR</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>District of Columbia</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "YEAR                  2014  2015  2016  2017  2018  2019  2020  2021  2022\n",
       "STATE                                                                     \n",
       "District of Columbia   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  57.3"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_fertility = fertility_rate.pivot(index='STATE', columns='YEAR', values='FERTILITY RATE')\n",
    "pivoted_fertility[pivoted_fertility.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656d151",
   "metadata": {},
   "source": [
    "Things to Consider:\n",
    "\n",
    "**Missingness**: While there is no empty cell in the original dataset, we see if we pivot it by year, Washington DC only shows up in 2022. We can simply delete this row as Washington DC isn't technically a state. This is Missing at Random because we know the reason there was no Washington DC from 2014-2021 is that DC isn't considered a state\n",
    "\n",
    "**Imbalance**: Since we aren't looking at different classes there is no imbalance\n",
    "\n",
    "**Scaling**: By considering Fertility Rate (and not Number of Births), we are essentially normalizing our data as we are dividing it by total population. This will be enough in order to scale the data as it takes into account the populations of each state such that no state is overly weighed. In addition, since fertility rate is a response variable, not a predictor, we don't have to consider how large our data is in relation to other variables. Thus, we don't have to scale it any further.\n",
    "\n",
    "**Other**: We need to encode the state variable as categorical if it will be used in modeling by creating dummy variables or one-hot encoding. This issue will be true for all datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b4bfa7",
   "metadata": {},
   "source": [
    "### Predictors\n",
    "_We used our previous knowledge and assumptions to create an **X** dataset of predictors that we believe may influence fertility rates_\n",
    "\n",
    "Because fertility rate is evaluated statewide , and states vary significantly in population size, we have decided that for all of the predictors, we are going to essentially **normalize** them by looking at the percentage of each state that fall into a specific category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0005638",
   "metadata": {},
   "source": [
    "## CENSUS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa6b619",
   "metadata": {},
   "source": [
    "We began by collecting essential demographic data from the U.S. Census Bureau's website (https://www.census.gov) to analyze trends across the U.S. population. This data included indicators such as socioeconomic status, foreign-born populations, education levels, and racial demographics. We organized the data into separate data frames—`SPM_df`, `foreignborn_df`, `education_dfs`, and `race_dfs`—to facilitate efficient management and analysis.\n",
    "\n",
    "The **SPM (Supplemental Poverty Measure)** data, loaded from `SPM.csv`, provides a broad measure of poverty across states, capturing household income levels below the poverty line and accounting for both cash income and non-cash benefits. Although this data originates from the Census Bureau, we accessed it through Statista (https://www.statista.com/statistics/312701/percentage-of-population-foreign-born-in-the-us-by-state/), where it was already aggregated and prepared for analysis. Upon examination, we found no missing values, making it straightforward to integrate into our analysis without additional preprocessing. The structure, with a single column for each state’s poverty estimate, is simple and well-suited for analysis.\n",
    "\n",
    "The **foreign-born population dataset**, sourced directly from Census data in `foreignborn2022.csv`, records the percentage of foreign-born individuals by state for 2022. This dataset offers insights into immigration trends, which may impact fertility rates, and was found to be free from missing values. The data is cleanly structured with only two columns (`State` and `Percent`), making it ready for immediate use in analysis.\n",
    "\n",
    "The **education data** files, loaded from various `*education.csv` files, provide shares of educational attainment across different age groups, offering a nuanced view of educational levels within each state. Each file includes detailed demographic breakdowns with hierarchical labels representing educational categories, such as `Total`, `Percent`, and specific demographic groupings (like age, gender). When analyzing these files, we found a complex column structure and hierarchical labels, which will require additional preprocessing for effective analysis. Moreover, each state had some missing values across specific columns, likely due to unavailable data for certain demographic subgroups. These missing values will need to be handled through imputation or selective removal of columns with excessive gaps.\n",
    "\n",
    "Finally, the **race data** files, loaded from various `*race.csv` files, provide demographic breakdowns by race for each state. This dataset mirrors the structure of the education data, with hierarchical labels that represent various racial categories and demographic details. Similar to the education data, the race data was found to have missing values and a complex structure. The intricate labeling and demographic specificity will require careful preprocessing to flatten the hierarchical labels and manage missing values, ensuring consistency with other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072fb112",
   "metadata": {},
   "source": [
    "### SPM Dataset\n",
    "\n",
    "**Missingness**: The SPM data, sourced from `SPM.csv`, contains no missing values, making it easy to integrate into the analysis without any imputation. The completeness of this dataset ensures we can use it directly to assess the relationship between poverty levels and fertility rates across states.\n",
    "\n",
    "**Imbalance**: This dataset represents one measurement (poverty estimate) per state, so there are no categorical classes that could lead to imbalance. Each state has a single poverty estimate, meaning no particular state is over- or underrepresented in terms of poverty measurement.\n",
    "\n",
    "**Scaling**: The SPM dataset is already normalized, as it provides the percentage of households below the poverty line for each state. This percentage-based format allows us to make direct comparisons between states without additional scaling, as the data is inherently comparable across geographic areas.\n",
    "\n",
    "### Foreign-Born Population Dataset\n",
    "\n",
    "**Missingness**: The foreign-born population dataset, from `foreignborn2022.csv`, is also complete with no missing values. This allows for straightforward inclusion in the analysis, with no need for handling missing data.\n",
    "\n",
    "**Imbalance**: This dataset has no categorical classes and represents a single percentage (foreign-born population) per state. There is, therefore, no issue of imbalance in this dataset, as each state is equally represented with one percentage value.\n",
    "\n",
    "**Scaling**:  The data is already in percentage form, representing the share of foreign-born individuals in each state's population. This normalized format allows for direct comparison across states without further scaling, as each percentage reflects a relative measure rather than an absolute count.\n",
    "\n",
    "### Education Dataset\n",
    "\n",
    "**Missingness**: The education data files, loaded from `*education.csv`, contain missing values across some columns, particularly in specific demographic breakdowns within each state. This missingness appears sporadically, likely due to data limitations in certain subgroups or categories. Handling these missing values will require careful consideration, with options to either impute values based on related data points or exclude columns with excessive gaps.\n",
    "\n",
    "**Imbalance**:  \n",
    "Imbalance is not a significant concern for the education dataset, as each state is represented with comprehensive data on various age and educational attainment levels. The dataset does not rely on categorical classes that could lead to imbalance issues, and all states provide data across similar categories of educational attainment.\n",
    "\n",
    "**Scaling**:  \n",
    "Since the education data is provided by age group, we need to scale it by the population size of each age group within each state to ensure accuracy when comparing educational attainment percentages. Scaling by age group population will allow us to make accurate cross-state comparisons of educational attainment levels, as it controls for differences in the age distributions across states. This normalization will make the educational attainment percentages more representative and comparable in relation to each state's age demographics.\n",
    "\n",
    "### Race Dataset\n",
    "\n",
    "**Missingness**:  \n",
    "The race data files, loaded from `*race.csv`, exhibit some missing values, likely due to unavailable data for certain racial categories or demographic subgroups within states. Similar to the education dataset, we will need to address these missing values through imputation or by selectively excluding columns with significant gaps.\n",
    "\n",
    "**Imbalance**:  \n",
    "This dataset contains detailed racial breakdowns across states, with some racial categories potentially having fewer entries than others. If analyzing racial subgroups individually, imbalance may occur due to underrepresentation of smaller demographic groups in certain states, which could lead to biased interpretations. Grouping data by broader racial categories or considering population weights may help to mitigate this issue.\n",
    "\n",
    "**Scaling**:  \n",
    "The data is already in percentage form, representing the share of individuals of differet races in each state's population. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc03d6",
   "metadata": {},
   "source": [
    "**Load and explore data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9e892e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPM Data Structure:\n",
      "   Year      States Estimate\n",
      "0  2022     Alabama     16.3\n",
      "1  2022      Alaska     10.8\n",
      "2  2022     Arizona     12.3\n",
      "3  2022    Arkansas     16.6\n",
      "4  2022  California     11.7\n",
      "\n",
      "Missing Values in SPM Data:\n",
      "Year        0\n",
      "States      0\n",
      "Estimate    0\n",
      "dtype: int64\n",
      "\n",
      "Foreign Born Data Structure:\n",
      "0       State  2022.0  2010.0\n",
      "0  California    26.7    27.2\n",
      "1  New Jersey    23.5    21.0\n",
      "2    New York    22.7    22.2\n",
      "3     Florida    21.7    19.4\n",
      "4      Nevada    18.9    18.8\n",
      "\n",
      "Missing Values in Foreign Born Data:\n",
      "0\n",
      "State     0\n",
      "2022.0    0\n",
      "2010.0    0\n",
      "dtype: int64\n",
      "\n",
      "Education Data from data/2015education.csv:\n",
      "                                  Label (Grouping) Alabama!!Total!!Estimate  \\\n",
      "0                    AGE BY EDUCATIONAL ATTAINMENT                      NaN   \n",
      "1                        Population 18 to 24 years                  468,319   \n",
      "2                   Less than high school graduate                   70,992   \n",
      "3      High school graduate (includes equivalency)                  149,612   \n",
      "4               Some college or associate's degree                  212,684   \n",
      "\n",
      "  Alabama!!Total!!Margin of Error Alabama!!Percent!!Estimate  \\\n",
      "0                             NaN                        NaN   \n",
      "1                          ±5,125                        (X)   \n",
      "2                          ±4,881                      15.2%   \n",
      "3                          ±5,667                      31.9%   \n",
      "4                          ±6,093                      45.4%   \n",
      "\n",
      "  Alabama!!Percent!!Margin of Error Alabama!!Males!!Estimate  \\\n",
      "0                               NaN                      NaN   \n",
      "1                               (X)                  235,692   \n",
      "2                              ±1.0                   40,806   \n",
      "3                              ±1.2                   83,861   \n",
      "4                              ±1.2                   97,919   \n",
      "\n",
      "  Alabama!!Males!!Margin of Error Alabama!!Percent Males!!Estimate  \\\n",
      "0                             NaN                              NaN   \n",
      "1                          ±2,985                              (X)   \n",
      "2                          ±3,371                            17.3%   \n",
      "3                          ±4,842                            35.6%   \n",
      "4                          ±4,446                            41.5%   \n",
      "\n",
      "  Alabama!!Percent Males!!Margin of Error Alabama!!Females!!Estimate  ...  \\\n",
      "0                                     NaN                        NaN  ...   \n",
      "1                                     (X)                    232,627  ...   \n",
      "2                                    ±1.4                     30,186  ...   \n",
      "3                                    ±2.0                     65,751  ...   \n",
      "4                                    ±1.9                    114,765  ...   \n",
      "\n",
      "  Puerto Rico!!Percent!!Estimate Puerto Rico!!Percent!!Margin of Error  \\\n",
      "0                            NaN                                   NaN   \n",
      "1                            (X)                                   (X)   \n",
      "2                           9.6%                                  ±0.9   \n",
      "3                          28.5%                                  ±1.3   \n",
      "4                          54.7%                                  ±1.5   \n",
      "\n",
      "  Puerto Rico!!Males!!Estimate Puerto Rico!!Males!!Margin of Error  \\\n",
      "0                          NaN                                 NaN   \n",
      "1                      178,653                              ±2,287   \n",
      "2                       20,386                              ±2,601   \n",
      "3                       54,569                              ±3,218   \n",
      "4                       92,867                              ±3,722   \n",
      "\n",
      "  Puerto Rico!!Percent Males!!Estimate  \\\n",
      "0                                  NaN   \n",
      "1                                  (X)   \n",
      "2                                11.4%   \n",
      "3                                30.5%   \n",
      "4                                52.0%   \n",
      "\n",
      "  Puerto Rico!!Percent Males!!Margin of Error Puerto Rico!!Females!!Estimate  \\\n",
      "0                                         NaN                            NaN   \n",
      "1                                         (X)                        175,610   \n",
      "2                                        ±1.5                         13,777   \n",
      "3                                        ±1.8                         46,351   \n",
      "4                                        ±1.9                        100,896   \n",
      "\n",
      "  Puerto Rico!!Females!!Margin of Error  \\\n",
      "0                                   NaN   \n",
      "1                                ±2,197   \n",
      "2                                ±2,141   \n",
      "3                                ±2,974   \n",
      "4                                ±3,849   \n",
      "\n",
      "  Puerto Rico!!Percent Females!!Estimate  \\\n",
      "0                                    NaN   \n",
      "1                                    (X)   \n",
      "2                                   7.8%   \n",
      "3                                  26.4%   \n",
      "4                                  57.5%   \n",
      "\n",
      "  Puerto Rico!!Percent Females!!Margin of Error  \n",
      "0                                           NaN  \n",
      "1                                           (X)  \n",
      "2                                          ±1.2  \n",
      "3                                          ±1.7  \n",
      "4                                          ±2.0  \n",
      "\n",
      "[5 rows x 625 columns]\n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Label (Grouping)                                 0\n",
      "Alabama!!Total!!Estimate                         4\n",
      "Alabama!!Total!!Margin of Error                  4\n",
      "Alabama!!Percent!!Estimate                       4\n",
      "Alabama!!Percent!!Margin of Error                4\n",
      "                                                ..\n",
      "Puerto Rico!!Percent Males!!Margin of Error      4\n",
      "Puerto Rico!!Females!!Estimate                   4\n",
      "Puerto Rico!!Females!!Margin of Error            4\n",
      "Puerto Rico!!Percent Females!!Estimate           4\n",
      "Puerto Rico!!Percent Females!!Margin of Error    4\n",
      "Length: 625, dtype: int64 \n",
      "\n",
      "\n",
      "Education Data from data/2022education.csv:\n",
      "                                    Label (Grouping) Alabama!!Total!!Estimate  \\\n",
      "0                      AGE BY EDUCATIONAL ATTAINMENT                      NaN   \n",
      "1                          Population 18 to 24 years                  488,344   \n",
      "2                     Less than high school graduate                   58,089   \n",
      "3          High school graduate (includes equival...                  176,620   \n",
      "4                 Some college or associate's degree                  208,994   \n",
      "\n",
      "  Alabama!!Total!!Margin of Error Alabama!!Percent!!Estimate  \\\n",
      "0                             NaN                        NaN   \n",
      "1                          ±5,417                        (X)   \n",
      "2                          ±4,934                      11.9%   \n",
      "3                          ±8,332                      36.2%   \n",
      "4                          ±8,746                      42.8%   \n",
      "\n",
      "  Alabama!!Percent!!Margin of Error Alabama!!Male!!Estimate  \\\n",
      "0                               NaN                     NaN   \n",
      "1                               (X)                 245,043   \n",
      "2                              ±1.0                  34,613   \n",
      "3                              ±1.6                  98,665   \n",
      "4                              ±1.8                  93,866   \n",
      "\n",
      "  Alabama!!Male!!Margin of Error Alabama!!Percent Male!!Estimate  \\\n",
      "0                            NaN                             NaN   \n",
      "1                         ±4,090                             (X)   \n",
      "2                         ±3,501                           14.1%   \n",
      "3                         ±4,827                           40.3%   \n",
      "4                         ±4,989                           38.3%   \n",
      "\n",
      "  Alabama!!Percent Male!!Margin of Error Alabama!!Female!!Estimate  ...  \\\n",
      "0                                    NaN                       NaN  ...   \n",
      "1                                    (X)                   243,301  ...   \n",
      "2                                   ±1.4                    23,476  ...   \n",
      "3                                   ±1.9                    77,955  ...   \n",
      "4                                   ±1.9                   115,128  ...   \n",
      "\n",
      "  Puerto Rico!!Percent!!Estimate Puerto Rico!!Percent!!Margin of Error  \\\n",
      "0                            NaN                                   NaN   \n",
      "1                            (X)                                   (X)   \n",
      "2                           7.2%                                  ±0.8   \n",
      "3                          28.7%                                  ±1.5   \n",
      "4                          52.5%                                  ±1.5   \n",
      "\n",
      "  Puerto Rico!!Male!!Estimate Puerto Rico!!Male!!Margin of Error  \\\n",
      "0                         NaN                                NaN   \n",
      "1                     151,694                             ±2,147   \n",
      "2                      12,355                             ±1,852   \n",
      "3                      52,446                             ±3,422   \n",
      "4                      74,609                             ±3,478   \n",
      "\n",
      "  Puerto Rico!!Percent Male!!Estimate  \\\n",
      "0                                 NaN   \n",
      "1                                 (X)   \n",
      "2                                8.1%   \n",
      "3                               34.6%   \n",
      "4                               49.2%   \n",
      "\n",
      "  Puerto Rico!!Percent Male!!Margin of Error Puerto Rico!!Female!!Estimate  \\\n",
      "0                                        NaN                           NaN   \n",
      "1                                        (X)                       147,321   \n",
      "2                                       ±1.2                         9,180   \n",
      "3                                       ±2.2                        33,480   \n",
      "4                                       ±2.1                        82,322   \n",
      "\n",
      "  Puerto Rico!!Female!!Margin of Error Puerto Rico!!Percent Female!!Estimate  \\\n",
      "0                                  NaN                                   NaN   \n",
      "1                               ±2,056                                   (X)   \n",
      "2                               ±1,829                                  6.2%   \n",
      "3                               ±3,097                                 22.7%   \n",
      "4                               ±3,646                                 55.9%   \n",
      "\n",
      "  Puerto Rico!!Percent Female!!Margin of Error  \n",
      "0                                          NaN  \n",
      "1                                          (X)  \n",
      "2                                         ±1.2  \n",
      "3                                         ±2.1  \n",
      "4                                         ±2.4  \n",
      "\n",
      "[5 rows x 625 columns]\n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Label (Grouping)                                0\n",
      "Alabama!!Total!!Estimate                        4\n",
      "Alabama!!Total!!Margin of Error                 4\n",
      "Alabama!!Percent!!Estimate                      4\n",
      "Alabama!!Percent!!Margin of Error               4\n",
      "                                               ..\n",
      "Puerto Rico!!Percent Male!!Margin of Error      4\n",
      "Puerto Rico!!Female!!Estimate                   4\n",
      "Puerto Rico!!Female!!Margin of Error            4\n",
      "Puerto Rico!!Percent Female!!Estimate           4\n",
      "Puerto Rico!!Percent Female!!Margin of Error    4\n",
      "Length: 625, dtype: int64 \n",
      "\n",
      "\n",
      "Education Data from data/2014education.csv:\n",
      "                              Label (Grouping) Alabama!!Total!!Estimate  \\\n",
      "0                    Population 18 to 24 years                  484,011   \n",
      "1               Less than high school graduate                    16.1%   \n",
      "2  High school graduate (includes equivalency)                    32.5%   \n",
      "3           Some college or associate's degree                    44.6%   \n",
      "4                  Bachelor's degree or higher                     6.8%   \n",
      "\n",
      "  Alabama!!Total!!Margin of Error Alabama!!Male!!Estimate  \\\n",
      "0                          ±4,587                 245,070   \n",
      "1                            ±1.0                   18.8%   \n",
      "2                            ±1.2                   36.3%   \n",
      "3                            ±1.4                   39.5%   \n",
      "4                            ±0.8                    5.4%   \n",
      "\n",
      "  Alabama!!Male!!Margin of Error Alabama!!Female!!Estimate  \\\n",
      "0                         ±3,588                   238,941   \n",
      "1                           ±1.4                     13.4%   \n",
      "2                           ±1.6                     28.5%   \n",
      "3                           ±1.9                     49.9%   \n",
      "4                           ±0.8                      8.1%   \n",
      "\n",
      "  Alabama!!Female!!Margin of Error Alaska!!Total!!Estimate  \\\n",
      "0                           ±2,848                  84,814   \n",
      "1                             ±1.3                   14.5%   \n",
      "2                             ±1.6                   37.3%   \n",
      "3                             ±1.9                   41.2%   \n",
      "4                             ±1.1                    7.0%   \n",
      "\n",
      "  Alaska!!Total!!Margin of Error Alaska!!Male!!Estimate  ...  \\\n",
      "0                         ±2,111                 48,494  ...   \n",
      "1                           ±1.6                  13.8%  ...   \n",
      "2                           ±2.7                  36.6%  ...   \n",
      "3                           ±2.7                  43.6%  ...   \n",
      "4                           ±1.5                   6.0%  ...   \n",
      "\n",
      "  Wyoming!!Male!!Estimate Wyoming!!Male!!Margin of Error  \\\n",
      "0                  30,668                         ±1,330   \n",
      "1                   14.2%                           ±3.1   \n",
      "2                   33.0%                           ±4.7   \n",
      "3                   47.4%                           ±4.6   \n",
      "4                    5.5%                           ±2.3   \n",
      "\n",
      "  Wyoming!!Female!!Estimate Wyoming!!Female!!Margin of Error  \\\n",
      "0                    26,759                             ±981   \n",
      "1                      9.5%                             ±2.4   \n",
      "2                     32.9%                             ±4.2   \n",
      "3                     49.7%                             ±4.8   \n",
      "4                      7.9%                             ±2.7   \n",
      "\n",
      "  Puerto Rico!!Total!!Estimate Puerto Rico!!Total!!Margin of Error  \\\n",
      "0                      374,975                              ±2,963   \n",
      "1                        11.0%                                ±0.9   \n",
      "2                        28.9%                                ±1.5   \n",
      "3                        52.5%                                ±1.6   \n",
      "4                         7.6%                                ±0.7   \n",
      "\n",
      "  Puerto Rico!!Male!!Estimate Puerto Rico!!Male!!Margin of Error  \\\n",
      "0                     193,170                             ±1,885   \n",
      "1                       13.6%                               ±1.3   \n",
      "2                       32.9%                               ±2.0   \n",
      "3                       49.0%                               ±2.1   \n",
      "4                        4.5%                               ±0.9   \n",
      "\n",
      "  Puerto Rico!!Female!!Estimate Puerto Rico!!Female!!Margin of Error  \n",
      "0                       181,805                               ±1,965  \n",
      "1                          8.3%                                 ±1.0  \n",
      "2                         24.5%                                 ±1.9  \n",
      "3                         56.3%                                 ±2.1  \n",
      "4                         10.9%                                 ±1.2  \n",
      "\n",
      "[5 rows x 313 columns]\n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Label (Grouping)                        0\n",
      "Alabama!!Total!!Estimate                3\n",
      "Alabama!!Total!!Margin of Error         3\n",
      "Alabama!!Male!!Estimate                 3\n",
      "Alabama!!Male!!Margin of Error          3\n",
      "                                       ..\n",
      "Puerto Rico!!Total!!Margin of Error     3\n",
      "Puerto Rico!!Male!!Estimate             3\n",
      "Puerto Rico!!Male!!Margin of Error      3\n",
      "Puerto Rico!!Female!!Estimate           3\n",
      "Puerto Rico!!Female!!Margin of Error    3\n",
      "Length: 313, dtype: int64 \n",
      "\n",
      "\n",
      "Education Data from data/2019education.csv:\n",
      "                                    Label (Grouping) Alabama!!Total!!Estimate  \\\n",
      "0                      AGE BY EDUCATIONAL ATTAINMENT                      NaN   \n",
      "1                          Population 18 to 24 years                  457,530   \n",
      "2                     Less than high school graduate                   56,454   \n",
      "3          High school graduate (includes equival...                  158,761   \n",
      "4                 Some college or associate's degree                  207,319   \n",
      "\n",
      "  Alabama!!Total!!Margin of Error Alabama!!Percent!!Estimate  \\\n",
      "0                             NaN                        NaN   \n",
      "1                          ±5,551                        (X)   \n",
      "2                          ±4,838                      12.3%   \n",
      "3                          ±7,166                      34.7%   \n",
      "4                          ±7,341                      45.3%   \n",
      "\n",
      "  Alabama!!Percent!!Margin of Error Alabama!!Male!!Estimate  \\\n",
      "0                               NaN                     NaN   \n",
      "1                               (X)                 226,648   \n",
      "2                              ±1.0                  33,366   \n",
      "3                              ±1.5                  85,196   \n",
      "4                              ±1.6                  96,007   \n",
      "\n",
      "  Alabama!!Male!!Margin of Error Alabama!!Percent Male!!Estimate  \\\n",
      "0                            NaN                             NaN   \n",
      "1                         ±3,523                             (X)   \n",
      "2                         ±3,985                           14.7%   \n",
      "3                         ±5,302                           37.6%   \n",
      "4                         ±5,865                           42.4%   \n",
      "\n",
      "  Alabama!!Percent Male!!Margin of Error Alabama!!Female!!Estimate  ...  \\\n",
      "0                                    NaN                       NaN  ...   \n",
      "1                                    (X)                   230,882  ...   \n",
      "2                                   ±1.7                    23,088  ...   \n",
      "3                                   ±2.4                    73,565  ...   \n",
      "4                                   ±2.4                   111,312  ...   \n",
      "\n",
      "  Puerto Rico!!Percent!!Estimate Puerto Rico!!Percent!!Margin of Error  \\\n",
      "0                            NaN                                   NaN   \n",
      "1                            (X)                                   (X)   \n",
      "2                           9.0%                                  ±1.0   \n",
      "3                          26.4%                                  ±1.4   \n",
      "4                          53.7%                                  ±1.6   \n",
      "\n",
      "  Puerto Rico!!Male!!Estimate Puerto Rico!!Male!!Margin of Error  \\\n",
      "0                         NaN                                NaN   \n",
      "1                     160,974                             ±1,913   \n",
      "2                      17,146                             ±2,084   \n",
      "3                      47,118                             ±3,253   \n",
      "4                      82,805                             ±3,697   \n",
      "\n",
      "  Puerto Rico!!Percent Male!!Estimate  \\\n",
      "0                                 NaN   \n",
      "1                                 (X)   \n",
      "2                               10.7%   \n",
      "3                               29.3%   \n",
      "4                               51.4%   \n",
      "\n",
      "  Puerto Rico!!Percent Male!!Margin of Error Puerto Rico!!Female!!Estimate  \\\n",
      "0                                        NaN                           NaN   \n",
      "1                                        (X)                       158,251   \n",
      "2                                       ±1.3                        11,512   \n",
      "3                                       ±2.0                        37,063   \n",
      "4                                       ±2.2                        88,521   \n",
      "\n",
      "  Puerto Rico!!Female!!Margin of Error Puerto Rico!!Percent Female!!Estimate  \\\n",
      "0                                  NaN                                   NaN   \n",
      "1                               ±2,204                                   (X)   \n",
      "2                               ±2,385                                  7.3%   \n",
      "3                               ±3,046                                 23.4%   \n",
      "4                               ±3,514                                 55.9%   \n",
      "\n",
      "  Puerto Rico!!Percent Female!!Margin of Error  \n",
      "0                                          NaN  \n",
      "1                                          (X)  \n",
      "2                                         ±1.5  \n",
      "3                                         ±1.8  \n",
      "4                                         ±2.2  \n",
      "\n",
      "[5 rows x 625 columns]\n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Label (Grouping)                                0\n",
      "Alabama!!Total!!Estimate                        4\n",
      "Alabama!!Total!!Margin of Error                 4\n",
      "Alabama!!Percent!!Estimate                      4\n",
      "Alabama!!Percent!!Margin of Error               4\n",
      "                                               ..\n",
      "Puerto Rico!!Percent Male!!Margin of Error      4\n",
      "Puerto Rico!!Female!!Estimate                   4\n",
      "Puerto Rico!!Female!!Margin of Error            4\n",
      "Puerto Rico!!Percent Female!!Estimate           4\n",
      "Puerto Rico!!Percent Female!!Margin of Error    4\n",
      "Length: 625, dtype: int64 \n",
      "\n",
      "\n",
      "Education Data from data/2018education.csv:\n",
      "                                    Label (Grouping) Alabama!!Total!!Estimate  \\\n",
      "0                      AGE BY EDUCATIONAL ATTAINMENT                      NaN   \n",
      "1                          Population 18 to 24 years                  461,302   \n",
      "2                     Less than high school graduate                   61,166   \n",
      "3          High school graduate (includes equival...                  161,286   \n",
      "4                 Some college or associate's degree                  202,178   \n",
      "\n",
      "  Alabama!!Total!!Margin of Error Alabama!!Percent!!Estimate  \\\n",
      "0                             NaN                        NaN   \n",
      "1                          ±5,123                        (X)   \n",
      "2                          ±4,682                      13.3%   \n",
      "3                          ±6,878                      35.0%   \n",
      "4                          ±7,159                      43.8%   \n",
      "\n",
      "  Alabama!!Percent!!Margin of Error Alabama!!Male!!Estimate  \\\n",
      "0                               NaN                     NaN   \n",
      "1                               (X)                 233,872   \n",
      "2                              ±1.0                  34,408   \n",
      "3                              ±1.4                  92,087   \n",
      "4                              ±1.5                  92,328   \n",
      "\n",
      "  Alabama!!Male!!Margin of Error Alabama!!Percent Male!!Estimate  \\\n",
      "0                            NaN                             NaN   \n",
      "1                         ±3,906                             (X)   \n",
      "2                         ±3,215                           14.7%   \n",
      "3                         ±5,322                           39.4%   \n",
      "4                         ±5,174                           39.5%   \n",
      "\n",
      "  Alabama!!Percent Male!!Margin of Error Alabama!!Female!!Estimate  ...  \\\n",
      "0                                    NaN                       NaN  ...   \n",
      "1                                    (X)                   227,430  ...   \n",
      "2                                   ±1.4                    26,758  ...   \n",
      "3                                   ±2.1                    69,199  ...   \n",
      "4                                   ±2.1                   109,850  ...   \n",
      "\n",
      "  Puerto Rico!!Percent!!Estimate Puerto Rico!!Percent!!Margin of Error  \\\n",
      "0                            NaN                                   NaN   \n",
      "1                            (X)                                   (X)   \n",
      "2                           8.9%                                  ±0.9   \n",
      "3                          27.1%                                  ±1.7   \n",
      "4                          54.2%                                  ±2.0   \n",
      "\n",
      "  Puerto Rico!!Male!!Estimate Puerto Rico!!Male!!Margin of Error  \\\n",
      "0                         NaN                                NaN   \n",
      "1                     159,379                             ±1,627   \n",
      "2                      17,192                             ±2,266   \n",
      "3                      48,781                             ±3,631   \n",
      "4                      83,129                             ±4,491   \n",
      "\n",
      "  Puerto Rico!!Percent Male!!Estimate  \\\n",
      "0                                 NaN   \n",
      "1                                 (X)   \n",
      "2                               10.8%   \n",
      "3                               30.6%   \n",
      "4                               52.2%   \n",
      "\n",
      "  Puerto Rico!!Percent Male!!Margin of Error Puerto Rico!!Female!!Estimate  \\\n",
      "0                                        NaN                           NaN   \n",
      "1                                        (X)                       153,734   \n",
      "2                                       ±1.4                        10,830   \n",
      "3                                       ±2.2                        36,142   \n",
      "4                                       ±2.8                        86,678   \n",
      "\n",
      "  Puerto Rico!!Female!!Margin of Error Puerto Rico!!Percent Female!!Estimate  \\\n",
      "0                                  NaN                                   NaN   \n",
      "1                               ±1,414                                   (X)   \n",
      "2                               ±1,569                                  7.0%   \n",
      "3                               ±3,162                                 23.5%   \n",
      "4                               ±3,766                                 56.4%   \n",
      "\n",
      "  Puerto Rico!!Percent Female!!Margin of Error  \n",
      "0                                          NaN  \n",
      "1                                          (X)  \n",
      "2                                         ±1.0  \n",
      "3                                         ±2.1  \n",
      "4                                         ±2.3  \n",
      "\n",
      "[5 rows x 625 columns]\n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Label (Grouping)                                0\n",
      "Alabama!!Total!!Estimate                        4\n",
      "Alabama!!Total!!Margin of Error                 4\n",
      "Alabama!!Percent!!Estimate                      4\n",
      "Alabama!!Percent!!Margin of Error               4\n",
      "                                               ..\n",
      "Puerto Rico!!Percent Male!!Margin of Error      4\n",
      "Puerto Rico!!Female!!Estimate                   4\n",
      "Puerto Rico!!Female!!Margin of Error            4\n",
      "Puerto Rico!!Percent Female!!Estimate           4\n",
      "Puerto Rico!!Percent Female!!Margin of Error    4\n",
      "Length: 625, dtype: int64 \n",
      "\n",
      "\n",
      "Education Data from data/2016education.csv:\n",
      "                                  Label (Grouping) Alabama!!Total!!Estimate  \\\n",
      "0                    AGE BY EDUCATIONAL ATTAINMENT                      NaN   \n",
      "1                        Population 18 to 24 years                  463,070   \n",
      "2                   Less than high school graduate                   63,369   \n",
      "3      High school graduate (includes equivalency)                  148,705   \n",
      "4               Some college or associate's degree                  218,092   \n",
      "\n",
      "  Alabama!!Total!!Margin of Error Alabama!!Percent!!Estimate  \\\n",
      "0                             NaN                        NaN   \n",
      "1                          ±4,595                        (X)   \n",
      "2                          ±4,435                      13.7%   \n",
      "3                          ±6,569                      32.1%   \n",
      "4                          ±6,157                      47.1%   \n",
      "\n",
      "  Alabama!!Percent!!Margin of Error Alabama!!Males!!Estimate  \\\n",
      "0                               NaN                      NaN   \n",
      "1                               (X)                  230,768   \n",
      "2                              ±0.9                   35,951   \n",
      "3                              ±1.4                   80,078   \n",
      "4                              ±1.3                  100,811   \n",
      "\n",
      "  Alabama!!Males!!Margin of Error Alabama!!Percent Males!!Estimate  \\\n",
      "0                             NaN                              NaN   \n",
      "1                          ±3,210                              (X)   \n",
      "2                          ±3,119                            15.6%   \n",
      "3                          ±4,879                            34.7%   \n",
      "4                          ±4,583                            43.7%   \n",
      "\n",
      "  Alabama!!Percent Males!!Margin of Error Alabama!!Females!!Estimate  ...  \\\n",
      "0                                     NaN                        NaN  ...   \n",
      "1                                     (X)                    232,302  ...   \n",
      "2                                    ±1.3                     27,418  ...   \n",
      "3                                    ±2.0                     68,627  ...   \n",
      "4                                    ±1.9                    117,281  ...   \n",
      "\n",
      "  Puerto Rico!!Percent!!Estimate Puerto Rico!!Percent!!Margin of Error  \\\n",
      "0                            NaN                                   NaN   \n",
      "1                            (X)                                   (X)   \n",
      "2                           9.0%                                  ±0.9   \n",
      "3                          27.1%                                  ±1.2   \n",
      "4                          54.5%                                  ±1.4   \n",
      "\n",
      "  Puerto Rico!!Males!!Estimate Puerto Rico!!Males!!Margin of Error  \\\n",
      "0                          NaN                                 NaN   \n",
      "1                      173,136                              ±2,276   \n",
      "2                       17,593                              ±1,898   \n",
      "3                       54,728                              ±3,139   \n",
      "4                       89,538                              ±3,832   \n",
      "\n",
      "  Puerto Rico!!Percent Males!!Estimate  \\\n",
      "0                                  NaN   \n",
      "1                                  (X)   \n",
      "2                                10.2%   \n",
      "3                                31.6%   \n",
      "4                                51.7%   \n",
      "\n",
      "  Puerto Rico!!Percent Males!!Margin of Error Puerto Rico!!Females!!Estimate  \\\n",
      "0                                         NaN                            NaN   \n",
      "1                                         (X)                        170,098   \n",
      "2                                        ±1.1                         13,356   \n",
      "3                                        ±1.8                         38,422   \n",
      "4                                        ±2.0                         97,571   \n",
      "\n",
      "  Puerto Rico!!Females!!Margin of Error  \\\n",
      "0                                   NaN   \n",
      "1                                ±2,021   \n",
      "2                                ±2,183   \n",
      "3                                ±2,443   \n",
      "4                                ±3,670   \n",
      "\n",
      "  Puerto Rico!!Percent Females!!Estimate  \\\n",
      "0                                    NaN   \n",
      "1                                    (X)   \n",
      "2                                   7.9%   \n",
      "3                                  22.6%   \n",
      "4                                  57.4%   \n",
      "\n",
      "  Puerto Rico!!Percent Females!!Margin of Error  \n",
      "0                                           NaN  \n",
      "1                                           (X)  \n",
      "2                                          ±1.3  \n",
      "3                                          ±1.5  \n",
      "4                                          ±2.0  \n",
      "\n",
      "[5 rows x 625 columns]\n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Label (Grouping)                                 0\n",
      "Alabama!!Total!!Estimate                         4\n",
      "Alabama!!Total!!Margin of Error                  4\n",
      "Alabama!!Percent!!Estimate                       4\n",
      "Alabama!!Percent!!Margin of Error                4\n",
      "                                                ..\n",
      "Puerto Rico!!Percent Males!!Margin of Error      4\n",
      "Puerto Rico!!Females!!Estimate                   4\n",
      "Puerto Rico!!Females!!Margin of Error            4\n",
      "Puerto Rico!!Percent Females!!Estimate           4\n",
      "Puerto Rico!!Percent Females!!Margin of Error    4\n",
      "Length: 625, dtype: int64 \n",
      "\n",
      "\n",
      "Education Data from data/2021education.csv:\n",
      "                                    Label (Grouping) Alabama!!Total!!Estimate  \\\n",
      "0                      AGE BY EDUCATIONAL ATTAINMENT                      NaN   \n",
      "1                          Population 18 to 24 years                  468,727   \n",
      "2                     Less than high school graduate                   62,982   \n",
      "3          High school graduate (includes equival...                  170,023   \n",
      "4                 Some college or associate's degree                  193,079   \n",
      "\n",
      "  Alabama!!Total!!Margin of Error Alabama!!Percent!!Estimate  \\\n",
      "0                             NaN                        NaN   \n",
      "1                          ±6,283                        (X)   \n",
      "2                          ±5,794                      13.4%   \n",
      "3                          ±8,141                      36.3%   \n",
      "4                          ±8,068                      41.2%   \n",
      "\n",
      "  Alabama!!Percent!!Margin of Error Alabama!!Male!!Estimate  \\\n",
      "0                               NaN                     NaN   \n",
      "1                               (X)                 232,322   \n",
      "2                              ±1.2                  35,285   \n",
      "3                              ±1.7                  92,663   \n",
      "4                              ±1.6                  86,872   \n",
      "\n",
      "  Alabama!!Male!!Margin of Error Alabama!!Percent Male!!Estimate  \\\n",
      "0                            NaN                             NaN   \n",
      "1                         ±3,449                             (X)   \n",
      "2                         ±3,998                           15.2%   \n",
      "3                         ±5,505                           39.9%   \n",
      "4                         ±5,237                           37.4%   \n",
      "\n",
      "  Alabama!!Percent Male!!Margin of Error Alabama!!Female!!Estimate  ...  \\\n",
      "0                                    NaN                       NaN  ...   \n",
      "1                                    (X)                   236,405  ...   \n",
      "2                                   ±1.7                    27,697  ...   \n",
      "3                                   ±2.3                    77,360  ...   \n",
      "4                                   ±2.2                   106,207  ...   \n",
      "\n",
      "  Puerto Rico!!Percent!!Estimate Puerto Rico!!Percent!!Margin of Error  \\\n",
      "0                            NaN                                   NaN   \n",
      "1                            (X)                                   (X)   \n",
      "2                           7.1%                                  ±0.8   \n",
      "3                          27.4%                                  ±1.7   \n",
      "4                          55.7%                                  ±1.7   \n",
      "\n",
      "  Puerto Rico!!Male!!Estimate Puerto Rico!!Male!!Margin of Error  \\\n",
      "0                         NaN                                NaN   \n",
      "1                     159,616                             ±3,010   \n",
      "2                      14,065                             ±2,323   \n",
      "3                      48,471                             ±4,013   \n",
      "4                      85,107                             ±4,048   \n",
      "\n",
      "  Puerto Rico!!Percent Male!!Estimate  \\\n",
      "0                                 NaN   \n",
      "1                                 (X)   \n",
      "2                                8.8%   \n",
      "3                               30.4%   \n",
      "4                               53.3%   \n",
      "\n",
      "  Puerto Rico!!Percent Male!!Margin of Error Puerto Rico!!Female!!Estimate  \\\n",
      "0                                        NaN                           NaN   \n",
      "1                                        (X)                       156,771   \n",
      "2                                       ±1.5                         8,510   \n",
      "3                                       ±2.4                        38,315   \n",
      "4                                       ±2.3                        91,107   \n",
      "\n",
      "  Puerto Rico!!Female!!Margin of Error Puerto Rico!!Percent Female!!Estimate  \\\n",
      "0                                  NaN                                   NaN   \n",
      "1                               ±2,385                                   (X)   \n",
      "2                               ±1,604                                  5.4%   \n",
      "3                               ±3,627                                 24.4%   \n",
      "4                               ±4,402                                 58.1%   \n",
      "\n",
      "  Puerto Rico!!Percent Female!!Margin of Error  \n",
      "0                                          NaN  \n",
      "1                                          (X)  \n",
      "2                                         ±1.0  \n",
      "3                                         ±2.3  \n",
      "4                                         ±2.6  \n",
      "\n",
      "[5 rows x 625 columns]\n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Label (Grouping)                                0\n",
      "Alabama!!Total!!Estimate                        4\n",
      "Alabama!!Total!!Margin of Error                 4\n",
      "Alabama!!Percent!!Estimate                      4\n",
      "Alabama!!Percent!!Margin of Error               4\n",
      "                                               ..\n",
      "Puerto Rico!!Percent Male!!Margin of Error      4\n",
      "Puerto Rico!!Female!!Estimate                   4\n",
      "Puerto Rico!!Female!!Margin of Error            4\n",
      "Puerto Rico!!Percent Female!!Estimate           4\n",
      "Puerto Rico!!Percent Female!!Margin of Error    4\n",
      "Length: 625, dtype: int64 \n",
      "\n",
      "\n",
      "Education Data from data/2017education.csv:\n",
      "                                  Label (Grouping) Alabama!!Total!!Estimate  \\\n",
      "0                    AGE BY EDUCATIONAL ATTAINMENT                      NaN   \n",
      "1                        Population 18 to 24 years                  471,079   \n",
      "2                   Less than high school graduate                   63,810   \n",
      "3      High school graduate (includes equivalency)                  161,311   \n",
      "4               Some college or associate's degree                  214,751   \n",
      "\n",
      "  Alabama!!Total!!Margin of Error Alabama!!Percent!!Estimate  \\\n",
      "0                             NaN                        NaN   \n",
      "1                          ±5,556                        (X)   \n",
      "2                          ±5,003                      13.5%   \n",
      "3                          ±6,371                      34.2%   \n",
      "4                          ±6,848                      45.6%   \n",
      "\n",
      "  Alabama!!Percent!!Margin of Error Alabama!!Male!!Estimate  \\\n",
      "0                               NaN                     NaN   \n",
      "1                               (X)                 234,658   \n",
      "2                              ±1.0                  37,085   \n",
      "3                              ±1.4                  90,367   \n",
      "4                              ±1.3                  95,174   \n",
      "\n",
      "  Alabama!!Male!!Margin of Error Alabama!!Percent Male!!Estimate  \\\n",
      "0                            NaN                             NaN   \n",
      "1                         ±3,611                             (X)   \n",
      "2                         ±3,295                           15.8%   \n",
      "3                         ±4,298                           38.5%   \n",
      "4                         ±5,328                           40.6%   \n",
      "\n",
      "  Alabama!!Percent Male!!Margin of Error Alabama!!Female!!Estimate  ...  \\\n",
      "0                                    NaN                       NaN  ...   \n",
      "1                                    (X)                   236,421  ...   \n",
      "2                                   ±1.3                    26,725  ...   \n",
      "3                                   ±2.0                    70,944  ...   \n",
      "4                                   ±2.0                   119,577  ...   \n",
      "\n",
      "  Puerto Rico!!Percent!!Estimate Puerto Rico!!Percent!!Margin of Error  \\\n",
      "0                            NaN                                   NaN   \n",
      "1                            (X)                                   (X)   \n",
      "2                           9.8%                                  ±1.1   \n",
      "3                          28.1%                                  ±1.8   \n",
      "4                          53.7%                                  ±1.9   \n",
      "\n",
      "  Puerto Rico!!Male!!Estimate Puerto Rico!!Male!!Margin of Error  \\\n",
      "0                         NaN                                NaN   \n",
      "1                     172,916                             ±2,593   \n",
      "2                      21,030                             ±2,966   \n",
      "3                      56,332                             ±4,378   \n",
      "4                      85,434                             ±4,850   \n",
      "\n",
      "  Puerto Rico!!Percent Male!!Estimate  \\\n",
      "0                                 NaN   \n",
      "1                                 (X)   \n",
      "2                               12.2%   \n",
      "3                               32.6%   \n",
      "4                               49.4%   \n",
      "\n",
      "  Puerto Rico!!Percent Male!!Margin of Error Puerto Rico!!Female!!Estimate  \\\n",
      "0                                        NaN                           NaN   \n",
      "1                                        (X)                       163,792   \n",
      "2                                       ±1.7                        11,847   \n",
      "3                                       ±2.5                        38,411   \n",
      "4                                       ±2.7                        95,428   \n",
      "\n",
      "  Puerto Rico!!Female!!Margin of Error Puerto Rico!!Percent Female!!Estimate  \\\n",
      "0                                  NaN                                   NaN   \n",
      "1                               ±2,443                                   (X)   \n",
      "2                               ±2,002                                  7.2%   \n",
      "3                               ±3,888                                 23.5%   \n",
      "4                               ±4,554                                 58.3%   \n",
      "\n",
      "  Puerto Rico!!Percent Female!!Margin of Error  \n",
      "0                                          NaN  \n",
      "1                                          (X)  \n",
      "2                                         ±1.2  \n",
      "3                                         ±2.4  \n",
      "4                                         ±2.6  \n",
      "\n",
      "[5 rows x 625 columns]\n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Label (Grouping)                                0\n",
      "Alabama!!Total!!Estimate                        4\n",
      "Alabama!!Total!!Margin of Error                 4\n",
      "Alabama!!Percent!!Estimate                      4\n",
      "Alabama!!Percent!!Margin of Error               4\n",
      "                                               ..\n",
      "Puerto Rico!!Percent Male!!Margin of Error      4\n",
      "Puerto Rico!!Female!!Estimate                   4\n",
      "Puerto Rico!!Female!!Margin of Error            4\n",
      "Puerto Rico!!Percent Female!!Estimate           4\n",
      "Puerto Rico!!Percent Female!!Margin of Error    4\n",
      "Length: 625, dtype: int64 \n",
      "\n",
      "\n",
      "Race Data from data/2016race.csv:\n",
      "  Title: Population Distribution by Race/Ethnicity | KFF\n",
      "0                                    Timeframe: 2016    \n",
      "1                                              Notes    \n",
      "2  The American Community Survey did not release ...    \n",
      "3                                                NaN    \n",
      "4  Population and demographic data on are based o...    \n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Title: Population Distribution by Race/Ethnicity | KFF    4\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "Race Data from data/2021race.csv:\n",
      "  Title: Population Distribution by Race/Ethnicity | KFF\n",
      "0                                    Timeframe: 2021    \n",
      "1                                              Notes    \n",
      "2  The American Community Survey did not release ...    \n",
      "3                                                NaN    \n",
      "4  Population and demographic data on are based o...    \n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Title: Population Distribution by Race/Ethnicity | KFF    4\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "Race Data from data/2017race.csv:\n",
      "  Title: Population Distribution by Race/Ethnicity | KFF\n",
      "0                                    Timeframe: 2017    \n",
      "1                                              Notes    \n",
      "2  The American Community Survey did not release ...    \n",
      "3                                                NaN    \n",
      "4  Population and demographic data on are based o...    \n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Title: Population Distribution by Race/Ethnicity | KFF    4\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "Race Data from data/2015race.csv:\n",
      "  Title: Population Distribution by Race/Ethnicity | KFF\n",
      "0                                    Timeframe: 2015    \n",
      "1                                              Notes    \n",
      "2  The American Community Survey did not release ...    \n",
      "3                                                NaN    \n",
      "4  Population and demographic data on are based o...    \n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Title: Population Distribution by Race/Ethnicity | KFF    4\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "Race Data from data/2014race.csv:\n",
      "  Title: Population Distribution by Race/Ethnicity | KFF\n",
      "0                                    Timeframe: 2014    \n",
      "1                                              Notes    \n",
      "2  The American Community Survey did not release ...    \n",
      "3                                                NaN    \n",
      "4  Population and demographic data on are based o...    \n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Title: Population Distribution by Race/Ethnicity | KFF    4\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "Race Data from data/2022race.csv:\n",
      "  Title: Population Distribution by Race/Ethnicity | KFF\n",
      "0                                    Timeframe: 2022    \n",
      "1                                              Notes    \n",
      "2  The American Community Survey did not release ...    \n",
      "3                                                NaN    \n",
      "4  Population and demographic data on are based o...    \n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Title: Population Distribution by Race/Ethnicity | KFF    4\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "Race Data from data/2018race.csv:\n",
      "  Title: Population Distribution by Race/Ethnicity | KFF\n",
      "0                                    Timeframe: 2018    \n",
      "1                                              Notes    \n",
      "2  The American Community Survey did not release ...    \n",
      "3                                                NaN    \n",
      "4  Population and demographic data on are based o...    \n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Title: Population Distribution by Race/Ethnicity | KFF    4\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "Race Data from data/2019race.csv:\n",
      "  Title: Population Distribution by Race/Ethnicity | KFF\n",
      "0                                    Timeframe: 2019    \n",
      "1                                              Notes    \n",
      "2  The American Community Survey did not release ...    \n",
      "3                                                NaN    \n",
      "4  Population and demographic data on are based o...    \n",
      "\n",
      "Missing Values in this DataFrame:\n",
      "Title: Population Distribution by Race/Ethnicity | KFF    4\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load SPM data and display structure\n",
    "SPM_df = pd.read_csv('data/SPM.csv')\n",
    "print(\"SPM Data Structure:\")\n",
    "print(SPM_df.head())\n",
    "print(\"\\nMissing Values in SPM Data:\")\n",
    "print(SPM_df.isna().sum())\n",
    "\n",
    "# Load foreign-born data and display structure\n",
    "foreignborn_df = pd.read_csv('data/foreignborn.csv', header=None)\n",
    "foreignborn_df.columns = foreignborn_df.iloc[0]  # Set the header to the first row\n",
    "foreignborn_df = foreignborn_df.drop(0)  # Drop the first row as it is now the header\n",
    "foreignborn_df.reset_index(drop=True, inplace=True)  # Reset the index if needed\n",
    "print(\"\\nForeign Born Data Structure:\")\n",
    "print(foreignborn_df.head())\n",
    "print(\"\\nMissing Values in Foreign Born Data:\")\n",
    "print(foreignborn_df.isna().sum())\n",
    "foreignborn_df = foreignborn_df.rename(columns={\"2022.0\": \"2022\", \"2010.0\": \"2010\"})\n",
    "\n",
    "# Function to load files matching a pattern\n",
    "def load_data_files(file_pattern, description):\n",
    "    files = glob.glob(file_pattern)\n",
    "    data_frames = {}\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            # Try loading with comma separator\n",
    "            df = pd.read_csv(file, sep=',', on_bad_lines='skip')\n",
    "            data_frames[file] = df\n",
    "            print(f\"\\n{description} Data from {file}:\")\n",
    "            print(df.head())\n",
    "            print(\"\\nMissing Values in this DataFrame:\")\n",
    "            print(df.isna().sum(), \"\\n\")\n",
    "        except pd.errors.ParserError:\n",
    "            print(f\"Parser error in {file}. Trying with a different delimiter or settings...\")\n",
    "            try:\n",
    "                # Try loading with a tab\n",
    "                df = pd.read_csv(file, sep='\\t', on_bad_lines='skip')\n",
    "                data_frames[file] = df\n",
    "                print(f\"\\n{description} Data from {file} (loaded with tab separator):\")\n",
    "                print(df.head())\n",
    "                print(\"\\nMissing Values in this DataFrame:\")\n",
    "                print(df.isna().sum(), \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load {file} even with alternate parsing: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error loading {file}: {e}\")\n",
    "    \n",
    "    return data_frames\n",
    "\n",
    "# Load education data \n",
    "education_dfs = load_data_files(\"data/*education.csv\", \"Education\")\n",
    "\n",
    "# Load race data\n",
    "race_dfs = load_data_files(\"data/*race.csv\", \"Race\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e521c",
   "metadata": {},
   "source": [
    "We also care about religion, political makeup, and whether certain abortion laws are in place (all of which are not in the census). We will find them different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7006199e",
   "metadata": {},
   "source": [
    "## RELIGION DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576fc93",
   "metadata": {},
   "source": [
    "We decided to determine people's **religiousness** based off of a state's adherence rate (number of people who adhere to their religion across 1000 ) which can be found at (https://www.thearda.com/us-religion/maps/us-state-maps)\n",
    "\n",
    "We accessed it by webscraping and saved it in `religion_data.csv` in our data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "60bf9ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UT', 'ND', 'DC', 'SD', 'MA', 'RI', 'MN', 'OK', 'WI', 'NY', 'NE', 'LA', 'IA', 'NM', 'PA', 'CT', 'NJ', 'AR', 'TX', 'IL', 'AL', 'MS', 'KY', 'MO', 'TN', 'KS', 'ID', 'NH', 'SC', 'WY', 'CA', 'NC', 'OH', 'GA', 'MT', 'MD', 'IN', 'MI', 'VA', 'FL', 'DE', 'AZ', 'CO', 'VT', 'ME', 'HI', 'WV', 'AK', 'NV', 'WA', 'OR']\n",
      "51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kg/wxm8v9d91mv575kb5wsgxf4h0000gp/T/ipykernel_28808/2974073243.py:12: DeprecationWarning:\n",
      "\n",
      "The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url2020 = \"https://www.thearda.com/us-religion/maps/us-state-maps\"\n",
    "url2010 = \"https://www.thearda.com/us-religion/maps/us-state-maps?color=orange&m1=2_2_9999_2010\"\n",
    "url2000 = \"https://www.thearda.com/us-religion/maps/us-state-maps?color=orange&m1=2_2_9999_2000\"\n",
    "response = requests.get(url2000)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    \n",
    "script_tag = soup.find('script', text=re.compile(r'usa_map_div299992000_data = '))\n",
    "script_content = script_tag.string\n",
    "start_index = script_content.find('usa_map_div299992000_data =')\n",
    "semicolon_index = script_content.find(';', start_index)\n",
    "mapData = script_content[start_index:semicolon_index]\n",
    "\n",
    "quoted_strings = re.findall(r'\"(.*?)\"', mapData)\n",
    "values_strings = re.findall(r'(\\d+\\.\\d+)', mapData)\n",
    "year = []\n",
    "for i in range(len(values_strings)):\n",
    "    year.append(2000)\n",
    "\n",
    "last_two_chars = [s[-2:] for s in quoted_strings]\n",
    "print(last_two_chars)\n",
    "print(len(values_strings))\n",
    "\n",
    "file_paths = [\n",
    "    'data/AllReligionAdherence_2000.csv',\n",
    "    'data/AllReligionAdherence_2010.csv',\n",
    "    'data/AllReligionAdherence_2020.csv'\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    year = file_name.split('_')[-1].split('.')[0]\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Year'] = year\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "33dcca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State                      0\n",
      "Adherence Rate per 1000    0\n",
      "Year                       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adherence Rate per 1000</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>153.000000</td>\n",
       "      <td>153.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>489.381569</td>\n",
       "      <td>2010.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>101.640693</td>\n",
       "      <td>8.19178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>272.190000</td>\n",
       "      <td>2000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>414.860000</td>\n",
       "      <td>2000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>492.770000</td>\n",
       "      <td>2010.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>555.040000</td>\n",
       "      <td>2020.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>791.060000</td>\n",
       "      <td>2020.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Adherence Rate per 1000        Year\n",
       "count               153.000000   153.00000\n",
       "mean                489.381569  2010.00000\n",
       "std                 101.640693     8.19178\n",
       "min                 272.190000  2000.00000\n",
       "25%                 414.860000  2000.00000\n",
       "50%                 492.770000  2010.00000\n",
       "75%                 555.040000  2020.00000\n",
       "max                 791.060000  2020.00000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "religion_data = pd.read_csv('data/religion_data.csv')\n",
    "print(religion_data.isna().sum(axis=0))\n",
    "religion_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff1449",
   "metadata": {},
   "source": [
    "Things to Consider:\n",
    "\n",
    "**Missingness**: After webscraping ARDA, there are no missing values in any cells, but definitely in the number of years (153 vs 451 above). This is because this index is only calculated every 10 years (2000, 2010, 2020). We have to decide how we want to impute this data. One option is to create a line going through the 2000, 2010, and 2020 datapoints, and then impute the data that would sit on this line for 2011-2019 and 2021-2022. \n",
    "\n",
    "**Imbalance**: Although the actual number of state datapoints are equal and fine, the source of the imbalance stems from which religions do these data sets stem from. All the main categories listed on the Association of Religious Digital Archives are branches of Christianity. I have no idea if these are actually the most popular religions in the US or is the source biased towards it.\n",
    "\n",
    "**Scaling**: Since adherence rates range widely (e.g., from 272.19 to 791.06), standardizing or normalizing values might be beneficial, particularly if this data will be used for machine learning or statistical modeling. Standardization (e.g., Z-score) could be applied if you want each adherence rate centered around zero, while min-max normalization scales values between a range like [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b77098",
   "metadata": {},
   "source": [
    "## POLITICS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ce4a9",
   "metadata": {},
   "source": [
    "We decided to determine people's political orienation based off of the Cook Partisan Voting Index (Cook PVI), which is a measure of each state's political leaning relative to the nation as a whole.\n",
    "\n",
    "The primary challenge with using this index is that the methodology was switched in 2022 to weigh the last presidential election 0.75 and the second to last 0.25, as opposed to the even (50/50) weighting of both years that was used in prior years. We will either figure out how to reweight the outcomes based on the raw data if we can get it or exclude 2022 from our analysis.\n",
    "\n",
    "The calculation of the index is described in more detail here: https://www.cookpolitical.com/cook-pvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1712e09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State           0\n",
      "2022_PVI        0\n",
      "2020_Biden      0\n",
      "2020_Trump      0\n",
      "2016_Clinton    0\n",
      "2016_Trump      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>2022_PVI</th>\n",
       "      <th>2020_Biden</th>\n",
       "      <th>2020_Trump</th>\n",
       "      <th>2016_Clinton</th>\n",
       "      <th>2016_Trump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>51</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>R+11</td>\n",
       "      <td>49.50%</td>\n",
       "      <td>48.80%</td>\n",
       "      <td>27.50%</td>\n",
       "      <td>45.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          State 2022_PVI 2020_Biden 2020_Trump 2016_Clinton 2016_Trump\n",
       "count        51       51         51         51           51         51\n",
       "unique       51       30         50         48           48         48\n",
       "top     Alabama     R+11     49.50%     48.80%       27.50%     45.50%\n",
       "freq          1        3          2          2            2          2"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_df = pd.read_csv('data/cook_pvi_2022.csv')\n",
    "print(pol_df.isna().sum(axis=0))\n",
    "pol_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed61383",
   "metadata": {},
   "source": [
    "**Missingness and imbalance**: None in the dataset \n",
    "\n",
    "**Scaling:** We will follow a similar methodology to that used in \"State-level Political Partisanship Strongly Correlates with Health Outcomes for US Children,\" (full citation below).* We converted the PVI's to numerical values, with negative values representing Democratic PVIs and positive numbers representing Republican ones (an arbitrary choice). Then, we scaled those values with sklearn's MinMaxScaler() so that states have a rating between 0 and 1 representing how conservative they are, with 1 being most conservatve and 0 being most liberal.\n",
    "\n",
    "We accessed the data for 2022 from this source: https://datawrapper.dwcdn.net/0djXs/2/ and saved it in cook_pvi_2022.csv in our data folder. We tried scraping the website but the data is not in the html but instead pulled from a source so we were unable to access the data using the same strategy as HW 1.\n",
    "\n",
    "The 2022 data is saved in `data/cook_pvi_2022.csv.`\n",
    "\n",
    "*Paul, M., Zhang, R., Liu, B. et al. State-level political partisanship strongly correlates with health outcomes for US children. Eur J Pediatr 181, 273–280 (2022). https://doi.org/10.1007/s00431-021-04203-y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0a8dc",
   "metadata": {},
   "source": [
    "## ABORTION DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98418d",
   "metadata": {},
   "source": [
    "We decided to determine state's **abortion laws** based off of how late into pregnancy, abortion is legally allowed which can be found at https://lawatlas.org/datasets/abortion-bans. We chose this dataset because it is the only one on the internet showing abortion bans in the 2014-2022 time frame and how they change (most just show abortion bans now)\n",
    "\n",
    "We accessed it by downloading it, converting from xlsx to csv, and saved it in `abortion_data.csv` in our data folder\n",
    "\n",
    "The main thing to consider is that **just because abortion is legal, doesn't mean it is accessible**. Many states may technically allow abortion but only have one clinic, so its not attainable. That said, we have chosen this metric (when is abortion legal), to coincide with current political debate about whether abortion should be legalized. \n",
    "\n",
    "There will be significant preprocessing required as the data is in format `Effective Date`, `Valid Through Date` for each law which must simply be converted into year (whichever law was the majority of the year), and each ban `6 weeks`, `8 weeks` etc is categorical! It would make more sense to simply make a variable listing the latest week aborition is legal (0,6,8,12,52 etc).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "577d87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "abortion_data = pd.read_csv('data/abortion_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1aafc7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>latest_abortion</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>20</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>20</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>20</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>40</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state  latest_abortion  year\n",
       "0  Alabama               20  2018\n",
       "2  Alabama               20  2019\n",
       "3  Alabama               20  2022\n",
       "4   Alaska               40  2018\n",
       "5  Arizona               18  2018"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"State\",\"Effective Date\",\"Valid Through Date\",\"Bans_gest_4 weeks postfertilization (6 weeks LMP) \" ,\"Bans_gest_6 weeks postfertilization (8 weeks LMP) \" ,\"Bans_gest_8 weeks postfertilization (10 weeks LMP)\",\"Bans_gest_10 weeks postfertilization (12 weeks LMP) \" ,\"Bans_gest_12 weeks postfertilization (14 weeks LMP)\",\"Bans_gest_13 weeks postfertilization (15 weeks LMP)\",\"Bans_gest_16 weeks postfertilization (18 weeks LMP) \",\"Bans_gest_18 weeks postfertilization (20 weeks LMP)\",\"Bans_gest_19 weeks postfertilization (21 weeks LMP)\",\"Bans_gest20 weeks postfertilization (22 weeks LMP)\",\"Bans_gest_21 weeks postfertilization (23 weeks LMP) \" ,\"Bans_gest_22 weeks postfertilization (24 weeks LMP)\",\"Bans_gest_24 weeks postfertilization (26 weeks LMP)\",\"Bans_gestViability\",\"Bans_gest_Fetus is capable of feeling pain\",\"Bans_gest_3rd trimester\"]\n",
    "abortion_data = abortion_data[columns]\n",
    "new_names = {\n",
    "    \"State\": \"state\",\n",
    "    \"Effective Date\": \"start\",\n",
    "    \"Valid Through Date\": \"end\",\n",
    "    \"Bans_gest_4 weeks postfertilization (6 weeks LMP) \": \"4\",\n",
    "    \"Bans_gest_6 weeks postfertilization (8 weeks LMP) \": \"6\",\n",
    "    \"Bans_gest_8 weeks postfertilization (10 weeks LMP)\": \"8\",\n",
    "    \"Bans_gest_10 weeks postfertilization (12 weeks LMP) \": \"10\",\n",
    "    \"Bans_gest_12 weeks postfertilization (14 weeks LMP)\": \"12\",\n",
    "    \"Bans_gest_13 weeks postfertilization (15 weeks LMP)\": \"13\",\n",
    "    \"Bans_gest_16 weeks postfertilization (18 weeks LMP) \": \"16\",\n",
    "    \"Bans_gest_18 weeks postfertilization (20 weeks LMP)\": \"18\",\n",
    "    \"Bans_gest_19 weeks postfertilization (21 weeks LMP)\": \"19\",\n",
    "    \"Bans_gest20 weeks postfertilization (22 weeks LMP)\": \"20\",\n",
    "    \"Bans_gest_21 weeks postfertilization (23 weeks LMP) \": \"21\",\n",
    "    \"Bans_gest_22 weeks postfertilization (24 weeks LMP)\": \"22\",\n",
    "    \"Bans_gest_24 weeks postfertilization (26 weeks LMP)\": \"24\",\n",
    "    \"Bans_gestViability\": \"24\", #chose via google\n",
    "    \"Bans_gest_Fetus is capable of feeling pain\": \"25\", #chose via google\n",
    "    \"Bans_gest_3rd trimester\": \"28\" #chose via google\n",
    "}\n",
    "abortion_data = abortion_data.rename(columns=new_names)\n",
    "\n",
    "abortion_processed = abortion_data[['state', 'start', 'end']].copy()\n",
    "abortion_processed['latest_abortion'] = abortion_data[abortion_data.columns].apply(\n",
    "    lambda row: next((int(col) for col in abortion_data.columns  if str(row[col]) == \"1\"), 40), axis=1)\n",
    "\n",
    "abortion_processed['start'] = pd.to_datetime(abortion_processed['start'])\n",
    "abortion_processed['end'] = pd.to_datetime(abortion_processed['end'])\n",
    "abortion_processed['year'] = abortion_processed['start'].dt.year\n",
    "abortion_processed['length'] = (abortion_processed['end'] - abortion_processed['start']).dt.days\n",
    "abortion_processed = abortion_processed.loc[abortion_processed.groupby(['state', 'year'])['length'].idxmax()]\n",
    "abortion_processed = abortion_processed.drop(columns=['start', 'end', 'length'])\n",
    "\n",
    "abortion_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f257f2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state              0\n",
      "latest_abortion    0\n",
      "year               0\n",
      "dtype: int64\n",
      "missing from entire dataframe 113\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latest_abortion</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>137.000000</td>\n",
       "      <td>137.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>25.598540</td>\n",
       "      <td>2019.729927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.328758</td>\n",
       "      <td>1.647206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2018.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>2018.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>2019.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>2021.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>2022.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       latest_abortion         year\n",
       "count       137.000000   137.000000\n",
       "mean         25.598540  2019.729927\n",
       "std          10.328758     1.647206\n",
       "min           4.000000  2018.000000\n",
       "25%          20.000000  2018.000000\n",
       "50%          20.000000  2019.000000\n",
       "75%          40.000000  2021.000000\n",
       "max          40.000000  2022.000000"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(abortion_processed.isna().sum(axis=0))\n",
    "print(\"missing from entire dataframe\", 50*5-len(abortion_processed))\n",
    "abortion_processed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de58eb7",
   "metadata": {},
   "source": [
    "Things to Consider:\n",
    "\n",
    "**Missingness**: We see while there are no null cells in our dataset, there are 113 \"missing cells\" from what would be expected (one for every state for every year). This is simply because of how our dataset was made -- there is only a new row if a new law is enacted (not one per year). Thus, if we are missing a row for a certain year, we can simply **impute** the data by copying the previous row. \n",
    "\n",
    "**Imbalance**: There is no imbalance as there is no population sampling. \n",
    "\n",
    "**Scaling**: We don't need to scale the data by itself but might scale data as a whole later so they don't have stds that are too high. \n",
    "\n",
    "**Other**: We decided to change our data from a 1-hot encoded dataset, to 1 quantitative varaible. We did this because there seems to be a clear relatonship between the previously encoded categories (a 10 week ban is stronger than a 15 week ban and less strong than a 4 week ban). Thus, we made it ordinal. In additon, note that if there was no abortion ban, we arbitrarily set this ordinal variable to 40, the length of the average pregancy. It might make some sense to add an **indicator variable** to be 0 if there is no abortion ban at all (40 weeks) and 1 if there is an abortion ban. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d87cb5b",
   "metadata": {},
   "source": [
    "Some issues we preliminary have considered before even inspecting the data includes:\n",
    "\n",
    "\n",
    "- **under reporting immigration status**: via google, people tend to underreport whether they are immigrants. This is potentially a missingness issue\n",
    "\n",
    "-  **multicollinearity**: There is most likely a relationship between racial background / household income and an individual's birth country (immigration status) so we can't use both as predictors in the same equation. We don't know how strong this correlation will be so are not worried yet, but would love to talk about it with a TF\n",
    "\n",
    "- **is this just too much data??**: looking at each of these attributes for each state for each year may just be too many dimensions. Is there really a big difference accross years? Should we just look at 1? If so, which? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b1745",
   "metadata": {},
   "source": [
    "Now we are going to import and inspect each dataset, looking for missingness, imbalance, and scaling issues!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef9b79",
   "metadata": {},
   "source": [
    "## 3. Understand and describe the preprocessing required such that data is in a form amenable to later downstream tasks such as visualizing and modeling, as is appropriate to the specific project goals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e8d54",
   "metadata": {},
   "source": [
    "One of our biggest considerations is that our predictor variables, X, is essentially 3d (reshaped), not 2d. We are looking at a bunch of predictors **over state** and **over time**. This might make our predictions complicated, especially because we are trying to see **why** fertility is changing over time (so the reason that there are less babies in 2022 can't simply be that it is 2022). Thus, it might make more sense to choose only **one year** to regress on. Also, it may be hard to **visualize** multiple years and states simultaniously as, for example, if we drew a map and colored each state by predictor category, we would have to choose one specific time to do so (and vice versa).  PLEASE LET US KNOW WHAT YOU THINK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0aff50",
   "metadata": {},
   "source": [
    "It also may make sense to just try to minimize predictors in general as we don't want to get too specific!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b047880",
   "metadata": {},
   "source": [
    "### This is how we envision generally preprocessing..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0171592b",
   "metadata": {},
   "source": [
    "As you can see, we generally started preprocessign above. For example, we changed abortion laws from categorical variables into an orderinal variable. In addition, as seen below, we standardized the poltical data by scaling it between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5c084",
   "metadata": {},
   "source": [
    "Below is the preprocessing for political data. We scale the Cook PVI scores to be on a scale of 0 to 1, where 1 is most conservative and 0 is least conservative. We drop DC because it is not a state. There is no missingness in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3ea028b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(-16)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop DC because it's not a state: \n",
    "pol_df = pol_df[pol_df['State'] != 'District of Columbia']\n",
    "pol_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "unscaled_ratings=[]\n",
    "for i in range(len(pol_df)):\n",
    "    magnitude = re.search(r\"(?<=\\+).*\", pol_df['2022_PVI'][i])\n",
    "    magnitude = int(magnitude.group())\n",
    "    \n",
    "    if pol_df['2022_PVI'][i][0] == 'R':\n",
    "        unscaled_ratings.append(magnitude)\n",
    "    else:\n",
    "        unscaled_ratings.append(magnitude * -1)\n",
    "\n",
    "pol_df['unscaled_rating'] = unscaled_ratings\n",
    "\n",
    "pol_rating_scaler = MinMaxScaler()\n",
    "scaled_pol_ratings = pol_rating_scaler.fit_transform(pol_df['unscaled_rating'].values.reshape(-1, 1))\n",
    "\n",
    "pol_df['scaled_rating'] = scaled_pol_ratings\n",
    "pol_df.head()\n",
    "\n",
    "pol_df['unscaled_rating'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52a21a",
   "metadata": {},
   "source": [
    "To prepare the education dataset for analysis, we began by iterating through each *education.csv file. We extracted the year from each file name to ensure we could track data across different time periods. Within each file, we removed the rows labeled \"Population 25 years and over\" along with the following nine rows, as they did not provide useful information for our analysis. We then isolated columns relevant to our study, retaining only the primary label column and columns ending with \"!!Percent Females!!Estimate,\" which represent the estimated educational attainment percentages for females in each state.\n",
    "\n",
    "For each age group (e.g., \"Population 18 to 24 years\"), we calculated weighted values for educational attainment percentages based on the total population for that group. This scaling ensures that educational attainment percentages are accurately reflected relative to each age group’s size, enabling meaningful comparisons across states. After processing each file, we created a consolidated data frame containing columns for Label, Year, State, Female Estimate (weighted educational attainment percentage), and Population. Finally, we dropped rows where \"Female Estimate\" values were missing, resulting in a cleaned and structured education_df ready for analysis.\n",
    "\n",
    "For the race dataset, we followed a similar iterative approach, processing each *race.csv file individually and extracting the year from each file name. Each file was loaded while skipping the first two lines to focus only on relevant data, and we added a Year column to track temporal changes. To simplify the dataset, we removed columns representing less relevant racial categories (such as “American Indian or Alaska Native,” “Native Hawaiian or Pacific Islander”) and redundant summary columns (like “Total” and “Footnotes”). We also excluded rows containing data for \"United States\" as an aggregate, ensuring that only state-level data remained. Finally, we dropped any rows where \"White\" values were missing, creating a clean and consistent structure across files. The processed race data frames were combined into a single race_df for use in further analysis.\n",
    "\n",
    "These steps produced two organized, cleaned data frames, education_df and race_df, each containing essential demographic details by year and state. The education data is now scaled appropriately by age group population, and both datasets are free of unnecessary or missing values, allowing for more accurate analysis of the relationships between education, race, and fertility rates across states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8041a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "# Process each education file\n",
    "for file in education_dfs:\n",
    "    # Get year from the file name\n",
    "    year = file.split('/')[-1][:4]\n",
    "\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Drop \"Population 25 years and over\" and the nine rows following it\n",
    "    pop_25_index = df[df.iloc[:, 0] == \"Population 25 years and over\"].index\n",
    "    if not pop_25_index.empty:\n",
    "        df = df.drop(index=range(pop_25_index[0], pop_25_index[0] + 10)).reset_index(drop=True)\n",
    "\n",
    "    # Keep only the first column (label) and columns ending with \"!!Percent Females!!Estimate\"\n",
    "    label_column = df.columns[0]\n",
    "    columns_to_keep = [label_column] + [col for col in df.columns if col.endswith(\"!!Percent Females!!Estimate\")]\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    # Process each age group section to calculate weighted averages\n",
    "    selected_data = []\n",
    "    current_population = None\n",
    "    total_population = 0 \n",
    "    for index, row in df.iterrows():\n",
    "        label = row[label_column]\n",
    "\n",
    "        # Check if row is a population group \n",
    "        if \"Population\" in label:\n",
    "            # Get population number and set as title\n",
    "            try:\n",
    "                total_population = int(label.split()[1]) if label.split()[1].isdigit() else None\n",
    "            except ValueError:\n",
    "                total_population = None\n",
    "            current_population = total_population \n",
    "        elif current_population and \"Percent\" not in label:\n",
    "            # Calculate weighted value for educational attainment percentages based on the total population\n",
    "            for col in df.columns[1:]:  # Exclude the label column\n",
    "                state = col.split(\"!!\")[0]\n",
    "                percent_str = row[col]\n",
    "                try:\n",
    "                    percent = float(str(percent_str).replace(\"%\", \"\").strip()) if percent_str else None\n",
    "                except ValueError:\n",
    "                    percent = None  # Set as None if conversion fails\n",
    "\n",
    "                if percent is not None:\n",
    "                    scaled_value = (percent / 100) * total_population\n",
    "                else:\n",
    "                    scaled_value = None\n",
    "\n",
    "                # Append the result\n",
    "                selected_data.append({\n",
    "                    \"Label\": label,\n",
    "                    \"Year\": year,\n",
    "                    \"State\": state,\n",
    "                    \"Female Estimate\": scaled_value,\n",
    "                    \"Population\": total_population\n",
    "                })\n",
    "\n",
    "    #Append to df_list\n",
    "    temp_df = pd.DataFrame(selected_data)\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "# Combine all data framesZASxzdw\n",
    "education_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Drop rows NaN\n",
    "education_df.dropna(subset=[\"Female Estimate\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b52c5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "# Process each race file\n",
    "for file_path in race_dfs:\n",
    "    # Get the year\n",
    "    year = file_path.split('/')[-1].split('race')[0]\n",
    "    \n",
    "    try:\n",
    "        # Load the CSV\n",
    "        df = pd.read_csv(file_path, skiprows=2, delimiter=',')\n",
    "        \n",
    "        # Add the year\n",
    "        df['Year'] = year\n",
    "        \n",
    "        # Drop columns\n",
    "        columns_to_drop = ['American Indian or Alaska Native', 'Native Hawaiian or Pacific Islander', 'Total', 'Footnotes']\n",
    "        df = df.loc[:, ~df.columns.str.contains('|'.join(columns_to_drop), na=False)]\n",
    "        \n",
    "        # Remove rows with \"United States\" \n",
    "        df = df[~df.iloc[:, 0].str.strip().str.lower().eq(\"united states\")]\n",
    "        \n",
    "        # Remove rows where \"White\" is NaN\n",
    "        df = df.dropna(subset=[\"White\"])\n",
    "        \n",
    "        # Append\n",
    "        df_list.append(df)\n",
    "    \n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Could not parse {file_path}. Skipping this file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with file {file_path}: {e}\")\n",
    "\n",
    "# Concatenate\n",
    "race_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e68086",
   "metadata": {},
   "source": [
    "We will also **impute/delete missingness** as described in each section above. This is especially relevent when we are missing certain years.\n",
    "Finally, we will combine all our data into one DF and **make the state variable categorical.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885408a2",
   "metadata": {},
   "source": [
    "# Starting Milestone 3/4: Prelim Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe03f89",
   "metadata": {},
   "source": [
    "## First off, we impute all ourn misising data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa6aeb",
   "metadata": {},
   "source": [
    "We started off by imputing/cleaning the data as described above. Our goal is to have 450 rows per data frame (50 states, 9 years from 2014-2022) and them to combine them along state and year axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e2138",
   "metadata": {},
   "source": [
    "## Isabella Preprocessing -- to 450"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d8428",
   "metadata": {},
   "source": [
    "### Impute: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "28d7da8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Location</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Multiple Races</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0.6640</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.0180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>0.6200</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>0.5640</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.0210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>0.7360</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014</td>\n",
       "      <td>California</td>\n",
       "      <td>0.3830</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.0310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2022</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>0.8190</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.0430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2015</td>\n",
       "      <td>Montana</td>\n",
       "      <td>0.8655</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.0225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2016</td>\n",
       "      <td>Montana</td>\n",
       "      <td>0.8655</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.0225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2017</td>\n",
       "      <td>Montana</td>\n",
       "      <td>0.8655</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.0225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>2018</td>\n",
       "      <td>Montana</td>\n",
       "      <td>0.8655</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.0225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year    Location   White  Black  Hispanic  Asian  Multiple Races\n",
       "0    2014     Alabama  0.6640  0.263     0.039  0.012          0.0180\n",
       "1    2014      Alaska  0.6200  0.030     0.066  0.061          0.0740\n",
       "2    2014     Arizona  0.5640  0.039     0.305  0.031          0.0210\n",
       "3    2014    Arkansas  0.7360  0.153     0.070  0.012          0.0200\n",
       "4    2014  California  0.3830  0.054     0.388  0.139          0.0310\n",
       "..    ...         ...     ...    ...       ...    ...             ...\n",
       "445  2022     Wyoming  0.8190  0.007     0.109  0.006          0.0430\n",
       "446  2015     Montana  0.8655  0.008     0.035  0.009          0.0225\n",
       "447  2016     Montana  0.8655  0.008     0.035  0.009          0.0225\n",
       "448  2017     Montana  0.8655  0.008     0.035  0.009          0.0225\n",
       "449  2018     Montana  0.8655  0.008     0.035  0.009          0.0225\n",
       "\n",
       "[450 rows x 7 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of years and states\n",
    "years = list(range(2014, 2023))\n",
    "states = [\n",
    "    'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', \n",
    "    'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', \n",
    "    'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', \n",
    "    'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', \n",
    "    'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', \n",
    "    'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', \n",
    "    'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', \n",
    "    'Wisconsin', 'Wyoming'\n",
    "]\n",
    "\n",
    "# Define demographic columns\n",
    "demographic_columns = [\"White\", \"Black\", \"Hispanic\", \"Asian\", \"Multiple Races\"]\n",
    "\n",
    "# Create a DF with combinations of years and states\n",
    "all_combinations = pd.DataFrame([(year, state) for year in years for state in states], columns=['Year', 'Location'])\n",
    "\n",
    "all_combinations['Year'] = all_combinations['Year'].astype(int)\n",
    "race_df['Year'] = race_df['Year'].astype(int)\n",
    "\n",
    "\n",
    "# Merge all_combinations\n",
    "race_df_full = all_combinations.merge(race_df, on=['Year', 'Location'], how='left')\n",
    "\n",
    "# Convert to numeric\n",
    "for col in demographic_columns:\n",
    "    race_df_full[col] = pd.to_numeric(race_df_full[col], errors='coerce')\n",
    "\n",
    "# Impute function\n",
    "def impute_multiple_columns(row, df, columns):\n",
    "    for col in columns:\n",
    "        if pd.notnull(row[col]):\n",
    "            continue \n",
    "\n",
    "        prev_year_value = df[(df['Location'] == row['Location']) & (df['Year'] == row['Year'] - 1)][col].values\n",
    "        next_year_value = df[(df['Location'] == row['Location']) & (df['Year'] == row['Year'] + 1)][col].values\n",
    "        if prev_year_value.size > 0 and next_year_value.size > 0:\n",
    "            row[col] = (prev_year_value[0] + next_year_value[0]) / 2\n",
    "        elif prev_year_value.size > 0:\n",
    "            row[col] = prev_year_value[0]\n",
    "        elif next_year_value.size > 0:\n",
    "            row[col] = next_year_value[0]\n",
    "    return row\n",
    "\n",
    "# Impute\n",
    "race_df_full = race_df_full.apply(impute_multiple_columns, axis=1, df=race_df_full, columns=demographic_columns)\n",
    "\n",
    "# Drop rows\n",
    "race_df_full.dropna(subset=demographic_columns, inplace=True)\n",
    "\n",
    "# Fix montana\n",
    "missing_years = [2015, 2016, 2017, 2018]\n",
    "\n",
    "if not race_df_full[(race_df_full['Year'] == 2014) & (race_df_full['Location'] == 'Montana')].empty and \\\n",
    "   not race_df_full[(race_df_full['Year'] == 2019) & (race_df_full['Location'] == 'Montana')].empty:\n",
    "\n",
    "    montana_data_2014 = race_df_full[(race_df_full['Year'] == 2014) & (race_df_full['Location'] == 'Montana')].iloc[0]\n",
    "    montana_data_2019 = race_df_full[(race_df_full['Year'] == 2019) & (race_df_full['Location'] == 'Montana')].iloc[0]\n",
    "\n",
    "    montana_missing_rows = pd.DataFrame({'Year': missing_years, 'Location': 'Montana'})\n",
    "    for col in demographic_columns:\n",
    "        if col in montana_data_2014 and col in montana_data_2019:\n",
    "            avg_value = (montana_data_2014[col] + montana_data_2019[col]) / 2\n",
    "            montana_missing_rows[col] = avg_value\n",
    "        else:\n",
    "            montana_missing_rows[col] = np.nan  # Handle missing columns gracefully\n",
    "\n",
    "    race_df_full = pd.concat([race_df_full, montana_missing_rows], ignore_index=True)\n",
    "\n",
    "# Reset the index \n",
    "race_df_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "race_df_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed4054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>Bachelor's degree or higher_18</th>\n",
       "      <th>Bachelor's degree or higher_25</th>\n",
       "      <th>Bachelor's degree or higher_35</th>\n",
       "      <th>Bachelor's degree or higher_45</th>\n",
       "      <th>Bachelor's degree or higher_65</th>\n",
       "      <th>High school graduate (includes equivalency)_18</th>\n",
       "      <th>High school graduate (includes equivalency)_65</th>\n",
       "      <th>High school graduate or higher_25</th>\n",
       "      <th>High school graduate or higher_35</th>\n",
       "      <th>High school graduate or higher_45</th>\n",
       "      <th>High school graduate or higher_65</th>\n",
       "      <th>Less than high school graduate_18</th>\n",
       "      <th>Less than high school graduate_65</th>\n",
       "      <th>Some college or associate's degree_18</th>\n",
       "      <th>Some college or associate's degree_65</th>\n",
       "      <th>Bachelor's degree or higher_65</th>\n",
       "      <th>High school graduate or higher_65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1.692</td>\n",
       "      <td>7.300</td>\n",
       "      <td>10.710</td>\n",
       "      <td>11.025</td>\n",
       "      <td>11.180</td>\n",
       "      <td>5.094</td>\n",
       "      <td>12.610</td>\n",
       "      <td>22.250</td>\n",
       "      <td>31.185</td>\n",
       "      <td>39.285</td>\n",
       "      <td>51.415</td>\n",
       "      <td>2.340</td>\n",
       "      <td>23.660</td>\n",
       "      <td>8.874</td>\n",
       "      <td>9.620</td>\n",
       "      <td>17.355</td>\n",
       "      <td>56.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1.422</td>\n",
       "      <td>8.050</td>\n",
       "      <td>11.830</td>\n",
       "      <td>14.490</td>\n",
       "      <td>18.070</td>\n",
       "      <td>6.804</td>\n",
       "      <td>9.555</td>\n",
       "      <td>23.225</td>\n",
       "      <td>32.865</td>\n",
       "      <td>42.165</td>\n",
       "      <td>56.355</td>\n",
       "      <td>2.430</td>\n",
       "      <td>12.545</td>\n",
       "      <td>7.362</td>\n",
       "      <td>5.005</td>\n",
       "      <td>24.700</td>\n",
       "      <td>62.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>1.602</td>\n",
       "      <td>7.450</td>\n",
       "      <td>10.605</td>\n",
       "      <td>12.510</td>\n",
       "      <td>14.950</td>\n",
       "      <td>5.004</td>\n",
       "      <td>12.480</td>\n",
       "      <td>22.175</td>\n",
       "      <td>29.890</td>\n",
       "      <td>39.285</td>\n",
       "      <td>55.250</td>\n",
       "      <td>2.628</td>\n",
       "      <td>22.750</td>\n",
       "      <td>8.766</td>\n",
       "      <td>7.605</td>\n",
       "      <td>18.590</td>\n",
       "      <td>57.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>1.584</td>\n",
       "      <td>6.875</td>\n",
       "      <td>9.800</td>\n",
       "      <td>10.305</td>\n",
       "      <td>10.010</td>\n",
       "      <td>4.968</td>\n",
       "      <td>12.415</td>\n",
       "      <td>22.500</td>\n",
       "      <td>31.115</td>\n",
       "      <td>39.510</td>\n",
       "      <td>51.675</td>\n",
       "      <td>2.070</td>\n",
       "      <td>23.335</td>\n",
       "      <td>9.396</td>\n",
       "      <td>9.620</td>\n",
       "      <td>15.470</td>\n",
       "      <td>57.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014</td>\n",
       "      <td>California</td>\n",
       "      <td>2.070</td>\n",
       "      <td>9.375</td>\n",
       "      <td>12.950</td>\n",
       "      <td>13.815</td>\n",
       "      <td>16.380</td>\n",
       "      <td>4.806</td>\n",
       "      <td>10.985</td>\n",
       "      <td>22.200</td>\n",
       "      <td>28.875</td>\n",
       "      <td>36.765</td>\n",
       "      <td>50.635</td>\n",
       "      <td>1.836</td>\n",
       "      <td>17.940</td>\n",
       "      <td>9.288</td>\n",
       "      <td>7.670</td>\n",
       "      <td>21.060</td>\n",
       "      <td>55.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2022</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>2.700</td>\n",
       "      <td>11.475</td>\n",
       "      <td>16.450</td>\n",
       "      <td>16.875</td>\n",
       "      <td>16.965</td>\n",
       "      <td>4.968</td>\n",
       "      <td>9.035</td>\n",
       "      <td>23.375</td>\n",
       "      <td>32.340</td>\n",
       "      <td>41.085</td>\n",
       "      <td>54.080</td>\n",
       "      <td>1.674</td>\n",
       "      <td>16.965</td>\n",
       "      <td>8.658</td>\n",
       "      <td>6.175</td>\n",
       "      <td>26.260</td>\n",
       "      <td>59.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2022</td>\n",
       "      <td>Washington</td>\n",
       "      <td>2.430</td>\n",
       "      <td>10.300</td>\n",
       "      <td>14.560</td>\n",
       "      <td>15.120</td>\n",
       "      <td>17.485</td>\n",
       "      <td>4.734</td>\n",
       "      <td>8.905</td>\n",
       "      <td>23.350</td>\n",
       "      <td>31.745</td>\n",
       "      <td>41.310</td>\n",
       "      <td>57.785</td>\n",
       "      <td>2.520</td>\n",
       "      <td>16.120</td>\n",
       "      <td>8.298</td>\n",
       "      <td>6.695</td>\n",
       "      <td>23.010</td>\n",
       "      <td>60.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2022</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>1.962</td>\n",
       "      <td>6.900</td>\n",
       "      <td>9.835</td>\n",
       "      <td>9.225</td>\n",
       "      <td>9.685</td>\n",
       "      <td>5.346</td>\n",
       "      <td>12.545</td>\n",
       "      <td>22.775</td>\n",
       "      <td>31.885</td>\n",
       "      <td>40.590</td>\n",
       "      <td>52.195</td>\n",
       "      <td>1.962</td>\n",
       "      <td>22.490</td>\n",
       "      <td>8.730</td>\n",
       "      <td>9.295</td>\n",
       "      <td>13.585</td>\n",
       "      <td>56.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2022</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2.430</td>\n",
       "      <td>10.075</td>\n",
       "      <td>14.035</td>\n",
       "      <td>13.095</td>\n",
       "      <td>13.715</td>\n",
       "      <td>5.058</td>\n",
       "      <td>7.995</td>\n",
       "      <td>23.625</td>\n",
       "      <td>32.830</td>\n",
       "      <td>42.480</td>\n",
       "      <td>57.915</td>\n",
       "      <td>1.710</td>\n",
       "      <td>18.070</td>\n",
       "      <td>8.784</td>\n",
       "      <td>6.695</td>\n",
       "      <td>20.735</td>\n",
       "      <td>61.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>2022</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>1.368</td>\n",
       "      <td>7.675</td>\n",
       "      <td>11.515</td>\n",
       "      <td>12.555</td>\n",
       "      <td>14.170</td>\n",
       "      <td>5.472</td>\n",
       "      <td>8.905</td>\n",
       "      <td>24.200</td>\n",
       "      <td>33.005</td>\n",
       "      <td>42.390</td>\n",
       "      <td>57.655</td>\n",
       "      <td>2.556</td>\n",
       "      <td>13.325</td>\n",
       "      <td>8.604</td>\n",
       "      <td>6.630</td>\n",
       "      <td>18.135</td>\n",
       "      <td>61.035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year          state      Bachelor's degree or higher_18  \\\n",
       "0    2014        Alabama                               1.692   \n",
       "1    2014         Alaska                               1.422   \n",
       "2    2014        Arizona                               1.602   \n",
       "3    2014       Arkansas                               1.584   \n",
       "4    2014     California                               2.070   \n",
       "..    ...            ...                                 ...   \n",
       "445  2022       Virginia                               2.700   \n",
       "446  2022     Washington                               2.430   \n",
       "447  2022  West Virginia                               1.962   \n",
       "448  2022      Wisconsin                               2.430   \n",
       "449  2022        Wyoming                               1.368   \n",
       "\n",
       "         Bachelor's degree or higher_25      Bachelor's degree or higher_35  \\\n",
       "0                                 7.300                              10.710   \n",
       "1                                 8.050                              11.830   \n",
       "2                                 7.450                              10.605   \n",
       "3                                 6.875                               9.800   \n",
       "4                                 9.375                              12.950   \n",
       "..                                  ...                                 ...   \n",
       "445                              11.475                              16.450   \n",
       "446                              10.300                              14.560   \n",
       "447                               6.900                               9.835   \n",
       "448                              10.075                              14.035   \n",
       "449                               7.675                              11.515   \n",
       "\n",
       "         Bachelor's degree or higher_45      Bachelor's degree or higher_65  \\\n",
       "0                                11.025                              11.180   \n",
       "1                                14.490                              18.070   \n",
       "2                                12.510                              14.950   \n",
       "3                                10.305                              10.010   \n",
       "4                                13.815                              16.380   \n",
       "..                                  ...                                 ...   \n",
       "445                              16.875                              16.965   \n",
       "446                              15.120                              17.485   \n",
       "447                               9.225                               9.685   \n",
       "448                              13.095                              13.715   \n",
       "449                              12.555                              14.170   \n",
       "\n",
       "         High school graduate (includes equivalency)_18  \\\n",
       "0                                                5.094    \n",
       "1                                                6.804    \n",
       "2                                                5.004    \n",
       "3                                                4.968    \n",
       "4                                                4.806    \n",
       "..                                                 ...    \n",
       "445                                              4.968    \n",
       "446                                              4.734    \n",
       "447                                              5.346    \n",
       "448                                              5.058    \n",
       "449                                              5.472    \n",
       "\n",
       "         High school graduate (includes equivalency)_65  \\\n",
       "0                                               12.610    \n",
       "1                                                9.555    \n",
       "2                                               12.480    \n",
       "3                                               12.415    \n",
       "4                                               10.985    \n",
       "..                                                 ...    \n",
       "445                                              9.035    \n",
       "446                                              8.905    \n",
       "447                                             12.545    \n",
       "448                                              7.995    \n",
       "449                                              8.905    \n",
       "\n",
       "         High school graduate or higher_25  \\\n",
       "0                                   22.250   \n",
       "1                                   23.225   \n",
       "2                                   22.175   \n",
       "3                                   22.500   \n",
       "4                                   22.200   \n",
       "..                                     ...   \n",
       "445                                 23.375   \n",
       "446                                 23.350   \n",
       "447                                 22.775   \n",
       "448                                 23.625   \n",
       "449                                 24.200   \n",
       "\n",
       "         High school graduate or higher_35  \\\n",
       "0                                   31.185   \n",
       "1                                   32.865   \n",
       "2                                   29.890   \n",
       "3                                   31.115   \n",
       "4                                   28.875   \n",
       "..                                     ...   \n",
       "445                                 32.340   \n",
       "446                                 31.745   \n",
       "447                                 31.885   \n",
       "448                                 32.830   \n",
       "449                                 33.005   \n",
       "\n",
       "         High school graduate or higher_45  \\\n",
       "0                                   39.285   \n",
       "1                                   42.165   \n",
       "2                                   39.285   \n",
       "3                                   39.510   \n",
       "4                                   36.765   \n",
       "..                                     ...   \n",
       "445                                 41.085   \n",
       "446                                 41.310   \n",
       "447                                 40.590   \n",
       "448                                 42.480   \n",
       "449                                 42.390   \n",
       "\n",
       "         High school graduate or higher_65  \\\n",
       "0                                   51.415   \n",
       "1                                   56.355   \n",
       "2                                   55.250   \n",
       "3                                   51.675   \n",
       "4                                   50.635   \n",
       "..                                     ...   \n",
       "445                                 54.080   \n",
       "446                                 57.785   \n",
       "447                                 52.195   \n",
       "448                                 57.915   \n",
       "449                                 57.655   \n",
       "\n",
       "         Less than high school graduate_18  \\\n",
       "0                                    2.340   \n",
       "1                                    2.430   \n",
       "2                                    2.628   \n",
       "3                                    2.070   \n",
       "4                                    1.836   \n",
       "..                                     ...   \n",
       "445                                  1.674   \n",
       "446                                  2.520   \n",
       "447                                  1.962   \n",
       "448                                  1.710   \n",
       "449                                  2.556   \n",
       "\n",
       "         Less than high school graduate_65  \\\n",
       "0                                   23.660   \n",
       "1                                   12.545   \n",
       "2                                   22.750   \n",
       "3                                   23.335   \n",
       "4                                   17.940   \n",
       "..                                     ...   \n",
       "445                                 16.965   \n",
       "446                                 16.120   \n",
       "447                                 22.490   \n",
       "448                                 18.070   \n",
       "449                                 13.325   \n",
       "\n",
       "         Some college or associate's degree_18  \\\n",
       "0                                        8.874   \n",
       "1                                        7.362   \n",
       "2                                        8.766   \n",
       "3                                        9.396   \n",
       "4                                        9.288   \n",
       "..                                         ...   \n",
       "445                                      8.658   \n",
       "446                                      8.298   \n",
       "447                                      8.730   \n",
       "448                                      8.784   \n",
       "449                                      8.604   \n",
       "\n",
       "         Some college or associate's degree_65  \\\n",
       "0                                        9.620   \n",
       "1                                        5.005   \n",
       "2                                        7.605   \n",
       "3                                        9.620   \n",
       "4                                        7.670   \n",
       "..                                         ...   \n",
       "445                                      6.175   \n",
       "446                                      6.695   \n",
       "447                                      9.295   \n",
       "448                                      6.695   \n",
       "449                                      6.630   \n",
       "\n",
       "             Bachelor's degree or higher_65  \\\n",
       "0                                    17.355   \n",
       "1                                    24.700   \n",
       "2                                    18.590   \n",
       "3                                    15.470   \n",
       "4                                    21.060   \n",
       "..                                      ...   \n",
       "445                                  26.260   \n",
       "446                                  23.010   \n",
       "447                                  13.585   \n",
       "448                                  20.735   \n",
       "449                                  18.135   \n",
       "\n",
       "             High school graduate or higher_65  \n",
       "0                                       56.615  \n",
       "1                                       62.205  \n",
       "2                                       57.590  \n",
       "3                                       57.135  \n",
       "4                                       55.315  \n",
       "..                                         ...  \n",
       "445                                     59.800  \n",
       "446                                     60.970  \n",
       "447                                     56.940  \n",
       "448                                     61.295  \n",
       "449                                     61.035  \n",
       "\n",
       "[450 rows x 19 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpore DF\n",
    "education_df['combined_label'] = education_df['Label'] + \"_\" + education_df['Population'].astype(str)\n",
    "education_df['year'] = education_df['Year'] \n",
    "education_df['state'] = education_df['State'] \n",
    "education_pivoted = education_df.pivot_table(\n",
    "    index=[\"year\", \"state\"],\n",
    "    columns=\"combined_label\",\n",
    "    values=\"Female Estimate\",\n",
    "    aggfunc='first'  # Assumes each (year, state, combined_label) combination is unique\n",
    ")\n",
    "education_pivoted.columns = education_pivoted.columns\n",
    "education_pivoted.reset_index(inplace=True)\n",
    "education_pivoted\n",
    "\n",
    "# Create a DataFrame with all year-state combinations\n",
    "years = list(range(2014, 2023))\n",
    "states = [\n",
    "    'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', \n",
    "    'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', \n",
    "    'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', \n",
    "    'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', \n",
    "    'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', \n",
    "    'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', \n",
    "    'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', \n",
    "    'Wisconsin', 'Wyoming'\n",
    "]\n",
    "\n",
    "all_combinations = pd.DataFrame([(year, state) for year in years for state in states], columns=['year', 'state'])\n",
    "education_complete = all_combinations.merge(education_pivoted, on=['year', 'state'], how='left')\n",
    "\n",
    "# Identify columns to impute\n",
    "columns_to_impute = [col for col in education_complete.columns if col not in ['year', 'state']]\n",
    "\n",
    "# Define imputation function\n",
    "def impute_column(df, column):\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.isnull(row[column]):\n",
    "            state, year = row['state'], row['year']\n",
    "            \n",
    "            # Get values\n",
    "            prev_years = df[(df['state'] == state) & (df['year'] < year)][['year', column]].dropna().sort_values('year', ascending=False)\n",
    "            next_years = df[(df['state'] == state) & (df['year'] > year)][['year', column]].dropna().sort_values('year', ascending=True)\n",
    "            \n",
    "            # Use closest available value\n",
    "            prev_value = prev_years[column].values[0] if not prev_years.empty else None\n",
    "            next_value = next_years[column].values[0] if not next_years.empty else None\n",
    "            \n",
    "            # Impute\n",
    "            if prev_value is not None and next_value is not None:\n",
    "                df.at[i, column] = (prev_value + next_value) / 2\n",
    "            elif prev_value is not None:\n",
    "                df.at[i, column] = prev_value\n",
    "            elif next_value is not None:\n",
    "                df.at[i, column] = next_value\n",
    "\n",
    "# Apply imputation\n",
    "for column in columns_to_impute:\n",
    "    impute_column(education_complete, column)\n",
    "\n",
    "\n",
    "# Reset index\n",
    "education_complete.dropna(inplace=True)\n",
    "education_complete.reset_index(drop=True, inplace=True)\n",
    "\n",
    "education_complete\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1a833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>SPM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>19.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>18.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>17.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2018</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2019</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2020</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2021</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>2022</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year    state   SPM\n",
       "0    2014  Alabama  19.3\n",
       "1    2015  Alabama  18.7\n",
       "2    2016  Alabama  17.1\n",
       "3    2017  Alabama  16.6\n",
       "4    2018  Alabama  16.6\n",
       "..    ...      ...   ...\n",
       "445  2018  Wyoming  11.0\n",
       "446  2019  Wyoming   9.8\n",
       "447  2020  Wyoming  10.5\n",
       "448  2021  Wyoming  11.2\n",
       "449  2022  Wyoming  11.4\n",
       "\n",
       "[450 rows x 3 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns\n",
    "SPM_df = SPM_df.rename(columns={\n",
    "    \"Year\": \"year\",\n",
    "    \"States\": \"state\",\n",
    "    \"Estimate\": \"SPM\"\n",
    "})\n",
    "\n",
    "# Filter for states and years\n",
    "SPM_df = SPM_df[(SPM_df['state'].isin(states)) & (SPM_df['year'].between(2014, 2022))]\n",
    "\n",
    "# Convert to numeric\n",
    "SPM_df['SPM'] = pd.to_numeric(SPM_df['SPM'], errors='coerce')\n",
    "\n",
    "# Find missing rows\n",
    "states_missing_2020 = set(SPM_df[SPM_df['year'] == 2020]['state'])\n",
    "states_all = set(SPM_df['state'].unique())\n",
    "missing_states_2020 = states_all - states_missing_2020\n",
    "\n",
    "imputed_rows = []\n",
    "\n",
    "for state in missing_states_2020:\n",
    "    # Get the SPM values for 2019 and 2021\n",
    "    spm_2019 = SPM_df[(SPM_df['state'] == state) & (SPM_df['year'] == 2019)]['SPM'].values\n",
    "    spm_2021 = SPM_df[(SPM_df['state'] == state) & (SPM_df['year'] == 2021)]['SPM'].values\n",
    "    \n",
    "    # Calculate the average\n",
    "    if spm_2019.size > 0 and spm_2021.size > 0:\n",
    "        spm_2020 = (spm_2019[0] + spm_2021[0]) / 2\n",
    "        imputed_rows.append({'year': 2020, 'state': state, 'SPM': spm_2020})\n",
    "\n",
    "# Imputed rows\n",
    "imputed_df = pd.DataFrame(imputed_rows)\n",
    "SPM_df = pd.concat([SPM_df, imputed_df], ignore_index=True)\n",
    "\n",
    "# Sort\n",
    "SPM_df = SPM_df.sort_values(by=['state', 'year']).reset_index(drop=True)\n",
    "\n",
    "# Display\n",
    "SPM_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a3c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kg/wxm8v9d91mv575kb5wsgxf4h0000gp/T/ipykernel_28808/4017145530.py:47: DeprecationWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>year</th>\n",
       "      <th>foreignborn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>3.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015</td>\n",
       "      <td>3.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2016</td>\n",
       "      <td>3.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2017</td>\n",
       "      <td>3.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2018</td>\n",
       "      <td>3.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2018</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2019</td>\n",
       "      <td>3.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2021</td>\n",
       "      <td>3.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2022</td>\n",
       "      <td>3.100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       state  year  foreignborn\n",
       "0    Alabama  2014        3.600\n",
       "1    Alabama  2015        3.625\n",
       "2    Alabama  2016        3.650\n",
       "3    Alabama  2017        3.675\n",
       "4    Alabama  2018        3.700\n",
       "..       ...   ...          ...\n",
       "445  Wyoming  2018        3.000\n",
       "446  Wyoming  2019        3.025\n",
       "447  Wyoming  2020        3.050\n",
       "448  Wyoming  2021        3.075\n",
       "449  Wyoming  2022        3.100\n",
       "\n",
       "[450 rows x 3 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns \n",
    "foreignborn_df = foreignborn_df.rename(columns={ \"State\": \"state\"})\n",
    "foreignborn_df.columns.values[1] = \"2022\"\n",
    "foreignborn_df.columns.values[2] = \"2010\"\n",
    "\n",
    "# reshape \n",
    "foreignborn_df_long = pd.melt(\n",
    "    foreignborn_df, \n",
    "    id_vars=[\"state\"],\n",
    "    value_vars=[\"2022\", \"2010\"],\n",
    "    var_name=\"year\",\n",
    "    value_name=\"foreignborn\"\n",
    ")\n",
    "\n",
    "# Convert year to integer\n",
    "foreignborn_df_long['year'] = foreignborn_df_long['year'].astype(int)\n",
    "\n",
    "# Create a DataFrame with all combinations of state and year\n",
    "years_to_impute = list(range(2010, 2023))\n",
    "states = foreignborn_df_long['state'].unique()\n",
    "all_combinations = pd.DataFrame([(state, year) for state in states for year in years_to_impute], columns=['state', 'year'])\n",
    "\n",
    "# Merge with the original data\n",
    "foreignborn_complete = pd.merge(all_combinations, foreignborn_df_long, on=['state', 'year'], how='left')\n",
    "\n",
    "# Function to fill in missing years\n",
    "def linear_impute(df):\n",
    "    start_value = df.loc[df['year'] == 2010, 'foreignborn'].values[0]\n",
    "    end_value = df.loc[df['year'] == 2022, 'foreignborn'].values[0]\n",
    "    \n",
    "    # Calculate the incremental change for each year\n",
    "    step = (end_value - start_value) / 12  # 12 intervals between 2010 and 2022\n",
    "    \n",
    "    # Fill in each year with the interpolated value\n",
    "    for i, year in enumerate(range(2011, 2022), start=1):\n",
    "        df.loc[df['year'] == year, 'foreignborn'] = start_value + step * i\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply imputation function \n",
    "foreignborn_complete = foreignborn_complete.groupby('state').apply(linear_impute)\n",
    "\n",
    "# Filter years\n",
    "foreignborn_filtered = foreignborn_complete[(foreignborn_complete['year'] >= 2014) & (foreignborn_complete['year'] <= 2022)]\n",
    "foreignborn_filtered = foreignborn_filtered.drop_duplicates(subset=['state', 'year'])\n",
    "\n",
    "# Reset index\n",
    "foreignborn_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "foreignborn_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7160deae",
   "metadata": {},
   "source": [
    "### Clean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e4405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6adaa205",
   "metadata": {},
   "source": [
    "## Peter Preprocessing -- to 450"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7e993",
   "metadata": {},
   "source": [
    "### Impute: \n",
    "\n",
    "We only had 3 years for religion adherence: 2000, 2010, and 2020. In order to impute the missing 8 years, we created a linear regression from 2010 to 2020. From there we predicted the adherence rates from 2011 to 2022. We eventually removed all duplicates and dropped all the years we don't want (we only want 2014-2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219a9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_data = pd.read_csv('data/religion_data.csv')\n",
    "\n",
    "state_abbrev_to_name = {\n",
    "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
    "    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n",
    "    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n",
    "    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'\n",
    "}\n",
    "\n",
    "data_2010 = religion_data[religion_data['Year'] == 2010]\n",
    "data_2020 = religion_data[religion_data['Year'] == 2020]\n",
    "\n",
    "common_states = set(data_2010['State']).intersection(set(data_2020['State']))\n",
    "data_2010 = data_2010[data_2010['State'].isin(common_states)]\n",
    "data_2020 = data_2020[data_2020['State'].isin(common_states)]\n",
    "\n",
    "data_2010 = data_2010.sort_values(by='State').reset_index(drop=True)\n",
    "data_2020 = data_2020.sort_values(by='State').reset_index(drop=True)\n",
    "\n",
    "years = np.array([2010, 2020]).reshape(-1, 1)\n",
    "adherence_rates_2010 = data_2010['Adherence Rate per 1000'].values\n",
    "adherence_rates_2020 = data_2020['Adherence Rate per 1000'].values\n",
    "\n",
    "imputed_data = []\n",
    "for state in common_states:\n",
    "    adherence_rates = np.array([adherence_rates_2010[data_2010['State'] == state].item(),\n",
    "                                adherence_rates_2020[data_2020['State'] == state].item()]).reshape(-1, 1)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(years, adherence_rates)\n",
    "    \n",
    "    for year in range(2011, 2023):\n",
    "        imputed_rate = model.predict(np.array([[year]]))[0][0]\n",
    "        imputed_data.append({'Year': year, 'State': state, 'Adherence Rate per 1000': imputed_rate})\n",
    "\n",
    "imputed_df = pd.DataFrame(imputed_data)\n",
    "\n",
    "religion_state_year = pd.concat([religion_data, imputed_df], ignore_index=True)\n",
    "religion_state_year = religion_state_year.sort_values(by=['State', 'Year']).reset_index(drop=True)\n",
    "religion_state_year = religion_state_year.drop_duplicates(subset=['State', 'Year'])\n",
    "religion_state_year = religion_state_year[religion_state_year['State'] != 'DC']\n",
    "religion_state_year['State'] =religion_state_year['State'].replace(state_abbrev_to_name)\n",
    "\n",
    "years_to_drop = list(range(2000, 2014))\n",
    "\n",
    "for years in years_to_drop:\n",
    "    religion_state_year = religion_state_year[religion_state_year['Year'] != years]\n",
    "\n",
    "state_counts = religion_state_year['State'].value_counts()\n",
    "\n",
    "religion_state_year.to_csv('data/religion_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5923c4f",
   "metadata": {},
   "source": [
    "## Eliza Preprocessing -- to 450"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18661894",
   "metadata": {},
   "source": [
    "### Impute: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce0433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8f02521",
   "metadata": {},
   "source": [
    "### Clean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b85bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e16d65b9",
   "metadata": {},
   "source": [
    "## Abortion Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452fe48d",
   "metadata": {},
   "source": [
    "The abortion data starts off as a table where each column is a type of abortion restriction and each row is for a law passed in a certain state. I start off by reshaping the data such that there is a column denoting the latest number of weeks at which abortion allowed (by the strictest law) in a given state and also added an indicator column denoting if there are no laws at all. Then, for the missing years (when no laws were enacted), I copied over the value from the previous year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9374b",
   "metadata": {},
   "source": [
    "### Reshape and Impute: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5dd95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "abortion_data = pd.read_csv('data/abortion_data.csv')\n",
    "\n",
    "# swap column names to be the number of weeks until abortion is banned\n",
    "columns = [\"State\",\"Effective Date\",\"Valid Through Date\",\"Bans_gest_4 weeks postfertilization (6 weeks LMP) \" ,\"Bans_gest_6 weeks postfertilization (8 weeks LMP) \" ,\"Bans_gest_8 weeks postfertilization (10 weeks LMP)\",\"Bans_gest_10 weeks postfertilization (12 weeks LMP) \" ,\"Bans_gest_12 weeks postfertilization (14 weeks LMP)\",\"Bans_gest_13 weeks postfertilization (15 weeks LMP)\",\"Bans_gest_16 weeks postfertilization (18 weeks LMP) \",\"Bans_gest_18 weeks postfertilization (20 weeks LMP)\",\"Bans_gest_19 weeks postfertilization (21 weeks LMP)\",\"Bans_gest20 weeks postfertilization (22 weeks LMP)\",\"Bans_gest_21 weeks postfertilization (23 weeks LMP) \" ,\"Bans_gest_22 weeks postfertilization (24 weeks LMP)\",\"Bans_gest_24 weeks postfertilization (26 weeks LMP)\",\"Bans_gestViability\",\"Bans_gest_Fetus is capable of feeling pain\",\"Bans_gest_3rd trimester\"]\n",
    "abortion_data = abortion_data[columns]\n",
    "new_names = {\n",
    "    \"State\": \"State\",\n",
    "    \"Effective Date\": \"start\",\n",
    "    \"Valid Through Date\": \"end\",\n",
    "    \"Bans_gest_4 weeks postfertilization (6 weeks LMP) \": \"4\",\n",
    "    \"Bans_gest_6 weeks postfertilization (8 weeks LMP) \": \"6\",\n",
    "    \"Bans_gest_8 weeks postfertilization (10 weeks LMP)\": \"8\",\n",
    "    \"Bans_gest_10 weeks postfertilization (12 weeks LMP) \": \"10\",\n",
    "    \"Bans_gest_12 weeks postfertilization (14 weeks LMP)\": \"12\",\n",
    "    \"Bans_gest_13 weeks postfertilization (15 weeks LMP)\": \"13\",\n",
    "    \"Bans_gest_16 weeks postfertilization (18 weeks LMP) \": \"16\",\n",
    "    \"Bans_gest_18 weeks postfertilization (20 weeks LMP)\": \"18\",\n",
    "    \"Bans_gest_19 weeks postfertilization (21 weeks LMP)\": \"19\",\n",
    "    \"Bans_gest20 weeks postfertilization (22 weeks LMP)\": \"20\",\n",
    "    \"Bans_gest_21 weeks postfertilization (23 weeks LMP) \": \"21\",\n",
    "    \"Bans_gest_22 weeks postfertilization (24 weeks LMP)\": \"22\",\n",
    "    \"Bans_gest_24 weeks postfertilization (26 weeks LMP)\": \"24\",\n",
    "    \"Bans_gestViability\": \"24\", #chose via google\n",
    "    \"Bans_gest_Fetus is capable of feeling pain\": \"25\", #chose via google\n",
    "    \"Bans_gest_3rd trimester\": \"28\" #chose via google\n",
    "}\n",
    "abortion_data = abortion_data.rename(columns=new_names)\n",
    "\n",
    "# create a column, latest_abortion, that has the value of the number of weeks allowed until abortion is banned\n",
    "abortion_processed = abortion_data[['State', 'start', 'end']].copy()\n",
    "abortion_processed['latest_abortion'] = abortion_data[abortion_data.columns].apply(\n",
    "    lambda row: next((int(col) for col in abortion_data.columns  if str(row[col]) == \"1\"), 40), axis=1)\n",
    "\n",
    "# add an indicator column if there is no abortion law\n",
    "abortion_processed['no_abortion_law'] = 0\n",
    "abortion_processed.loc[abortion_processed['latest_abortion'] == 40, 'no_abortion_law'] = 1\n",
    "\n",
    "# if there are multiple laws in a year, choose the one with the longest duration\n",
    "abortion_processed['start'] = pd.to_datetime(abortion_processed['start'])\n",
    "abortion_processed['end'] = pd.to_datetime(abortion_processed['end'])\n",
    "abortion_processed['Year'] = abortion_processed['start'].dt.year\n",
    "abortion_processed['length'] = (abortion_processed['end'] - abortion_processed['start']).dt.days\n",
    "abortion_processed = abortion_processed.loc[abortion_processed.groupby(['State', 'Year'])['length'].idxmax()]\n",
    "abortion_processed = abortion_processed.drop(columns=['start', 'end', 'length'])\n",
    "\n",
    "# remove dc\n",
    "abortion_no_dc = abortion_processed[abortion_processed['State'] != \"District of Columbia\"]\n",
    "\n",
    "# create a dataframe include missing years\n",
    "years = range(2018, 2022+1)\n",
    "all_combinations = pd.MultiIndex.from_product(\n",
    "    [abortion_no_dc['State'].unique(), years],\n",
    "    names=['State', 'Year']\n",
    ")\n",
    "\n",
    "#impute with most recent law\n",
    "abortion_state_year = abortion_no_dc.set_index(['State', 'Year']).reindex(all_combinations).reset_index()\n",
    "abortion_state_year['latest_abortion'] = abortion_state_year.groupby('State')['latest_abortion'].ffill()\n",
    "abortion_state_year['no_abortion_law'] = abortion_state_year.groupby('State')['no_abortion_law'].ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfe90d",
   "metadata": {},
   "source": [
    "## Now we combine our different datasets along State and Years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00352189",
   "metadata": {},
   "source": [
    "Note since we only have abortion data from 2018-2022, we made two datasetes, one including 2014-2017 and one without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f97e495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>latest_abortion</th>\n",
       "      <th>no_abortion_law</th>\n",
       "      <th>Adherence Rate per 1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2018</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>634.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2019</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>635.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>636.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2021</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>636.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>637.366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     State  Year  latest_abortion  no_abortion_law  Adherence Rate per 1000\n",
       "0  Alabama  2018             20.0              0.0                  634.654\n",
       "1  Alabama  2019             20.0              0.0                  635.332\n",
       "2  Alabama  2020             20.0              0.0                  636.010\n",
       "3  Alabama  2021             20.0              0.0                  636.688\n",
       "4  Alabama  2022             20.0              0.0                  637.366"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_2018_to_2022 = pd.merge(abortion_state_year, religion_state_year, on=['State', 'Year'], how='inner')\n",
    "combined_2018_to_2022.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c96d052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>latest_abortion</th>\n",
       "      <th>no_abortion_law</th>\n",
       "      <th>Adherence Rate per 1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>631.942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>632.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>633.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>633.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2018</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>634.654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     State  Year  latest_abortion  no_abortion_law  Adherence Rate per 1000\n",
       "0  Alabama  2014              NaN              NaN                  631.942\n",
       "1  Alabama  2015              NaN              NaN                  632.620\n",
       "2  Alabama  2016              NaN              NaN                  633.298\n",
       "3  Alabama  2017              NaN              NaN                  633.976\n",
       "4  Alabama  2018             20.0              0.0                  634.654"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_2014_to_2022 = pd.merge(abortion_state_year, religion_state_year, on=['State', 'Year'], how='outer')\n",
    "combined_2014_to_2022.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a031ef",
   "metadata": {},
   "source": [
    "### One way to preliminarily understand why fertility rate is decreasing is to look at where it changes over time, and think about what factors dominate those areas.... aka do PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc0196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this accounts for 90.25 % variance\n"
     ]
    }
   ],
   "source": [
    "pivoted_fertility = fertility_rate.pivot(index='STATE', columns='YEAR', values='FERTILITY RATE')\n",
    "X_std =  StandardScaler().fit_transform(pivoted_fertility.fillna(0)) \n",
    "pca = PCA(n_components=1) \n",
    "principal_components = pca.fit_transform(X_std)\n",
    "\n",
    "states = pivoted_fertility.index\n",
    "pca_states = pd.DataFrame(data=principal_components, columns=['PC1'], index=states).reset_index()\n",
    "pca_states.columns = ['state', 'PC1']\n",
    "\n",
    "print(\"this accounts for\", np.round(100*pca.explained_variance_ratio_[0], 2), \"% variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "geo": "geo",
         "hovertemplate": "state=%{location}<br>Principal Component Value=%{z}<extra></extra>",
         "locationmode": "USA-states",
         "locations": [
          "AK",
          "AL",
          "AR",
          "AZ",
          "CA",
          "CO",
          "CT",
          "DE",
          "District of Columbia",
          "FL",
          "GA",
          "HI",
          "IA",
          "ID",
          "IL",
          "IN",
          "KS",
          "KY",
          "LA",
          "MA",
          "MD",
          "ME",
          "MI",
          "MN",
          "MO",
          "MS",
          "MT",
          "NC",
          "ND",
          "NE",
          "NH",
          "NJ",
          "NM",
          "NV",
          "NY",
          "OH",
          "OK",
          "OR",
          "PA",
          "RI",
          "SC",
          "SD",
          "TN",
          "TX",
          "UT",
          "VA",
          "VT",
          "WA",
          "WI",
          "WV",
          "WY"
         ],
         "name": "",
         "type": "choropleth",
         "z": [
          3.541283712977135,
          0.6709412182023973,
          1.5457949635374837,
          0.10649601868313613,
          -0.6917694677307884,
          -1.1011690210012075,
          -2.1863383716862783,
          -0.37832791196931714,
          -16.233015809645977,
          -0.528790432081549,
          -0.05460097994243515,
          1.6461409794543915,
          1.5048116278832806,
          1.9646225328459557,
          -0.5949947846694897,
          1.0685882178489023,
          1.7938455128410884,
          1.3653788074915456,
          1.8402640528169016,
          -2.7120392538854134,
          0.12559309177697608,
          -2.1222387563099274,
          -0.3141942755414467,
          1.05389380426039,
          0.749391890778195,
          1.0968872405309764,
          0.2727677589110798,
          -0.02471643195687145,
          3.890011060657697,
          2.9059396366882417,
          -2.9188544099122273,
          0.13734118876124227,
          -0.08137037564430298,
          -0.018514278358477677,
          -0.6701232387274071,
          0.5452279391799576,
          1.801088682535652,
          -2.015507684068103,
          -0.7042952458645648,
          -2.763023749367187,
          -0.09768345031071042,
          4.288426453218404,
          0.6257515800884539,
          1.7738449052948442,
          3.3577915513545573,
          -0.10421969181466428,
          -3.432102609219369,
          -0.27078633430077365,
          -0.12296469901310918,
          -0.5505104046423946,
          1.0200272390451
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmax": 5,
         "cmin": -5,
         "colorbar": {
          "title": {
           "text": "Principal Component Value"
          }
         },
         "colorscale": [
          [
           0,
           "purple"
          ],
          [
           0.5,
           "white"
          ],
          [
           1,
           "green"
          ]
         ]
        },
        "geo": {
         "center": {},
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "scope": "usa"
        },
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "What states have the highest variation in fertility rates?"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.choropleth(\n",
    "    pca_states,\n",
    "    locations='state',\n",
    "    locationmode=\"USA-states\",\n",
    "    color='PC1',\n",
    "    scope=\"usa\",  \n",
    "    range_color=[-5,5],\n",
    "    color_continuous_scale=['purple', 'white', 'green'], \n",
    "    labels={'PC1': 'Principal Component Value'}\n",
    ")\n",
    "\n",
    "fig.update_layout(title_text=\"What states have the highest variation in fertility rates?\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a5c55",
   "metadata": {},
   "source": [
    "We see that the first principle componetent represents 90% of varition over state and time. Thus, most of the change (net decrease) in fertility rate is seen when the coasts (purples) move strongly in one direction (presumably decrease given the net decrease) and the center of the US moves slightly in the other. We can use this information to understand what variables would be good predictors, aka the ones that match the coloring above. For example, a wealth distribution or political map would see similar differences with one color on the coast and another in the center. Thus we know that looking at household income and political affiliation are going to be really good predictor variables!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> expected_url = './data/html/screenboston.html'\n>>> assert os.path.isfile(expected_url), f'Expected local file {expected_url}'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> expected_url = './data/html/screenboston.html'\n>>> with open(expected_url, 'r') as f:\n...     content = f.read()\n...     s = BeautifulSoup(content)\n...     assert s.select_one('p').text == 'Screen Boston', f'Content of file saved from {expected_url} should contain a <p> tag with the page name.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> n = 50\n>>> assert len(movies) > n, f'`movies` should contain more than {n} elements; you have {len(movies)}.'\n>>> assert all((isinstance(m, dict) for m in movies)), 'Elements of `movies` should all be dictionaries.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> keys = {'title', 'directors', 'year', 'genre', 'runtime', 'theater', 'screen_date', 'screen_times'}\n>>> assert all((set(keys).issubset(m.keys()) for m in movies)), f'Each dictionary in `movies` should contain all of these keys: {', '.join(keys)}. At least one dictionary is missing one or more keys.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> assert all((isinstance(m['year'], int) for m in movies)), \"The 'year' value should be an integer in all dictionaries.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> snapshots = glob.glob('./data/html/snapshot_*.html')\n>>> assert len(snapshots) >= 4, f'You should have at least 4 snapshots, but found {len(snapshots)} files with paths like ./data/html/snapshot_*.html'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> snapshots = glob.glob('./data/html/snapshot_*.html')\n>>> assert all((re.match('snapshot_\\\\d{8}\\\\.html', os.path.basename(f)) for f in snapshots)), \"All snapshot files should be named in the format 'snapshot_YYYYMMDD.html'\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> assert os.path.isfile('data/movies.json'), \"The file 'data/movies.json' should exist.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> with open('data/movies.json', 'r') as f:\n...     movies = json.load(f)\n>>> n = 300\n>>> assert len(movies) > n, f'`movies` should now contain more than {n} elements; you have {len(movies)}.'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> keys = {'title', 'directors', 'year', 'genre', 'runtime', 'theater', 'screen_date', 'screen_times'}\n>>> assert all((isinstance(m, dict) and set(keys).issubset(m.keys()) for m in movies)), f'Each dictionary in `movies` should contain all of these keys: {', '.join(keys)}. At least one dictionary is missing one or more keys.'\n",
         "hidden": false,
         "locked": false,
         "points": 5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(df, pd.DataFrame), \"You should have stored your DataFrame in a variable called 'df'\"\n>>> assert df.shape[1] == 8, 'Your DataFrame, df, should have 8 columns'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> assert df.shape[0] > 300, 'You should have found at least 300 non-duplicate rows'\n>>> assert df.shape[0] == df.drop_duplicates().shape[0], 'There are still duplicate rows in your DataFrame'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> def is_datetime_column_hacky(column):\n...     dtype_str = str(column.dtype)\n...     valid_types = ['date', 'timestamp', 'timedelta', 'duration']\n...     return any((t in dtype_str.lower() for t in valid_types))\n>>> assert is_datetime_column_hacky(df['screen_date']), \"The 'screen_date' column must be either a Pandas or PyArrow date/datetime type.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> def is_datetime_column_hacky(column):\n...     dtype_str = str(column.dtype)\n...     valid_types = ['date', 'timestamp', 'timedelta', 'duration']\n...     return any((t in dtype_str.lower() for t in valid_types))\n>>> assert is_datetime_column_hacky(df['runtime']), \"The 'runtime' column must be either a Pandas or PyArrow timedelta type.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> assert df['screen_date'].is_monotonic_increasing, \"The 'screen_date' column is not sorted in ascending order\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": 7,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'wiki_id' in df.columns, \"`df` should now have a column called 'wiki_id'.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> p = 0.5\n>>> assert (q := df['wiki_id'].notna().mean()) >= p, f'You should have been able to find wiki IDs for at least {p:.0%} of the screenings. You found {q:.0%}.'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'wiki_html' in df.columns, \"`df` should now have a column called 'wiki_html'.\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> p = 0.5\n>>> assert (q := df['wiki_html'].notna().mean()) >= p, f'You should have been able to acquire wiki page HTML content for at least {p:.0%} of the screenings. You found {q:.0%}.'\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "wrapup": {
     "name": "wrapup",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert float(hours_spent_on_hw), 'Please select a time in hours (int or float) to specify how long you spent on this assignment.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
