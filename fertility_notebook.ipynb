{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1586e42",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7423c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d882ae3",
   "metadata": {},
   "source": [
    "Please note, we will be collaborating via Git. Please find our project at https://github.com/ppich1169/fertilityProjectCs109"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d461283f",
   "metadata": {},
   "source": [
    "# Milestone 1: Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07556bb7",
   "metadata": {},
   "source": [
    "Over the past fifty years, fertility rates in the US have plummeted and are currently at a historic low. Conversations about why fertility has fallen so substantially and how we can address the implications of this shift for government programs like social security have been quite salient in recent public discourse and in the 2024 election cycle. \n",
    "\n",
    "Interestingly, there is significant variation in fertility rates across US states. We’d like to understand the relative importance of various factors in determining a state’s fertility rate. \n",
    "\n",
    "We plan to run a multiple regression of fertility rate (can get state-by-state here from the CDC’s National Center for Health Statistics) on a number of regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d422fe2",
   "metadata": {},
   "source": [
    "**Goal:** Create a regression that can predict the fertility rate of a state. Then, analyze coefficients and/or use causal inference to understand why fertility rate is going down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2306c",
   "metadata": {},
   "source": [
    "# Milestone 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091b7ea",
   "metadata": {},
   "source": [
    "## 1. Access the data that you will be using for the final project by downloading, collecting, or scraping* from the relevant source(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6af71",
   "metadata": {},
   "source": [
    "### Response Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ab7e9",
   "metadata": {},
   "source": [
    "Our response variable is **fertility rate by state over time** which can be found at https://www.cdc.gov/nchs/pressroom/sosmap/fertility_rate/fertility_rates.htm. \n",
    "\n",
    "We accessed it via download and saved it as `fertility_rate_census.csv`. \n",
    "\n",
    "Please note, fertility rate is defined as  **total number of births per 1,000 women aged 15-44** and our dataset looks at fertility rate for each of the 50 states over 9 years (2014-2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b4bfa7",
   "metadata": {},
   "source": [
    "### Predictors\n",
    "_We used our previous knowledge and assumptions to create an **X** dataset of predictors that we believe may influence fertility rates_\n",
    "\n",
    "Because fertility rate is evaluated statewide , and states vary significantly in population size, we have decided that for all of the predictors, we are going to essentially **normalize** them by looking at the percentage of each state that fall into a specific category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0005638",
   "metadata": {},
   "source": [
    "**ISABELLA SECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We care about basic demographic data which we downloaded get from the **census** at [**THIS LINK INSERT HERE**]\n",
    "Here is the census data we consider important (based on our own subjective opinions):\n",
    "- race\n",
    "- socio economic status (household income) - percentage of households below the poverty line\n",
    "- education level - share that are high school graduates \n",
    "- immigration status\n",
    "\n",
    "And obviously we need **age**, **sex** , **year** , and **state**  in order to aggregate our data\n",
    "\n",
    "We accessed it by [**SCRAPING (CODE BELOW) / DOWNLOADING**] and saved it in `dataset_name.csv` in our data folder\n",
    "\n",
    "Some things to consider include: [**INSERT HERE**] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also care about religion, political makeup, and whether certain abortion laws are in place (all of which are not in the census). We will find them different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00aeee",
   "metadata": {},
   "source": [
    "**PETER SECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576fc93",
   "metadata": {},
   "source": [
    "We decided to determine people's **religiousness** based off of [**INSERT HERE**] which can be found at [**INSERT LINK HERE**]\n",
    "\n",
    "We accessed it by [**SCRAPING (CODE BELOW) / DOWNLOADING**] and saved it in `dataset_name.csv` in our data folder\n",
    "\n",
    "Some things to consider include: [**INSERT HERE**] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b77098",
   "metadata": {},
   "source": [
    "**ELIZA SECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ce4a9",
   "metadata": {},
   "source": [
    "We decided to determine people's **political orienation** based off of [**INSERT HERE**] which can be found at [**INSERT LINK HERE**]\n",
    "\n",
    "We accessed it by [**SCRAPING (CODE BELOW) / DOWNLOADING**] and saved it in `dataset_name.csv` in our data folder\n",
    "\n",
    "Some things to consider include: [**INSERT HERE**] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f7f12c",
   "metadata": {},
   "source": [
    "**MAIA SECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98418d",
   "metadata": {},
   "source": [
    "We decided to determine state's **abortion laws** based off of [**INSERT HERE**] which can be found at [**INSERT LINK HERE**]\n",
    "\n",
    "We accessed it by [**SCRAPING (CODE BELOW) / DOWNLOADING**] and saved it in `dataset_name.csv` in our data folder\n",
    "\n",
    "Some things to consider include: [**INSERT HERE**] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the data into a Jupyter notebook and understand the data by examining, among other characteristics of interest, data missingness, imbalance, and scaling issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some issues we preliminary have considered before even inspecting the data includes:\n",
    "\n",
    "\n",
    "- **under reporting immigration status**: via google, people tend to underreport whether they are immigrants. This is potentially a missingness issue\n",
    "\n",
    "-  **multicollinearity**: There is most likely a relationship between racial background / household income and an individual's birth country (immigration status) so we can't use both as predictors in the same equation. We don't know how strong this correlation will be so are not worried yet, but would love to talk about it with a TF\n",
    "\n",
    "- **is this just too much data??**: looking at each of these attributes for each state for each year may just be too many dimensions. Is there really a big difference accross years? Should we just look at 1? If so, which? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to import and inspect each dataset, looking for missingness, imbalance, and scaling issues!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ebdcb",
   "metadata": {},
   "source": [
    "### Fertility Rate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR              0\n",
      "STATE             0\n",
      "FERTILITY RATE    0\n",
      "BIRTHS            0\n",
      "URL               0\n",
      "dtype: int64\n",
      "              YEAR  FERTILITY RATE         BIRTHS\n",
      "count   451.000000      451.000000     451.000000\n",
      "mean   2018.008869       60.057871   75783.962306\n",
      "std       2.588850        6.420758   86837.134690\n",
      "min    2014.000000       44.300000    5133.000000\n",
      "25%    2016.000000       56.000000   21758.500000\n",
      "50%    2018.000000       60.200000   55971.000000\n",
      "75%    2020.000000       63.500000   86486.000000\n",
      "max    2022.000000       80.000000  502879.000000\n"
     ]
    }
   ],
   "source": [
    "fertility_rate = pd.read_csv('data/fertility_rate_census.csv')\n",
    "print(fertility_rate.isna().sum(axis=0))\n",
    "fertility_rate.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016931d7",
   "metadata": {},
   "source": [
    "Things to Consider:\n",
    "\n",
    "**Missingness**:\n",
    "\n",
    "**Imbalance**:\n",
    "\n",
    "**Scaling**:\n",
    "\n",
    "**Other**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_data = pd.read_csv('data/')\n",
    "print(census_data.isna().sum(axis=0))\n",
    "census_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0276f4f3",
   "metadata": {},
   "source": [
    "Things to Consider:\n",
    "\n",
    "**Missingness**:\n",
    "\n",
    "**Imbalance**:\n",
    "\n",
    "**Scaling**:\n",
    "\n",
    "**Other**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eaf37b",
   "metadata": {},
   "source": [
    "### Religion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_data = pd.read_csv('data/')\n",
    "print(religion_data.isna().sum(axis=0))\n",
    "religion_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b6996",
   "metadata": {},
   "source": [
    "Things to Consider:\n",
    "\n",
    "**Missingness**:\n",
    "\n",
    "**Imbalance**:\n",
    "\n",
    "**Scaling**:\n",
    "\n",
    "**Other**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741289e",
   "metadata": {},
   "source": [
    "### Politics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_data = pd.read_csv('data/')\n",
    "print(politics_data.isna().sum(axis=0))\n",
    "politics_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b47016b",
   "metadata": {},
   "source": [
    "Things to Consider:\n",
    "\n",
    "**Missingness**:\n",
    "\n",
    "**Imbalance**:\n",
    "\n",
    "**Scaling**:\n",
    "\n",
    "**Other**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a65ab",
   "metadata": {},
   "source": [
    "### Abortion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29abc0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "abortion_data = pd.read_csv('data/')\n",
    "print(abortion_data.isna().sum(axis=0))\n",
    "abortion_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae04e9c",
   "metadata": {},
   "source": [
    "Things to Consider:\n",
    "\n",
    "**Missingness**:\n",
    "\n",
    "**Imbalance**:\n",
    "\n",
    "**Scaling**:\n",
    "\n",
    "**Other**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understand and describe the preprocessing required such that data is in a form amenable to later downstream tasks such as visualizing and modeling, as is appropriate to the specific project goals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e8d54",
   "metadata": {},
   "source": [
    "One of our biggest considerations is that our predictor variables, X, is essentially 3d (reshaped), not 2d. We are looking at a bunch of predictors **over state** and **over time**. This might make our predictions complicated, especially because we are trying to see **why** fertility is changing over time (so the reason that there are less babies in 2022 can't simply be that it is 2022). Thus, it might make more sense to choose only **one year** to regress on. Also, it may be hard to **visualize** multiple years and states simultaniously as, for example, if we drew a map and colored each state by predictor category, we would have to choose one specific time to do so (and vice versa).  PLEASE LET US KNOW WHAT YOU THINK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0aff50",
   "metadata": {},
   "source": [
    "It also may make sense to just try to minimize predictors in general as we don't want to get too specific!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b047880",
   "metadata": {},
   "source": [
    "### This is how we envision generally preprocessing..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> expected_url = './data/html/screenboston.html'\n>>> assert os.path.isfile(expected_url), f'Expected local file {expected_url}'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> expected_url = './data/html/screenboston.html'\n>>> with open(expected_url, 'r') as f:\n...     content = f.read()\n...     s = BeautifulSoup(content)\n...     assert s.select_one('p').text == 'Screen Boston', f'Content of file saved from {expected_url} should contain a <p> tag with the page name.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> n = 50\n>>> assert len(movies) > n, f'`movies` should contain more than {n} elements; you have {len(movies)}.'\n>>> assert all((isinstance(m, dict) for m in movies)), 'Elements of `movies` should all be dictionaries.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> keys = {'title', 'directors', 'year', 'genre', 'runtime', 'theater', 'screen_date', 'screen_times'}\n>>> assert all((set(keys).issubset(m.keys()) for m in movies)), f'Each dictionary in `movies` should contain all of these keys: {', '.join(keys)}. At least one dictionary is missing one or more keys.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> assert all((isinstance(m['year'], int) for m in movies)), \"The 'year' value should be an integer in all dictionaries.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> snapshots = glob.glob('./data/html/snapshot_*.html')\n>>> assert len(snapshots) >= 4, f'You should have at least 4 snapshots, but found {len(snapshots)} files with paths like ./data/html/snapshot_*.html'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> snapshots = glob.glob('./data/html/snapshot_*.html')\n>>> assert all((re.match('snapshot_\\\\d{8}\\\\.html', os.path.basename(f)) for f in snapshots)), \"All snapshot files should be named in the format 'snapshot_YYYYMMDD.html'\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> assert os.path.isfile('data/movies.json'), \"The file 'data/movies.json' should exist.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> with open('data/movies.json', 'r') as f:\n...     movies = json.load(f)\n>>> n = 300\n>>> assert len(movies) > n, f'`movies` should now contain more than {n} elements; you have {len(movies)}.'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> keys = {'title', 'directors', 'year', 'genre', 'runtime', 'theater', 'screen_date', 'screen_times'}\n>>> assert all((isinstance(m, dict) and set(keys).issubset(m.keys()) for m in movies)), f'Each dictionary in `movies` should contain all of these keys: {', '.join(keys)}. At least one dictionary is missing one or more keys.'\n",
         "hidden": false,
         "locked": false,
         "points": 5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(df, pd.DataFrame), \"You should have stored your DataFrame in a variable called 'df'\"\n>>> assert df.shape[1] == 8, 'Your DataFrame, df, should have 8 columns'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> assert df.shape[0] > 300, 'You should have found at least 300 non-duplicate rows'\n>>> assert df.shape[0] == df.drop_duplicates().shape[0], 'There are still duplicate rows in your DataFrame'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> def is_datetime_column_hacky(column):\n...     dtype_str = str(column.dtype)\n...     valid_types = ['date', 'timestamp', 'timedelta', 'duration']\n...     return any((t in dtype_str.lower() for t in valid_types))\n>>> assert is_datetime_column_hacky(df['screen_date']), \"The 'screen_date' column must be either a Pandas or PyArrow date/datetime type.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> def is_datetime_column_hacky(column):\n...     dtype_str = str(column.dtype)\n...     valid_types = ['date', 'timestamp', 'timedelta', 'duration']\n...     return any((t in dtype_str.lower() for t in valid_types))\n>>> assert is_datetime_column_hacky(df['runtime']), \"The 'runtime' column must be either a Pandas or PyArrow timedelta type.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> assert df['screen_date'].is_monotonic_increasing, \"The 'screen_date' column is not sorted in ascending order\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": 7,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'wiki_id' in df.columns, \"`df` should now have a column called 'wiki_id'.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> p = 0.5\n>>> assert (q := df['wiki_id'].notna().mean()) >= p, f'You should have been able to find wiki IDs for at least {p:.0%} of the screenings. You found {q:.0%}.'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'wiki_html' in df.columns, \"`df` should now have a column called 'wiki_html'.\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> p = 0.5\n>>> assert (q := df['wiki_html'].notna().mean()) >= p, f'You should have been able to acquire wiki page HTML content for at least {p:.0%} of the screenings. You found {q:.0%}.'\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "wrapup": {
     "name": "wrapup",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert float(hours_spent_on_hw), 'Please select a time in hours (int or float) to specify how long you spent on this assignment.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
