{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1586e42",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7423c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly\n",
    "#import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import shuffle\n",
    "from warnings import simplefilter\n",
    "simplefilter('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d882ae3",
   "metadata": {},
   "source": [
    "Please note, we will be collaborating via Git. Please find our project at https://github.com/ppich1169/fertilityProjectCs109"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d461283f",
   "metadata": {},
   "source": [
    "# Milestone 1: Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07556bb7",
   "metadata": {},
   "source": [
    "Over the past fifty years, fertility rates in the US have plummeted and are currently at a historic low. Conversations about why fertility has fallen so substantially and how we can address the implications of this shift for government programs like social security have been quite salient in recent public discourse and in the 2024 election cycle. \n",
    "\n",
    "Interestingly, there is significant variation in fertility rates across US states. We’d like to understand the relative importance of various factors in determining a state’s fertility rate. \n",
    "\n",
    "We plan to run a multiple regression of fertility rate (can get state-by-state here from the CDC’s National Center for Health Statistics) on a number of regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d422fe2",
   "metadata": {},
   "source": [
    "**Goal:** Create a regression that can predict the fertility rate of a state. Then, analyze coefficients and/or use causal inference to understand why fertility rate is going down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2306c",
   "metadata": {},
   "source": [
    "# Milestone 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091b7ea",
   "metadata": {},
   "source": [
    "## 1. Access the data that you will be using for the final project by downloading, collecting, or scraping* from the relevant source(s) and 2. Load the data into a Jupyter notebook and understand the data by examining, among other characteristics of interest, data missingness, imbalance, and scaling issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225eca3b",
   "metadata": {},
   "source": [
    "**TL;DR** Hi! We didn't know how long to make our doc but wanted to get as much feedback as possible so included everything here. We are more than happy to procide an abridged version. Essentially, we aquired data with a combination of webscraping and downloading from websites. Most of the data has some element of missingess as there are some gaps in years. We would fill this missingness by creating a linear regression on the data we have, and filling in our unknown data. We also needed to scale some of our data into the range 0-1. Finally, we proprocessed that data by making some categorical variables ordinal, others one-hot encoded, and finally putting all our individal data sets together (linking on year and state). Thank you for taking the time to read this!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6af71",
   "metadata": {},
   "source": [
    "### Response Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ab7e9",
   "metadata": {},
   "source": [
    "Our response variable is **fertility rate by state over time** which can be found at https://www.cdc.gov/nchs/pressroom/sosmap/fertility_rate/fertility_rates.htm. \n",
    "\n",
    "We accessed it via download and saved it as `fertility_rate_census.csv`. \n",
    "\n",
    "Please note, fertility rate is defined as  **total number of births per 1,000 women aged 15-44** and our dataset looks at fertility rate for each of the 50 states over 9 years (2014-2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28728668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR              0\n",
      "STATE             0\n",
      "FERTILITY RATE    0\n",
      "BIRTHS            0\n",
      "URL               0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>FERTILITY RATE</th>\n",
       "      <th>BIRTHS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>451.000000</td>\n",
       "      <td>451.000000</td>\n",
       "      <td>451.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2018.008869</td>\n",
       "      <td>60.057871</td>\n",
       "      <td>75783.962306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.588850</td>\n",
       "      <td>6.420758</td>\n",
       "      <td>86837.134690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2014.000000</td>\n",
       "      <td>44.300000</td>\n",
       "      <td>5133.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2016.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>21758.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>60.200000</td>\n",
       "      <td>55971.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>86486.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2022.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>502879.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              YEAR  FERTILITY RATE         BIRTHS\n",
       "count   451.000000      451.000000     451.000000\n",
       "mean   2018.008869       60.057871   75783.962306\n",
       "std       2.588850        6.420758   86837.134690\n",
       "min    2014.000000       44.300000    5133.000000\n",
       "25%    2016.000000       56.000000   21758.500000\n",
       "50%    2018.000000       60.200000   55971.000000\n",
       "75%    2020.000000       63.500000   86486.000000\n",
       "max    2022.000000       80.000000  502879.000000"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fertility_rate = pd.read_csv('data/fertility_rate_census.csv')\n",
    "print(fertility_rate.isna().sum(axis=0))\n",
    "fertility_rate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ca340f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>YEAR</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>District of Columbia</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "YEAR                  2014  2015  2016  2017  2018  2019  2020  2021  2022\n",
       "STATE                                                                     \n",
       "District of Columbia   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  57.3"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_fertility = fertility_rate.pivot(index='STATE', columns='YEAR', values='FERTILITY RATE')\n",
    "pivoted_fertility[pivoted_fertility.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656d151",
   "metadata": {},
   "source": [
    "Things to Consider:\n",
    "\n",
    "**Missingness**: While there is no empty cell in the original dataset, we see if we pivot it by year, Washington DC only shows up in 2022. We can simply delete this row as Washington DC isn't technically a state. This is Missing at Random because we know the reason there was no Washington DC from 2014-2021 is that DC isn't considered a state\n",
    "\n",
    "**Imbalance**: Since we aren't looking at different classes there is no imbalance\n",
    "\n",
    "**Scaling**: By considering Fertility Rate (and not Number of Births), we are essentially normalizing our data as we are dividing it by total population. This will be enough in order to scale the data as it takes into account the populations of each state such that no state is overly weighed. In addition, since fertility rate is a response variable, not a predictor, we don't have to consider how large our data is in relation to other variables. Thus, we don't have to scale it any further.\n",
    "\n",
    "**Other**: We need to encode the state variable as categorical if it will be used in modeling by creating dummy variables or one-hot encoding. This issue will be true for all datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b4bfa7",
   "metadata": {},
   "source": [
    "### Predictors\n",
    "_We used our previous knowledge and assumptions to create an **X** dataset of predictors that we believe may influence fertility rates_\n",
    "\n",
    "Because fertility rate is evaluated statewide , and states vary significantly in population size, we have decided that for all of the predictors, we are going to essentially **normalize** them by looking at the percentage of each state that fall into a specific category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0005638",
   "metadata": {},
   "source": [
    "## CENSUS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa6b619",
   "metadata": {},
   "source": [
    "We began by collecting essential demographic data from the U.S. Census Bureau's website (https://www.census.gov) to analyze trends across the U.S. population. This data included indicators such as socioeconomic status, foreign-born populations, education levels, and racial demographics. We organized the data into separate data frames—`SPM_df`, `foreignborn_df`, `education_dfs`, and `race_dfs`—to facilitate efficient management and analysis.\n",
    "\n",
    "The **SPM (Supplemental Poverty Measure)** data, loaded from `SPM.csv`, provides a broad measure of poverty across states, capturing household income levels below the poverty line and accounting for both cash income and non-cash benefits. Although this data originates from the Census Bureau, we accessed it through Statista (https://www.statista.com/statistics/312701/percentage-of-population-foreign-born-in-the-us-by-state/), where it was already aggregated and prepared for analysis. Upon examination, we found no missing values, making it straightforward to integrate into our analysis without additional preprocessing. The structure, with a single column for each state’s poverty estimate, is simple and well-suited for analysis.\n",
    "\n",
    "The **foreign-born population dataset**, sourced directly from Census data in `foreignborn2022.csv`, records the percentage of foreign-born individuals by state for 2022. This dataset offers insights into immigration trends, which may impact fertility rates, and was found to be free from missing values. The data is cleanly structured with only two columns (`State` and `Percent`), making it ready for immediate use in analysis.\n",
    "\n",
    "The **education data** files, loaded from various `*education.csv` files, provide shares of educational attainment across different age groups, offering a nuanced view of educational levels within each state. Each file includes detailed demographic breakdowns with hierarchical labels representing educational categories, such as `Total`, `Percent`, and specific demographic groupings (like age, gender). When analyzing these files, we found a complex column structure and hierarchical labels, which will require additional preprocessing for effective analysis. Moreover, each state had some missing values across specific columns, likely due to unavailable data for certain demographic subgroups. These missing values will need to be handled through imputation or selective removal of columns with excessive gaps.\n",
    "\n",
    "Finally, the **race data** files, loaded from various `*race.csv` files, provide demographic breakdowns by race for each state. This dataset mirrors the structure of the education data, with hierarchical labels that represent various racial categories and demographic details. Similar to the education data, the race data was found to have missing values and a complex structure. The intricate labeling and demographic specificity will require careful preprocessing to flatten the hierarchical labels and manage missing values, ensuring consistency with other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072fb112",
   "metadata": {},
   "source": [
    "### SPM Dataset\n",
    "\n",
    "**Missingness**: The SPM data, sourced from `SPM.csv`, contains no missing values, making it easy to integrate into the analysis without any imputation. The completeness of this dataset ensures we can use it directly to assess the relationship between poverty levels and fertility rates across states.\n",
    "\n",
    "**Imbalance**: This dataset represents one measurement (poverty estimate) per state, so there are no categorical classes that could lead to imbalance. Each state has a single poverty estimate, meaning no particular state is over- or underrepresented in terms of poverty measurement.\n",
    "\n",
    "**Scaling**: The SPM dataset is already normalized, as it provides the percentage of households below the poverty line for each state. This percentage-based format allows us to make direct comparisons between states without additional scaling, as the data is inherently comparable across geographic areas.\n",
    "\n",
    "### Foreign-Born Population Dataset\n",
    "\n",
    "**Missingness**: The foreign-born population dataset, from `foreignborn2022.csv`, is also complete with no missing values. This allows for straightforward inclusion in the analysis, with no need for handling missing data.\n",
    "\n",
    "**Imbalance**: This dataset has no categorical classes and represents a single percentage (foreign-born population) per state. There is, therefore, no issue of imbalance in this dataset, as each state is equally represented with one percentage value.\n",
    "\n",
    "**Scaling**:  The data is already in percentage form, representing the share of foreign-born individuals in each state's population. This normalized format allows for direct comparison across states without further scaling, as each percentage reflects a relative measure rather than an absolute count.\n",
    "\n",
    "### Education Dataset\n",
    "\n",
    "**Missingness**: The education data files, loaded from `*education.csv`, contain missing values across some columns, particularly in specific demographic breakdowns within each state. This missingness appears sporadically, likely due to data limitations in certain subgroups or categories. Handling these missing values will require careful consideration, with options to either impute values based on related data points or exclude columns with excessive gaps.\n",
    "\n",
    "**Imbalance**:  \n",
    "Imbalance is not a significant concern for the education dataset, as each state is represented with comprehensive data on various age and educational attainment levels. The dataset does not rely on categorical classes that could lead to imbalance issues, and all states provide data across similar categories of educational attainment.\n",
    "\n",
    "**Scaling**:  \n",
    "Since the education data is provided by age group, we need to scale it by the population size of each age group within each state to ensure accuracy when comparing educational attainment percentages. Scaling by age group population will allow us to make accurate cross-state comparisons of educational attainment levels, as it controls for differences in the age distributions across states. This normalization will make the educational attainment percentages more representative and comparable in relation to each state's age demographics.\n",
    "\n",
    "### Race Dataset\n",
    "\n",
    "**Missingness**:  \n",
    "The race data files, loaded from `*race.csv`, exhibit some missing values, likely due to unavailable data for certain racial categories or demographic subgroups within states. Similar to the education dataset, we will need to address these missing values through imputation or by selectively excluding columns with significant gaps.\n",
    "\n",
    "**Imbalance**:  \n",
    "This dataset contains detailed racial breakdowns across states, with some racial categories potentially having fewer entries than others. If analyzing racial subgroups individually, imbalance may occur due to underrepresentation of smaller demographic groups in certain states, which could lead to biased interpretations. Grouping data by broader racial categories or considering population weights may help to mitigate this issue.\n",
    "\n",
    "**Scaling**:  \n",
    "The data is already in percentage form, representing the share of individuals of differet races in each state's population. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc03d6",
   "metadata": {},
   "source": [
    "**Load and explore data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e892e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPM Data Structure:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>2022</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>States</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimate</th>\n",
       "      <td>16.3</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0       1\n",
       "Year         2022    2022\n",
       "States    Alabama  Alaska\n",
       "Estimate     16.3    10.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values in SPM Data:\n",
      "Year        0\n",
      "States      0\n",
      "Estimate    0\n",
      "dtype: int64\n",
      "\n",
      "Foreign Born Data Structure:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>California</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022.0</th>\n",
       "      <td>26.7</td>\n",
       "      <td>23.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010.0</th>\n",
       "      <td>27.2</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0           1\n",
       "0                             \n",
       "State   California  New Jersey\n",
       "2022.0        26.7        23.5\n",
       "2010.0        27.2        21.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values in Foreign Born Data:\n",
      "0\n",
      "State     0\n",
      "2022.0    0\n",
      "2010.0    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load SPM data and display structure\n",
    "SPM_df = pd.read_csv('data/SPM.csv')\n",
    "print(\"SPM Data Structure:\")\n",
    "display(SPM_df.head(2).T)\n",
    "print(\"\\nMissing Values in SPM Data:\")\n",
    "print(SPM_df.isna().sum())\n",
    "\n",
    "# Load foreign-born data and display structure\n",
    "foreignborn_df = pd.read_csv('data/foreignborn.csv', header=None)\n",
    "foreignborn_df.columns = foreignborn_df.iloc[0]  # Set the header to the first row\n",
    "foreignborn_df = foreignborn_df.drop(0)  # Drop the first row as it is now the header\n",
    "foreignborn_df.reset_index(drop=True, inplace=True)  # Reset the index if needed\n",
    "print(\"\\nForeign Born Data Structure:\")\n",
    "display(foreignborn_df.head(2).T)\n",
    "print(\"\\nMissing Values in Foreign Born Data:\")\n",
    "print(foreignborn_df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e521c",
   "metadata": {},
   "source": [
    "We also care about religion, political makeup, and whether certain abortion laws are in place (all of which are not in the census). We will find them different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7006199e",
   "metadata": {},
   "source": [
    "## RELIGION DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576fc93",
   "metadata": {},
   "source": [
    "We decided to determine people's **religiousness** based off of a state's adherence rate (number of people who adhere to their religion across 1000 ) which can be found at (https://www.thearda.com/us-religion/maps/us-state-maps)\n",
    "\n",
    "We accessed it by webscraping and saved it in `religion_data.csv` in our data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "60bf9ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UT', 'ND', 'DC', 'SD', 'MA', 'RI', 'MN', 'OK', 'WI', 'NY', 'NE', 'LA', 'IA', 'NM', 'PA', 'CT', 'NJ', 'AR', 'TX', 'IL', 'AL', 'MS', 'KY', 'MO', 'TN', 'KS', 'ID', 'NH', 'SC', 'WY', 'CA', 'NC', 'OH', 'GA', 'MT', 'MD', 'IN', 'MI', 'VA', 'FL', 'DE', 'AZ', 'CO', 'VT', 'ME', 'HI', 'WV', 'AK', 'NV', 'WA', 'OR']\n",
      "51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kg/wxm8v9d91mv575kb5wsgxf4h0000gp/T/ipykernel_36133/2974073243.py:12: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  script_tag = soup.find('script', text=re.compile(r'usa_map_div299992000_data = '))\n"
     ]
    }
   ],
   "source": [
    "url2020 = \"https://www.thearda.com/us-religion/maps/us-state-maps\"\n",
    "url2010 = \"https://www.thearda.com/us-religion/maps/us-state-maps?color=orange&m1=2_2_9999_2010\"\n",
    "url2000 = \"https://www.thearda.com/us-religion/maps/us-state-maps?color=orange&m1=2_2_9999_2000\"\n",
    "response = requests.get(url2000)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    \n",
    "script_tag = soup.find('script', text=re.compile(r'usa_map_div299992000_data = '))\n",
    "script_content = script_tag.string\n",
    "start_index = script_content.find('usa_map_div299992000_data =')\n",
    "semicolon_index = script_content.find(';', start_index)\n",
    "mapData = script_content[start_index:semicolon_index]\n",
    "\n",
    "quoted_strings = re.findall(r'\"(.*?)\"', mapData)\n",
    "values_strings = re.findall(r'(\\d+\\.\\d+)', mapData)\n",
    "year = []\n",
    "for i in range(len(values_strings)):\n",
    "    year.append(2000)\n",
    "\n",
    "last_two_chars = [s[-2:] for s in quoted_strings]\n",
    "print(last_two_chars)\n",
    "print(len(values_strings))\n",
    "\n",
    "file_paths = [\n",
    "    'data/AllReligionAdherence_2000.csv',\n",
    "    'data/AllReligionAdherence_2010.csv',\n",
    "    'data/AllReligionAdherence_2020.csv'\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    year = file_name.split('_')[-1].split('.')[0]\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Year'] = year\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33dcca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State                      0\n",
      "Adherence Rate per 1000    0\n",
      "Year                       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adherence Rate per 1000</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>477.607640</td>\n",
       "      <td>2018.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>91.939343</td>\n",
       "      <td>2.584863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>256.324000</td>\n",
       "      <td>2014.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>414.211250</td>\n",
       "      <td>2016.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>485.895000</td>\n",
       "      <td>2018.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>526.577000</td>\n",
       "      <td>2020.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>779.020000</td>\n",
       "      <td>2022.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Adherence Rate per 1000         Year\n",
       "count               450.000000   450.000000\n",
       "mean                477.607640  2018.000000\n",
       "std                  91.939343     2.584863\n",
       "min                 256.324000  2014.000000\n",
       "25%                 414.211250  2016.000000\n",
       "50%                 485.895000  2018.000000\n",
       "75%                 526.577000  2020.000000\n",
       "max                 779.020000  2022.000000"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "religion_data = pd.read_csv('data/religion_data.csv')\n",
    "print(religion_data.isna().sum(axis=0))\n",
    "religion_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff1449",
   "metadata": {},
   "source": [
    "Things to Consider:\n",
    "\n",
    "**Missingness**: After webscraping ARDA, there are no missing values in any cells, but definitely in the number of years (153 vs 451 above). This is because this index is only calculated every 10 years (2000, 2010, 2020). We have to decide how we want to impute this data. One option is to create a line going through the 2000, 2010, and 2020 datapoints, and then impute the data that would sit on this line for 2011-2019 and 2021-2022. \n",
    "\n",
    "**Imbalance**: Although the actual number of state datapoints are equal and fine, the source of the imbalance stems from which religions do these data sets stem from. All the main categories listed on the Association of Religious Digital Archives are branches of Christianity. I have no idea if these are actually the most popular religions in the US or is the source biased towards it.\n",
    "\n",
    "**Scaling**: Since adherence rates range widely (e.g., from 272.19 to 791.06), standardizing or normalizing values might be beneficial, particularly if this data will be used for machine learning or statistical modeling. Standardization (e.g., Z-score) could be applied if you want each adherence rate centered around zero, while min-max normalization scales values between a range like [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b77098",
   "metadata": {},
   "source": [
    "## POLITICS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ce4a9",
   "metadata": {},
   "source": [
    "We decided to determine people's political orienation based off of the Cook Partisan Voting Index (Cook PVI), which is a measure of each state's political leaning relative to the nation as a whole.\n",
    "\n",
    "The primary challenge with using this index is that the methodology was switched in 2022 to weigh the last presidential election 0.75 and the second to last 0.25, as opposed to the even (50/50) weighting of both years that was used in prior years. We will either figure out how to reweight the outcomes based on the raw data if we can get it or exclude 2022 from our analysis.\n",
    "\n",
    "The calculation of the index is described in more detail here: https://www.cookpolitical.com/cook-pvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1712e09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State           0\n",
      "2022_PVI        0\n",
      "2020_Biden      0\n",
      "2020_Trump      0\n",
      "2016_Clinton    0\n",
      "2016_Trump      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>2022_PVI</th>\n",
       "      <th>2020_Biden</th>\n",
       "      <th>2020_Trump</th>\n",
       "      <th>2016_Clinton</th>\n",
       "      <th>2016_Trump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>51</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>R+11</td>\n",
       "      <td>49.50%</td>\n",
       "      <td>48.80%</td>\n",
       "      <td>27.50%</td>\n",
       "      <td>45.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          State 2022_PVI 2020_Biden 2020_Trump 2016_Clinton 2016_Trump\n",
       "count        51       51         51         51           51         51\n",
       "unique       51       30         50         48           48         48\n",
       "top     Alabama     R+11     49.50%     48.80%       27.50%     45.50%\n",
       "freq          1        3          2          2            2          2"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_df = pd.read_csv('data/cook_pvi_2022.csv')\n",
    "print(pol_df.isna().sum(axis=0))\n",
    "pol_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed61383",
   "metadata": {},
   "source": [
    "**Missingness and imbalance**: None in the dataset \n",
    "\n",
    "**Scaling:** We will follow a similar methodology to that used in \"State-level Political Partisanship Strongly Correlates with Health Outcomes for US Children,\" (full citation below).* We converted the PVI's to numerical values, with negative values representing Democratic PVIs and positive numbers representing Republican ones (an arbitrary choice). Then, we scaled those values with sklearn's MinMaxScaler() so that states have a rating between 0 and 1 representing how conservative they are, with 1 being most conservatve and 0 being most liberal.\n",
    "\n",
    "We accessed the data for 2022 from this source: https://datawrapper.dwcdn.net/0djXs/2/ and saved it in cook_pvi_2022.csv in our data folder. We tried scraping the website but the data is not in the html but instead pulled from a source so we were unable to access the data using the same strategy as HW 1.\n",
    "\n",
    "The 2022 data is saved in `data/cook_pvi_2022.csv.`\n",
    "\n",
    "*Paul, M., Zhang, R., Liu, B. et al. State-level political partisanship strongly correlates with health outcomes for US children. Eur J Pediatr 181, 273–280 (2022). https://doi.org/10.1007/s00431-021-04203-y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0a8dc",
   "metadata": {},
   "source": [
    "## ABORTION DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98418d",
   "metadata": {},
   "source": [
    "We decided to determine state's **abortion laws** based off of how late into pregnancy, abortion is legally allowed which can be found at https://lawatlas.org/datasets/abortion-bans. We chose this dataset because it is the only one on the internet showing abortion bans in the 2014-2022 time frame and how they change (most just show abortion bans now)\n",
    "\n",
    "We accessed it by downloading it, converting from xlsx to csv, and saved it in `abortion_data.csv` in our data folder\n",
    "\n",
    "The main thing to consider is that **just because abortion is legal, doesn't mean it is accessible**. Many states may technically allow abortion but only have one clinic, so its not attainable. That said, we have chosen this metric (when is abortion legal), to coincide with current political debate about whether abortion should be legalized. \n",
    "\n",
    "There will be significant preprocessing required as the data is in format `Effective Date`, `Valid Through Date` for each law which must simply be converted into year (whichever law was the majority of the year), and each ban `6 weeks`, `8 weeks` etc is categorical! It would make more sense to simply make a variable listing the latest week aborition is legal (0,6,8,12,52 etc).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f257f2e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abortion_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mabortion_processed\u001b[49m\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing from entire dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m50\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(abortion_processed))\n\u001b[1;32m      3\u001b[0m abortion_processed\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'abortion_processed' is not defined"
     ]
    }
   ],
   "source": [
    "print(abortion_processed.isna().sum(axis=0))\n",
    "print(\"missing from entire dataframe\", 50*5-len(abortion_processed))\n",
    "abortion_processed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de58eb7",
   "metadata": {},
   "source": [
    "Things to Consider:\n",
    "\n",
    "**Missingness**: We see while there are no null cells in our dataset, there are 113 \"missing cells\" from what would be expected (one for every state for every year). This is simply because of how our dataset was made -- there is only a new row if a new law is enacted (not one per year). Thus, if we are missing a row for a certain year, we can simply **impute** the data by copying the previous row. \n",
    "\n",
    "**Imbalance**: There is no imbalance as there is no population sampling. \n",
    "\n",
    "**Scaling**: We don't need to scale the data by itself but might scale data as a whole later so they don't have stds that are too high. \n",
    "\n",
    "**Other**: We decided to change our data from a 1-hot encoded dataset, to 1 quantitative varaible. We did this because there seems to be a clear relatonship between the previously encoded categories (a 10 week ban is stronger than a 15 week ban and less strong than a 4 week ban). Thus, we made it ordinal. In additon, note that if there was no abortion ban, we arbitrarily set this ordinal variable to 40, the length of the average pregancy. It might make some sense to add an **indicator variable** to be 0 if there is no abortion ban at all (40 weeks) and 1 if there is an abortion ban. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d87cb5b",
   "metadata": {},
   "source": [
    "Some issues we preliminary have considered before even inspecting the data includes:\n",
    "\n",
    "\n",
    "- **under reporting immigration status**: via google, people tend to underreport whether they are immigrants. This is potentially a missingness issue\n",
    "\n",
    "-  **multicollinearity**: There is most likely a relationship between racial background / household income and an individual's birth country (immigration status) so we can't use both as predictors in the same equation. We don't know how strong this correlation will be so are not worried yet, but would love to talk about it with a TF\n",
    "\n",
    "- **is this just too much data??**: looking at each of these attributes for each state for each year may just be too many dimensions. Is there really a big difference accross years? Should we just look at 1? If so, which? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b1745",
   "metadata": {},
   "source": [
    "Now we are going to import and inspect each dataset, looking for missingness, imbalance, and scaling issues!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef9b79",
   "metadata": {},
   "source": [
    "## 3. Understand and describe the preprocessing required such that data is in a form amenable to later downstream tasks such as visualizing and modeling, as is appropriate to the specific project goals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e8d54",
   "metadata": {},
   "source": [
    "One of our biggest considerations is that our predictor variables, X, is essentially 3d (reshaped), not 2d. We are looking at a bunch of predictors **over state** and **over time**. This might make our predictions complicated, especially because we are trying to see **why** fertility is changing over time (so the reason that there are less babies in 2022 can't simply be that it is 2022). Thus, it might make more sense to choose only **one year** to regress on. Also, it may be hard to **visualize** multiple years and states simultaniously as, for example, if we drew a map and colored each state by predictor category, we would have to choose one specific time to do so (and vice versa).  PLEASE LET US KNOW WHAT YOU THINK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0aff50",
   "metadata": {},
   "source": [
    "It also may make sense to just try to minimize predictors in general as we don't want to get too specific!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b047880",
   "metadata": {},
   "source": [
    "### This is how we envision generally preprocessing..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0171592b",
   "metadata": {},
   "source": [
    "As you can see, we generally started preprocessign above. For example, we changed abortion laws from categorical variables into an orderinal variable. In addition, as seen below, we standardized the poltical data by scaling it between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5c084",
   "metadata": {},
   "source": [
    "Below is the preprocessing for political data. We scale the Cook PVI scores to be on a scale of 0 to 1, where 1 is most conservative and 0 is least conservative. We drop DC because it is not a state. There is no missingness in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52a21a",
   "metadata": {},
   "source": [
    "To prepare the education dataset for analysis, we began by iterating through each *education.csv file. We extracted the year from each file name to ensure we could track data across different time periods. Within each file, we removed the rows labeled \"Population 25 years and over\" along with the following nine rows, as they did not provide useful information for our analysis. We then isolated columns relevant to our study, retaining only the primary label column and columns ending with \"!!Percent Females!!Estimate,\" which represent the estimated educational attainment percentages for females in each state.\n",
    "\n",
    "For each age group (e.g., \"Population 18 to 24 years\"), we calculated weighted values for educational attainment percentages based on the total population for that group. This scaling ensures that educational attainment percentages are accurately reflected relative to each age group’s size, enabling meaningful comparisons across states. After processing each file, we created a consolidated data frame containing columns for Label, Year, State, Female Estimate (weighted educational attainment percentage), and Population. Finally, we dropped rows where \"Female Estimate\" values were missing, resulting in a cleaned and structured education_df ready for analysis.\n",
    "\n",
    "For the race dataset, we followed a similar iterative approach, processing each *race.csv file individually and extracting the year from each file name. Each file was loaded while skipping the first two lines to focus only on relevant data, and we added a Year column to track temporal changes. To simplify the dataset, we removed columns representing less relevant racial categories (such as “American Indian or Alaska Native,” “Native Hawaiian or Pacific Islander”) and redundant summary columns (like “Total” and “Footnotes”). We also excluded rows containing data for \"United States\" as an aggregate, ensuring that only state-level data remained. Finally, we dropped any rows where \"White\" values were missing, creating a clean and consistent structure across files. The processed race data frames were combined into a single race_df for use in further analysis.\n",
    "\n",
    "These steps produced two organized, cleaned data frames, education_df and race_df, each containing essential demographic details by year and state. The education data is now scaled appropriately by age group population, and both datasets are free of unnecessary or missing values, allowing for more accurate analysis of the relationships between education, race, and fertility rates across states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e68086",
   "metadata": {},
   "source": [
    "We will also **impute/delete missingness** as described in each section above. This is especially relevent when we are missing certain years.\n",
    "Finally, we will combine all our data into one DF and **make the state variable categorical.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885408a2",
   "metadata": {},
   "source": [
    "# Starting Milestone 3/4: Prelim Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe03f89",
   "metadata": {},
   "source": [
    "## First off, we impute all ourn misising data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa6aeb",
   "metadata": {},
   "source": [
    "We started off by imputing/cleaning the data as described above. Our goal is to have 450 rows per data frame (50 states, 9 years from 2014-2022) and them to combine them along state and year axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f16670",
   "metadata": {},
   "source": [
    "## Universal Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca32655",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\n",
    "    'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', \n",
    "    'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', \n",
    "    'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', \n",
    "    'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', \n",
    "    'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', \n",
    "    'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', \n",
    "    'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', \n",
    "    'Wisconsin', 'Wyoming'\n",
    "]\n",
    "state_abbrev_to_name = {\n",
    "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
    "    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n",
    "    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n",
    "    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'\n",
    "}\n",
    "law_to_week = {\n",
    "    \"State\": \"State\",\n",
    "    \"Effective Date\": \"start\",\n",
    "    \"Valid Through Date\": \"end\",\n",
    "    \"Bans_gest_4 weeks postfertilization (6 weeks LMP) \": \"4\",\n",
    "    \"Bans_gest_6 weeks postfertilization (8 weeks LMP) \": \"6\",\n",
    "    \"Bans_gest_8 weeks postfertilization (10 weeks LMP)\": \"8\",\n",
    "    \"Bans_gest_10 weeks postfertilization (12 weeks LMP) \": \"10\",\n",
    "    \"Bans_gest_12 weeks postfertilization (14 weeks LMP)\": \"12\",\n",
    "    \"Bans_gest_13 weeks postfertilization (15 weeks LMP)\": \"13\",\n",
    "    \"Bans_gest_16 weeks postfertilization (18 weeks LMP) \": \"16\",\n",
    "    \"Bans_gest_18 weeks postfertilization (20 weeks LMP)\": \"18\",\n",
    "    \"Bans_gest_19 weeks postfertilization (21 weeks LMP)\": \"19\",\n",
    "    \"Bans_gest20 weeks postfertilization (22 weeks LMP)\": \"20\",\n",
    "    \"Bans_gest_21 weeks postfertilization (23 weeks LMP) \": \"21\",\n",
    "    \"Bans_gest_22 weeks postfertilization (24 weeks LMP)\": \"22\",\n",
    "    \"Bans_gest_24 weeks postfertilization (26 weeks LMP)\": \"24\",\n",
    "    \"Bans_gestViability\": \"24\", #chose via google\n",
    "    \"Bans_gest_Fetus is capable of feeling pain\": \"25\", #chose via google\n",
    "    \"Bans_gest_3rd trimester\": \"28\" #chose via google\n",
    "}\n",
    "\n",
    "years = list(range(2014, 2023))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e2138",
   "metadata": {},
   "source": [
    "## Census Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd3449",
   "metadata": {},
   "source": [
    "Load Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load files matching a pattern\n",
    "def load_data_files(file_pattern, description):\n",
    "    files = glob.glob(file_pattern)\n",
    "    data_frames = {}\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            # Try loading with comma separator\n",
    "            df = pd.read_csv(file, sep=',', on_bad_lines='skip')\n",
    "            data_frames[file] = df\n",
    "        except pd.errors.ParserError:\n",
    "            df = pd.read_csv(file, sep='\\t', on_bad_lines='skip')\n",
    "            data_frames[file] = df\n",
    "\n",
    "    \n",
    "    return data_frames\n",
    "\n",
    "# Load education data \n",
    "education_dfs = load_data_files(\"data/*education.csv\", \"Education\")\n",
    "\n",
    "# Load race data\n",
    "race_dfs = load_data_files(\"data/*race.csv\", \"Race\")\n",
    "\n",
    "# load SPM data\n",
    "SPM_df = pd.read_csv('data/SPM.csv')\n",
    "\n",
    "# Load foreign-born data and display structure\n",
    "foreignborn_df = pd.read_csv('data/foreignborn.csv', header=None)\n",
    "foreignborn_df.columns = foreignborn_df.iloc[0]  # Set the header to the first row\n",
    "foreignborn_df = foreignborn_df.drop(0)  # Drop the first row as it is now the header\n",
    "foreignborn_df.reset_index(drop=True, inplace=True)  # Reset the index if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dce258",
   "metadata": {},
   "source": [
    "Combine and pre clean education Dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4004bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "# Process each education file\n",
    "for file in education_dfs:\n",
    "    # Get year from the file name\n",
    "    year = file.split('/')[-1][:4]\n",
    "\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Drop \"Population 25 years and over\" and the nine rows following it\n",
    "    pop_25_index = df[df.iloc[:, 0] == \"Population 25 years and over\"].index\n",
    "    if not pop_25_index.empty:\n",
    "        df = df.drop(index=range(pop_25_index[0], pop_25_index[0] + 10)).reset_index(drop=True)\n",
    "\n",
    "    # Keep only the first column (label) and columns ending with \"!!Percent Females!!Estimate\"\n",
    "    label_column = df.columns[0]\n",
    "    columns_to_keep = [label_column] + [col for col in df.columns if col.endswith(\"!!Percent Females!!Estimate\")]\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    # Process each age group section to calculate weighted averages\n",
    "    selected_data = []\n",
    "    current_population = None\n",
    "    total_population = 0 \n",
    "    for index, row in df.iterrows():\n",
    "        label = row[label_column]\n",
    "\n",
    "        # Check if row is a population group \n",
    "        if \"Population\" in label:\n",
    "            # Get population number and set as title\n",
    "            try:\n",
    "                total_population = int(label.split()[1]) if label.split()[1].isdigit() else None\n",
    "            except ValueError:\n",
    "                total_population = None\n",
    "            current_population = total_population \n",
    "        elif current_population and \"Percent\" not in label:\n",
    "            # Calculate weighted value for educational attainment percentages based on the total population\n",
    "            for col in df.columns[1:]:  # Exclude the label column\n",
    "                state = col.split(\"!!\")[0]\n",
    "                percent_str = row[col]\n",
    "                try:\n",
    "                    percent = float(str(percent_str).replace(\"%\", \"\").strip()) if percent_str else None\n",
    "                except ValueError:\n",
    "                    percent = None  # Set as None if conversion fails\n",
    "\n",
    "                if percent is not None:\n",
    "                    scaled_value = (percent / 100) * total_population\n",
    "                else:\n",
    "                    scaled_value = None\n",
    "\n",
    "                # Append the result\n",
    "                selected_data.append({\n",
    "                    \"Label\": label,\n",
    "                    \"Year\": year,\n",
    "                    \"State\": state,\n",
    "                    \"Female Estimate\": scaled_value,\n",
    "                    \"Population\": total_population\n",
    "                })\n",
    "\n",
    "    #Append to df_list\n",
    "    temp_df = pd.DataFrame(selected_data)\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "# Combine all data frames\n",
    "education_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Drop rows NaN\n",
    "education_df.dropna(subset=[\"Female Estimate\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f683da",
   "metadata": {},
   "source": [
    "Combine and pre clean race dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "# Process each race file\n",
    "for file_path in race_dfs:\n",
    "    # Get the year\n",
    "    year = file_path.split('/')[-1].split('race')[0]\n",
    "    \n",
    "    try:\n",
    "        # Load the CSV\n",
    "        df = pd.read_csv(file_path, skiprows=2, delimiter=',')\n",
    "        \n",
    "        # Add the year\n",
    "        df['Year'] = year\n",
    "        \n",
    "        # Drop columns\n",
    "        columns_to_drop = ['American Indian or Alaska Native', 'Native Hawaiian or Pacific Islander', 'Total', 'Footnotes']\n",
    "        df = df.loc[:, ~df.columns.str.contains('|'.join(columns_to_drop), na=False)]\n",
    "        \n",
    "        # Remove rows with \"United States\" \n",
    "        df = df[~df.iloc[:, 0].str.strip().str.lower().eq(\"united states\")]\n",
    "        \n",
    "        # Remove rows where \"White\" is NaN\n",
    "        df = df.dropna(subset=[\"White\"])\n",
    "        \n",
    "        # Append\n",
    "        df_list.append(df)\n",
    "    \n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Could not parse {file_path}. Skipping this file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with file {file_path}: {e}\")\n",
    "\n",
    "# Concatenate\n",
    "race_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459858ff",
   "metadata": {},
   "source": [
    "Impute race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define demographic columns\n",
    "demographic_columns = [\"White\", \"Black\", \"Hispanic\", \"Asian\", \"Multiple Races\"]\n",
    "\n",
    "# Create a DF with combinations of years and states\n",
    "all_combinations = pd.DataFrame([(year, state) for year in years for state in states], columns=['Year', 'Location'])\n",
    "\n",
    "all_combinations['Year'] = all_combinations['Year'].astype(int)\n",
    "race_df['Year'] = race_df['Year'].astype(int)\n",
    "\n",
    "# Merge all_combinations\n",
    "race_df_full = all_combinations.merge(race_df, on=['Year', 'Location'], how='left')\n",
    "\n",
    "# Convert to numeric\n",
    "for col in demographic_columns:\n",
    "    race_df_full[col] = pd.to_numeric(race_df_full[col], errors='coerce')\n",
    "\n",
    "# Impute function\n",
    "def impute_multiple_columns(row, df, columns):\n",
    "    for col in columns:\n",
    "        if pd.notnull(row[col]):\n",
    "            continue \n",
    "\n",
    "        prev_year_value = df[(df['Location'] == row['Location']) & (df['Year'] == row['Year'] - 1)][col].values\n",
    "        next_year_value = df[(df['Location'] == row['Location']) & (df['Year'] == row['Year'] + 1)][col].values\n",
    "        if prev_year_value.size > 0 and next_year_value.size > 0:\n",
    "            row[col] = (prev_year_value[0] + next_year_value[0]) / 2\n",
    "        elif prev_year_value.size > 0:\n",
    "            row[col] = prev_year_value[0]\n",
    "        elif next_year_value.size > 0:\n",
    "            row[col] = next_year_value[0]\n",
    "    return row\n",
    "\n",
    "# Impute\n",
    "race_df_full = race_df_full.apply(impute_multiple_columns, axis=1, df=race_df_full, columns=demographic_columns)\n",
    "\n",
    "# Drop rows\n",
    "race_df_full.dropna(subset=demographic_columns, inplace=True)\n",
    "\n",
    "# Fix montana\n",
    "missing_years = [2015, 2016, 2017, 2018]\n",
    "\n",
    "if not race_df_full[(race_df_full['Year'] == 2014) & (race_df_full['Location'] == 'Montana')].empty and \\\n",
    "   not race_df_full[(race_df_full['Year'] == 2019) & (race_df_full['Location'] == 'Montana')].empty:\n",
    "\n",
    "    montana_data_2014 = race_df_full[(race_df_full['Year'] == 2014) & (race_df_full['Location'] == 'Montana')].iloc[0]\n",
    "    montana_data_2019 = race_df_full[(race_df_full['Year'] == 2019) & (race_df_full['Location'] == 'Montana')].iloc[0]\n",
    "\n",
    "    montana_missing_rows = pd.DataFrame({'Year': missing_years, 'Location': 'Montana'})\n",
    "    for col in demographic_columns:\n",
    "        if col in montana_data_2014 and col in montana_data_2019:\n",
    "            avg_value = (montana_data_2014[col] + montana_data_2019[col]) / 2\n",
    "            montana_missing_rows[col] = avg_value\n",
    "        else:\n",
    "            montana_missing_rows[col] = np.nan  # Handle missing columns gracefully\n",
    "\n",
    "    race_state_year = pd.concat([race_df_full, montana_missing_rows], ignore_index=True)\n",
    "\n",
    "# Reset the index \n",
    "race_state_year.reset_index(drop=True, inplace=True)\n",
    "race_state_year.rename(columns={'Location': 'State'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b812a",
   "metadata": {},
   "source": [
    "Impute education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a0a141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kg/wxm8v9d91mv575kb5wsgxf4h0000gp/T/ipykernel_36133/437605411.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  education_df_filtered['year'] = education_df_filtered['Year']\n",
      "/var/folders/kg/wxm8v9d91mv575kb5wsgxf4h0000gp/T/ipykernel_36133/437605411.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  education_df_filtered['state'] = education_df_filtered['State']\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to only include Population values 18, 25, or 35\n",
    "education_df_filtered = education_df[education_df['Population'].isin([18, 25, 35])]\n",
    "\n",
    "# Set year and state columns\n",
    "education_df_filtered['year'] = education_df_filtered['Year']\n",
    "education_df_filtered['state'] = education_df_filtered['State']\n",
    "\n",
    "# Pivot the data\n",
    "education_pivoted = education_df_filtered.pivot_table(\n",
    "    index=[\"year\", \"state\"],\n",
    "    columns=\"Label\",\n",
    "    values=\"Female Estimate\",\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Reset the index\n",
    "education_pivoted.reset_index(inplace=True)\n",
    "\n",
    "# Create all year-state combinations\n",
    "all_combinations = pd.DataFrame([(year, state) for year in years for state in states], columns=['year', 'state'])\n",
    "\n",
    "# Merge\n",
    "all_combinations['year'] = all_combinations['year'].astype(int)\n",
    "education_pivoted['year'] = education_pivoted['year'].astype(int)\n",
    "education_complete = all_combinations.merge(education_pivoted, on=['year', 'state'], how='left')\n",
    "\n",
    "# Find columns to impute\n",
    "columns_to_impute = [col for col in education_complete.columns if col not in ['year', 'state']]\n",
    "\n",
    "# Define imputation function\n",
    "def impute_column(df, column):\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.isnull(row[column]):\n",
    "            state, year = row['state'], row['year']\n",
    "            prev_years = df[(df['state'] == state) & (df['year'] < year)][['year', column]].dropna().sort_values('year', ascending=False)\n",
    "            next_years = df[(df['state'] == state) & (df['year'] > year)][['year', column]].dropna().sort_values('year', ascending=True)\n",
    "            prev_value = prev_years[column].values[0] if not prev_years.empty else None\n",
    "            next_value = next_years[column].values[0] if not next_years.empty else None\n",
    "            if prev_value is not None and next_value is not None:\n",
    "                df.at[i, column] = (prev_value + next_value) / 2\n",
    "            elif prev_value is not None:\n",
    "                df.at[i, column] = prev_value\n",
    "            elif next_value is not None:\n",
    "                df.at[i, column] = next_value\n",
    "\n",
    "# Apply imputation\n",
    "for column in columns_to_impute:\n",
    "    impute_column(education_complete, column)\n",
    "\n",
    "# Drop any rows with remaining NaN values and reset index\n",
    "education_complete.dropna(inplace=True)\n",
    "education_complete.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d1af0",
   "metadata": {},
   "source": [
    "Impute SPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1a833",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/cs109a/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'state'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m SPM_df \u001b[38;5;241m=\u001b[39m SPM_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPM\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m })\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Filter for states and years\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m SPM_df \u001b[38;5;241m=\u001b[39m SPM_df[(\u001b[43mSPM_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misin(states)) \u001b[38;5;241m&\u001b[39m (SPM_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbetween(\u001b[38;5;241m2014\u001b[39m, \u001b[38;5;241m2022\u001b[39m))]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Convert to numeric\u001b[39;00m\n\u001b[1;32m     12\u001b[0m SPM_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSPM\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(SPM_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSPM\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/cs109a/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/micromamba/envs/cs109a/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'state'"
     ]
    }
   ],
   "source": [
    "# Rename columns to lowercase\n",
    "SPM_df = SPM_df.rename(columns={\n",
    "    \"Year\": \"year\",\n",
    "    \"States\": \"state\",\n",
    "    \"Estimate\": \"estimate\"\n",
    "})\n",
    "\n",
    "# Filter for states and years\n",
    "SPM_df = SPM_df[(SPM_df['state'].isin(states)) & (SPM_df['year'].between(2014, 2022))]\n",
    "\n",
    "# Convert 'estimate' column to numeric\n",
    "SPM_df['estimate'] = pd.to_numeric(SPM_df['estimate'], errors='coerce')\n",
    "\n",
    "# Find missing rows for 2020\n",
    "states_missing_2020 = set(SPM_df[SPM_df['year'] == 2020]['state'])\n",
    "states_all = set(SPM_df['state'].unique())\n",
    "missing_states_2020 = states_all - states_missing_2020\n",
    "\n",
    "imputed_rows = []\n",
    "\n",
    "for state in missing_states_2020:\n",
    "    # Get the estimate values for 2019 and 2021\n",
    "    estimate_2019 = SPM_df[(SPM_df['state'] == state) & (SPM_df['year'] == 2019)]['estimate'].values\n",
    "    estimate_2021 = SPM_df[(SPM_df['state'] == state) & (SPM_df['year'] == 2021)]['estimate'].values\n",
    "    \n",
    "    # Calculate the average for 2020\n",
    "    if estimate_2019.size > 0 and estimate_2021.size > 0:\n",
    "        estimate_2020 = (estimate_2019[0] + estimate_2021[0]) / 2\n",
    "        imputed_rows.append({'year': 2020, 'state': state, 'estimate': estimate_2020})\n",
    "\n",
    "# Imputed DF\n",
    "imputed_df = pd.DataFrame(imputed_rows)\n",
    "spm_state_year = pd.concat([SPM_df, imputed_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017d7e0e",
   "metadata": {},
   "source": [
    "Impute foreign born"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns \n",
    "foreignborn_df = foreignborn_df.rename(columns={ \"State\": \"state\"})\n",
    "foreignborn_df.columns.values[1] = \"2022\"\n",
    "foreignborn_df.columns.values[2] = \"2010\"\n",
    "\n",
    "# Reshape \n",
    "foreignborn_df_long = pd.melt(\n",
    "    foreignborn_df, \n",
    "    id_vars=[\"state\"],\n",
    "    value_vars=[\"2022\", \"2010\"],\n",
    "    var_name=\"year\",\n",
    "    value_name=\"foreignborn\"\n",
    ")\n",
    "\n",
    "# Convert year to integer\n",
    "foreignborn_df_long['year'] = foreignborn_df_long['year'].astype(int)\n",
    "\n",
    "# Create a DataFrame with all combinations of state and year\n",
    "years_to_impute = list(range(2010, 2023))\n",
    "states = foreignborn_df_long['state'].unique()\n",
    "all_combinations = pd.DataFrame([(state, year) for state in states for year in years_to_impute], columns=['state', 'year'])\n",
    "\n",
    "# Merge with the original data\n",
    "foreignborn_complete = pd.merge(all_combinations, foreignborn_df_long, on=['state', 'year'], how='left')\n",
    "\n",
    "# Function to fill in missing years\n",
    "def linear_impute(df):\n",
    "    start_value = df.loc[df['year'] == 2010, 'foreignborn'].values[0]\n",
    "    end_value = df.loc[df['year'] == 2022, 'foreignborn'].values[0]\n",
    "    \n",
    "    # Calculate the incremental change for each year\n",
    "    step = (end_value - start_value) / 12  # 12 intervals between 2010 and 2022\n",
    "    \n",
    "    # Fill in each year with the interpolated value\n",
    "    for i, year in enumerate(range(2011, 2022), start=1):\n",
    "        df.loc[df['year'] == year, 'foreignborn'] = start_value + step * i\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply imputation function \n",
    "foreignborn_complete = foreignborn_complete.groupby('state').apply(linear_impute)\n",
    "\n",
    "# Filter years\n",
    "foreignborn_state_year = foreignborn_complete[(foreignborn_complete['year'] >= 2014) & (foreignborn_complete['year'] <= 2022)]\n",
    "foreignborn_state_year = foreignborn_state_year.drop_duplicates(subset=['state', 'year'])\n",
    "\n",
    "# Reset index\n",
    "foreignborn_state_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "foreignborn_state_year = foreignborn_state_year.rename(columns={'state': 'State', 'year': 'Year'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adaa205",
   "metadata": {},
   "source": [
    "## Religion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_data = pd.read_csv('data/religion_data.csv')\n",
    "\n",
    "data_2010 = religion_data[religion_data['Year'] == 2010]\n",
    "data_2020 = religion_data[religion_data['Year'] == 2020]\n",
    "\n",
    "common_states = set(data_2010['State']).intersection(set(data_2020['State']))\n",
    "data_2010 = data_2010[data_2010['State'].isin(common_states)]\n",
    "data_2020 = data_2020[data_2020['State'].isin(common_states)]\n",
    "\n",
    "data_2010 = data_2010.sort_values(by='State').reset_index(drop=True)\n",
    "data_2020 = data_2020.sort_values(by='State').reset_index(drop=True)\n",
    "\n",
    "years = np.array([2010, 2020]).reshape(-1, 1)\n",
    "adherence_rates_2010 = data_2010['Adherence Rate per 1000'].values\n",
    "adherence_rates_2020 = data_2020['Adherence Rate per 1000'].values\n",
    "\n",
    "imputed_data = []\n",
    "for state in common_states:\n",
    "    adherence_rates = np.array([adherence_rates_2010[data_2010['State'] == state].item(),\n",
    "                                adherence_rates_2020[data_2020['State'] == state].item()]).reshape(-1, 1)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(years, adherence_rates)\n",
    "    \n",
    "    for year in range(2011, 2023):\n",
    "        imputed_rate = model.predict(np.array([[year]]))[0][0]\n",
    "        imputed_data.append({'Year': year, 'State': state, 'Adherence Rate per 1000': imputed_rate})\n",
    "\n",
    "imputed_df = pd.DataFrame(imputed_data)\n",
    "\n",
    "religion_state_year = pd.concat([religion_data, imputed_df], ignore_index=True)\n",
    "religion_state_year = religion_state_year.sort_values(by=['State', 'Year']).reset_index(drop=True)\n",
    "religion_state_year = religion_state_year.drop_duplicates(subset=['State', 'Year'])\n",
    "religion_state_year = religion_state_year[religion_state_year['State'] != 'DC']\n",
    "religion_state_year['State'] =religion_state_year['State'].replace(state_abbrev_to_name)\n",
    "\n",
    "years_to_drop = list(range(2000, 2014))\n",
    "\n",
    "for years in years_to_drop:\n",
    "    religion_state_year = religion_state_year[religion_state_year['Year'] != years]\n",
    "\n",
    "state_counts = religion_state_year['State'].value_counts()\n",
    "\n",
    "religion_state_year.to_csv('data/religion_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5923c4f",
   "metadata": {},
   "source": [
    "## Politics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18661894",
   "metadata": {},
   "source": [
    "### Impute: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fce0433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>political_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.236842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.236842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  Year  political_ranking\n",
       "0     Alabama  2014           0.236842\n",
       "1      Alaska  2014           0.421053\n",
       "2     Arizona  2014           0.578947\n",
       "3    Arkansas  2014           0.236842\n",
       "4  California  2014           0.973684"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_2022_df = pd.read_csv('data/cook_pvi_2022.csv')\n",
    "\n",
    "#Drop DC because it's not a state: \n",
    "pol_2022_df = pol_2022_df[pol_2022_df['State'] != 'District of Columbia']\n",
    "pol_2022_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert 2020_Biden, 2020_Trump, 2016_Clinton, 2016_Trump to ints for future calculations\n",
    "perc_to_int_columns = ['2020_Biden', '2020_Trump', '2016_Clinton', '2016_Trump']\n",
    "pol_2022_df[perc_to_int_columns] = pol_2022_df[perc_to_int_columns].apply(\n",
    "    lambda col: col.astype(str).str.rstrip('%').astype(float)\n",
    ")\n",
    "\n",
    "# Calculate the 2022 PVI using the old methodology:\n",
    "national_2020 = 4.4 #D+4.4\n",
    "national_2016 = 2.1 #D+2.1 \n",
    "lean_2020 =  pol_2022_df[\"2020_Biden\"] - pol_2022_df[\"2020_Trump\"] #How democrat it leans (- democrat is republican)\n",
    "lean_2016 =  pol_2022_df[\"2016_Clinton\"] - pol_2022_df[\"2016_Trump\"] #same as lean_2020\n",
    "\n",
    "# Positive differences are d\n",
    "diff_from_nat_2016 = lean_2016 - national_2016 # If positive leans dem, if negative leans republican\n",
    "diff_from_nat_2020 = lean_2020 - national_2020 # Sams as ^\n",
    "\n",
    "# Make a column for old methodology - unscaled rating (negative is Republican, positive is Dem)\n",
    "# Dividing by two twice bc that's what it seems like Cook PVI methodology did\n",
    "# Also rounding to the nearest int\n",
    "pol_2022_df[\"2022_unscaled_rating_old\"] = round(((diff_from_nat_2016 + diff_from_nat_2020) / 2) / 2)\n",
    "\n",
    "# Now, scale the 2020_unscaled_rating_old\n",
    "pol_rating_scaler = MinMaxScaler()\n",
    "scaled_2022_ratings_old = pol_rating_scaler.fit_transform(pol_2022_df['2022_unscaled_rating_old'].values.reshape(-1,1))\n",
    "\n",
    "pol_2022_df[\"2022_scaled_rating_old\"] = scaled_2022_ratings_old\n",
    "pol_2022_df[:20]\n",
    "\n",
    "pol_2022_df[\"Year\"] = 2022\n",
    "\n",
    "years = [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "pol_by_year = []\n",
    "\n",
    "for i in range(len(years)):\n",
    "    year_df = f'pol_{years[i]}_df'\n",
    "    year_df = pol_2022_df.copy()\n",
    "    year_df[\"Year\"] = years[i]\n",
    "    pol_by_year.append(year_df)\n",
    "\n",
    "print(len(pol_by_year))\n",
    "print(type(pol_by_year[0]))\n",
    "\n",
    "pol_by_year.append(pol_2022_df)\n",
    "\n",
    "# 0 is extreme conservartive, 1 is extreme liberal\n",
    "pol_state_year = pd.concat(pol_by_year, ignore_index=True)\n",
    "pol_state_year = pol_state_year[['State', 'Year', '2022_scaled_rating_old']]\n",
    "pol_state_year = pol_state_year.rename(columns={'2022_scaled_rating_old': 'political_ranking'})\n",
    "pol_state_year.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d65b9",
   "metadata": {},
   "source": [
    "## Abortion Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452fe48d",
   "metadata": {},
   "source": [
    "The abortion data starts off as a table where each column is a type of abortion restriction and each row is for a law passed in a certain state. I start off by reshaping the data such that there is a column denoting the latest number of weeks at which abortion allowed (by the strictest law) in a given state and also added an indicator column denoting if there are no laws at all. Then, for the missing years (when no laws were enacted), I copied over the value from the previous year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9374b",
   "metadata": {},
   "source": [
    "### Reshape and Impute: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "abortion_data = pd.read_csv('data/abortion_data.csv')\n",
    "\n",
    "# swap column names to be the number of weeks until abortion is banned\n",
    "columns = [\"State\",\"Effective Date\",\"Valid Through Date\",\"Bans_gest_4 weeks postfertilization (6 weeks LMP) \" ,\"Bans_gest_6 weeks postfertilization (8 weeks LMP) \" ,\"Bans_gest_8 weeks postfertilization (10 weeks LMP)\",\"Bans_gest_10 weeks postfertilization (12 weeks LMP) \" ,\"Bans_gest_12 weeks postfertilization (14 weeks LMP)\",\"Bans_gest_13 weeks postfertilization (15 weeks LMP)\",\"Bans_gest_16 weeks postfertilization (18 weeks LMP) \",\"Bans_gest_18 weeks postfertilization (20 weeks LMP)\",\"Bans_gest_19 weeks postfertilization (21 weeks LMP)\",\"Bans_gest20 weeks postfertilization (22 weeks LMP)\",\"Bans_gest_21 weeks postfertilization (23 weeks LMP) \" ,\"Bans_gest_22 weeks postfertilization (24 weeks LMP)\",\"Bans_gest_24 weeks postfertilization (26 weeks LMP)\",\"Bans_gestViability\",\"Bans_gest_Fetus is capable of feeling pain\",\"Bans_gest_3rd trimester\"]\n",
    "abortion_data = abortion_data[columns]\n",
    "abortion_data = abortion_data.rename(columns=law_to_week)\n",
    "\n",
    "# create a column, latest_abortion, that has the value of the number of weeks allowed until abortion is banned\n",
    "abortion_processed = abortion_data[['State', 'start', 'end']].copy()\n",
    "abortion_processed['latest_abortion'] = abortion_data[abortion_data.columns].apply(\n",
    "    lambda row: next((int(col) for col in abortion_data.columns  if str(row[col]) == \"1\"), 40), axis=1)\n",
    "\n",
    "# add an indicator column if there is no abortion law\n",
    "abortion_processed['no_abortion_law'] = 0\n",
    "abortion_processed.loc[abortion_processed['latest_abortion'] == 40, 'no_abortion_law'] = 1\n",
    "\n",
    "# if there are multiple laws in a year, choose the one with the longest duration\n",
    "abortion_processed['start'] = pd.to_datetime(abortion_processed['start'])\n",
    "abortion_processed['end'] = pd.to_datetime(abortion_processed['end'])\n",
    "abortion_processed['Year'] = abortion_processed['start'].dt.year\n",
    "abortion_processed['length'] = (abortion_processed['end'] - abortion_processed['start']).dt.days\n",
    "abortion_processed = abortion_processed.loc[abortion_processed.groupby(['State', 'Year'])['length'].idxmax()]\n",
    "abortion_processed = abortion_processed.drop(columns=['start', 'end', 'length'])\n",
    "\n",
    "# remove dc\n",
    "abortion_no_dc = abortion_processed[abortion_processed['State'] != \"District of Columbia\"]\n",
    "\n",
    "# create a dataframe include missing years\n",
    "years = range(2018, 2022+1)\n",
    "all_combinations = pd.MultiIndex.from_product(\n",
    "    [abortion_no_dc['State'].unique(), years],\n",
    "    names=['State', 'Year']\n",
    ")\n",
    "\n",
    "#impute with most recent law\n",
    "abortion_state_year = abortion_no_dc.set_index(['State', 'Year']).reindex(all_combinations).reset_index()\n",
    "abortion_state_year['latest_abortion'] = abortion_state_year.groupby('State')['latest_abortion'].ffill()\n",
    "abortion_state_year['no_abortion_law'] = abortion_state_year.groupby('State')['no_abortion_law'].ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfe90d",
   "metadata": {},
   "source": [
    "## Now we combine our different datasets along State and Years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00352189",
   "metadata": {},
   "source": [
    "Note since we only have abortion data from 2018-2022, we made two datasetes, one including 2014-2017 and one without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c983340a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>State</th>\n",
       "      <th>Fertility_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>AL</td>\n",
       "      <td>58.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>AK</td>\n",
       "      <td>64.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>AZ</td>\n",
       "      <td>54.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>AR</td>\n",
       "      <td>60.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>CA</td>\n",
       "      <td>52.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2014</td>\n",
       "      <td>VA</td>\n",
       "      <td>61.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2014</td>\n",
       "      <td>WA</td>\n",
       "      <td>63.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2014</td>\n",
       "      <td>WV</td>\n",
       "      <td>60.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>2014</td>\n",
       "      <td>WI</td>\n",
       "      <td>61.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>2014</td>\n",
       "      <td>WY</td>\n",
       "      <td>69.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year State  Fertility_Rate\n",
       "0    2022    AL            58.7\n",
       "1    2022    AK            64.9\n",
       "2    2022    AZ            54.9\n",
       "3    2022    AR            60.2\n",
       "4    2022    CA            52.8\n",
       "..    ...   ...             ...\n",
       "446  2014    VA            61.3\n",
       "447  2014    WA            63.3\n",
       "448  2014    WV            60.3\n",
       "449  2014    WI            61.8\n",
       "450  2014    WY            69.4\n",
       "\n",
       "[450 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fertility_rate = pd.read_csv('data/fertility_rate_census.csv')\n",
    "fertility_rate = fertility_rate.rename(columns={\n",
    "    'YEAR': 'Year',\n",
    "    'STATE': 'State',\n",
    "    'FERTILITY RATE': 'Fertility_Rate'\n",
    "})[['Year', 'State', 'Fertility_Rate']]\n",
    "fertility_state_year = fertility_rate[fertility_rate['State'] != \"District of Columbia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f97e495",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spm_state_year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m combined_2018_to_2022 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(abortion_state_year, religion_state_year, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m combined_2018_to_2022 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(combined_2018_to_2022, race_state_year, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m combined_2018_to_2022 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(combined_2018_to_2022, \u001b[43mspm_state_year\u001b[49m, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m combined_2018_to_2022 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(combined_2018_to_2022, foreignborn_state_year, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m combined_2018_to_2022 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(combined_2018_to_2022, education_state_year, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spm_state_year' is not defined"
     ]
    }
   ],
   "source": [
    "combined_2018_to_2022 = pd.merge(fertility_state_year, religion_state_year, on=['State', 'Year'], how='inner')\n",
    "combined_2018_to_2022 = pd.merge(combined_2018_to_2022, race_state_year, on=['State', 'Year'], how='inner')\n",
    "combined_2018_to_2022 = pd.merge(combined_2018_to_2022, spm_state_year, on=['State', 'Year'], how='inner')\n",
    "combined_2018_to_2022 = pd.merge(combined_2018_to_2022, foreignborn_state_year, on=['State', 'Year'], how='inner')\n",
    "combined_2018_to_2022 = pd.merge(combined_2018_to_2022, education_state_year, on=['State', 'Year'], how='inner')\n",
    "combined_2018_to_2022 = pd.merge(combined_2018_to_2022, pol_state_year, on=['State', 'Year'], how='inner')\n",
    "combined_2018_to_2022 = pd.merge(combined_2018_to_2022, abortion_state_year, on=['State', 'Year'], how='inner')\n",
    "\n",
    "combined_2018_to_2022.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c96d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_2014_to_2022 = pd.merge(fertility_state_year, religion_state_year, on=['State', 'Year'], how='outer')\n",
    "combined_2014_to_2022 = pd.merge(combined_2014_to_2022, race_state_year, on=['State', 'Year'], how='outer')\n",
    "combined_2014_to_2022 = pd.merge(combined_2014_to_2022, spm_state_year, on=['State', 'Year'], how='outer')\n",
    "combined_2014_to_2022 = pd.merge(combined_2014_to_2022, foreignborn_state_year, on=['State', 'Year'], how='outer')\n",
    "combined_2014_to_2022 = pd.merge(combined_2014_to_2022, education_state_year, on=['State', 'Year'], how='outer')\n",
    "combined_2014_to_2022 = pd.merge(combined_2014_to_2022, pol_state_year, on=['State', 'Year'], how='outer')\n",
    "combined_2014_to_2022 = pd.merge(combined_2014_to_2022, abortion_state_year, on=['State', 'Year'], how='outer')\n",
    "combined_2014_to_2022.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a031ef",
   "metadata": {},
   "source": [
    "### One way to preliminarily understand why fertility rate is decreasing is to look at where it changes over time, and think about what factors dominate those areas.... aka do PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_fertility = fertility_rate.pivot(index='STATE', columns='YEAR', values='FERTILITY RATE')\n",
    "X_std =  StandardScaler().fit_transform(pivoted_fertility.fillna(0)) \n",
    "pca = PCA(n_components=1) \n",
    "principal_components = pca.fit_transform(X_std)\n",
    "\n",
    "states = pivoted_fertility.index\n",
    "pca_states = pd.DataFrame(data=principal_components, columns=['PC1'], index=states).reset_index()\n",
    "pca_states.columns = ['state', 'PC1']\n",
    "\n",
    "print(\"this accounts for\", np.round(100*pca.explained_variance_ratio_[0], 2), \"% variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(\n",
    "    pca_states,\n",
    "    locations='state',\n",
    "    locationmode=\"USA-states\",\n",
    "    color='PC1',\n",
    "    scope=\"usa\",  \n",
    "    range_color=[-5,5],\n",
    "    color_continuous_scale=['purple', 'white', 'green'], \n",
    "    labels={'PC1': 'Principal Component Value'}\n",
    ")\n",
    "\n",
    "fig.update_layout(title_text=\"What states have the highest variation in fertility rates?\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a5c55",
   "metadata": {},
   "source": [
    "We see that the first principle componetent represents 90% of varition over state and time. Thus, most of the change (net decrease) in fertility rate is seen when the coasts (purples) move strongly in one direction (presumably decrease given the net decrease) and the center of the US moves slightly in the other. We can use this information to understand what variables would be good predictors, aka the ones that match the coloring above. For example, a wealth distribution or political map would see similar differences with one color on the coast and another in the center. Thus we know that looking at household income and political affiliation are going to be really good predictor variables!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> expected_url = './data/html/screenboston.html'\n>>> assert os.path.isfile(expected_url), f'Expected local file {expected_url}'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> expected_url = './data/html/screenboston.html'\n>>> with open(expected_url, 'r') as f:\n...     content = f.read()\n...     s = BeautifulSoup(content)\n...     assert s.select_one('p').text == 'Screen Boston', f'Content of file saved from {expected_url} should contain a <p> tag with the page name.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> n = 50\n>>> assert len(movies) > n, f'`movies` should contain more than {n} elements; you have {len(movies)}.'\n>>> assert all((isinstance(m, dict) for m in movies)), 'Elements of `movies` should all be dictionaries.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> keys = {'title', 'directors', 'year', 'genre', 'runtime', 'theater', 'screen_date', 'screen_times'}\n>>> assert all((set(keys).issubset(m.keys()) for m in movies)), f'Each dictionary in `movies` should contain all of these keys: {', '.join(keys)}. At least one dictionary is missing one or more keys.'\n",
         "hidden": false,
         "locked": false,
         "points": 4
        },
        {
         "code": ">>> assert all((isinstance(m['year'], int) for m in movies)), \"The 'year' value should be an integer in all dictionaries.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> snapshots = glob.glob('./data/html/snapshot_*.html')\n>>> assert len(snapshots) >= 4, f'You should have at least 4 snapshots, but found {len(snapshots)} files with paths like ./data/html/snapshot_*.html'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> snapshots = glob.glob('./data/html/snapshot_*.html')\n>>> assert all((re.match('snapshot_\\\\d{8}\\\\.html', os.path.basename(f)) for f in snapshots)), \"All snapshot files should be named in the format 'snapshot_YYYYMMDD.html'\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> assert os.path.isfile('data/movies.json'), \"The file 'data/movies.json' should exist.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> with open('data/movies.json', 'r') as f:\n...     movies = json.load(f)\n>>> n = 300\n>>> assert len(movies) > n, f'`movies` should now contain more than {n} elements; you have {len(movies)}.'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> keys = {'title', 'directors', 'year', 'genre', 'runtime', 'theater', 'screen_date', 'screen_times'}\n>>> assert all((isinstance(m, dict) and set(keys).issubset(m.keys()) for m in movies)), f'Each dictionary in `movies` should contain all of these keys: {', '.join(keys)}. At least one dictionary is missing one or more keys.'\n",
         "hidden": false,
         "locked": false,
         "points": 5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(df, pd.DataFrame), \"You should have stored your DataFrame in a variable called 'df'\"\n>>> assert df.shape[1] == 8, 'Your DataFrame, df, should have 8 columns'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> assert df.shape[0] > 300, 'You should have found at least 300 non-duplicate rows'\n>>> assert df.shape[0] == df.drop_duplicates().shape[0], 'There are still duplicate rows in your DataFrame'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> def is_datetime_column_hacky(column):\n...     dtype_str = str(column.dtype)\n...     valid_types = ['date', 'timestamp', 'timedelta', 'duration']\n...     return any((t in dtype_str.lower() for t in valid_types))\n>>> assert is_datetime_column_hacky(df['screen_date']), \"The 'screen_date' column must be either a Pandas or PyArrow date/datetime type.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> def is_datetime_column_hacky(column):\n...     dtype_str = str(column.dtype)\n...     valid_types = ['date', 'timestamp', 'timedelta', 'duration']\n...     return any((t in dtype_str.lower() for t in valid_types))\n>>> assert is_datetime_column_hacky(df['runtime']), \"The 'runtime' column must be either a Pandas or PyArrow timedelta type.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> assert df['screen_date'].is_monotonic_increasing, \"The 'screen_date' column is not sorted in ascending order\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": 7,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'wiki_id' in df.columns, \"`df` should now have a column called 'wiki_id'.\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> p = 0.5\n>>> assert (q := df['wiki_id'].notna().mean()) >= p, f'You should have been able to find wiki IDs for at least {p:.0%} of the screenings. You found {q:.0%}.'\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'wiki_html' in df.columns, \"`df` should now have a column called 'wiki_html'.\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> p = 0.5\n>>> assert (q := df['wiki_html'].notna().mean()) >= p, f'You should have been able to acquire wiki page HTML content for at least {p:.0%} of the screenings. You found {q:.0%}.'\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "wrapup": {
     "name": "wrapup",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert float(hours_spent_on_hw), 'Please select a time in hours (int or float) to specify how long you spent on this assignment.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
